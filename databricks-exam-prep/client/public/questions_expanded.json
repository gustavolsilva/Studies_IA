[
  {
    "id": 1,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Forçando o uso de cache em todas as queries",
      "B": "Apenas aumentando a memória do cluster",
      "C": "Removendo dados desnecessários automaticamente",
      "D": "Através de análise de padrões de execução e recomendações de índices"
    },
    "correctAnswer": "D",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 2,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Dashboards estáticos",
      "B": "Apenas logs de erro",
      "C": "Documentação de referência",
      "D": "Query Profile e Spark UI"
    },
    "correctAnswer": "D",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 3,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Sempre usa o maior cluster disponível",
      "B": "Aleatoriamente entre opções disponíveis",
      "C": "Através de análise de padrões de workload e recomendações automáticas",
      "D": "Apenas baseado em tamanho de dados"
    },
    "correctAnswer": "C",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 4,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Simplificar decisões de particionamento e organização de dados",
      "B": "Apenas documentação",
      "C": "Apenas performance de leitura",
      "D": "Apenas segurança"
    },
    "correctAnswer": "A",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 5,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Apenas aumentando a memória do cluster",
      "B": "Forçando o uso de cache em todas as queries",
      "C": "Através de análise de padrões de execução e recomendações de índices",
      "D": "Removendo dados desnecessários automaticamente"
    },
    "correctAnswer": "C",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 6,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Query Profile e Spark UI",
      "B": "Apenas logs de erro",
      "C": "Documentação de referência",
      "D": "Dashboards estáticos"
    },
    "correctAnswer": "A",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 7,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Através de análise de padrões de workload e recomendações automáticas",
      "B": "Sempre usa o maior cluster disponível",
      "C": "Apenas baseado em tamanho de dados",
      "D": "Aleatoriamente entre opções disponíveis"
    },
    "correctAnswer": "A",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 8,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Apenas performance de leitura",
      "B": "Simplificar decisões de particionamento e organização de dados",
      "C": "Apenas segurança",
      "D": "Apenas documentação"
    },
    "correctAnswer": "B",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 9,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Através de análise de padrões de execução e recomendações de índices",
      "B": "Removendo dados desnecessários automaticamente",
      "C": "Apenas aumentando a memória do cluster",
      "D": "Forçando o uso de cache em todas as queries"
    },
    "correctAnswer": "A",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 10,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Documentação de referência",
      "B": "Query Profile e Spark UI",
      "C": "Apenas logs de erro",
      "D": "Dashboards estáticos"
    },
    "correctAnswer": "B",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 11,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Apenas baseado em tamanho de dados",
      "B": "Sempre usa o maior cluster disponível",
      "C": "Através de análise de padrões de workload e recomendações automáticas",
      "D": "Aleatoriamente entre opções disponíveis"
    },
    "correctAnswer": "C",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 12,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Simplificar decisões de particionamento e organização de dados",
      "B": "Apenas documentação",
      "C": "Apenas performance de leitura",
      "D": "Apenas segurança"
    },
    "correctAnswer": "A",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 13,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Forçando o uso de cache em todas as queries",
      "B": "Apenas aumentando a memória do cluster",
      "C": "Através de análise de padrões de execução e recomendações de índices",
      "D": "Removendo dados desnecessários automaticamente"
    },
    "correctAnswer": "C",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 14,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Dashboards estáticos",
      "B": "Apenas logs de erro",
      "C": "Documentação de referência",
      "D": "Query Profile e Spark UI"
    },
    "correctAnswer": "D",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 15,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Através de análise de padrões de workload e recomendações automáticas",
      "B": "Sempre usa o maior cluster disponível",
      "C": "Apenas baseado em tamanho de dados",
      "D": "Aleatoriamente entre opções disponíveis"
    },
    "correctAnswer": "A",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 16,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Apenas documentação",
      "B": "Apenas performance de leitura",
      "C": "Simplificar decisões de particionamento e organização de dados",
      "D": "Apenas segurança"
    },
    "correctAnswer": "C",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 17,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Removendo dados desnecessários automaticamente",
      "B": "Através de análise de padrões de execução e recomendações de índices",
      "C": "Forçando o uso de cache em todas as queries",
      "D": "Apenas aumentando a memória do cluster"
    },
    "correctAnswer": "B",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 18,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Query Profile e Spark UI",
      "B": "Dashboards estáticos",
      "C": "Documentação de referência",
      "D": "Apenas logs de erro"
    },
    "correctAnswer": "A",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 19,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Através de análise de padrões de workload e recomendações automáticas",
      "B": "Apenas baseado em tamanho de dados",
      "C": "Sempre usa o maior cluster disponível",
      "D": "Aleatoriamente entre opções disponíveis"
    },
    "correctAnswer": "A",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 20,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Apenas performance de leitura",
      "B": "Simplificar decisões de particionamento e organização de dados",
      "C": "Apenas segurança",
      "D": "Apenas documentação"
    },
    "correctAnswer": "B",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 21,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Através de análise de padrões de execução e recomendações de índices",
      "B": "Removendo dados desnecessários automaticamente",
      "C": "Forçando o uso de cache em todas as queries",
      "D": "Apenas aumentando a memória do cluster"
    },
    "correctAnswer": "A",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 22,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Query Profile e Spark UI",
      "B": "Dashboards estáticos",
      "C": "Documentação de referência",
      "D": "Apenas logs de erro"
    },
    "correctAnswer": "A",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 23,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Sempre usa o maior cluster disponível",
      "B": "Aleatoriamente entre opções disponíveis",
      "C": "Apenas baseado em tamanho de dados",
      "D": "Através de análise de padrões de workload e recomendações automáticas"
    },
    "correctAnswer": "D",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 24,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Simplificar decisões de particionamento e organização de dados",
      "B": "Apenas performance de leitura",
      "C": "Apenas segurança",
      "D": "Apenas documentação"
    },
    "correctAnswer": "A",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 25,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Apenas aumentando a memória do cluster",
      "B": "Forçando o uso de cache em todas as queries",
      "C": "Removendo dados desnecessários automaticamente",
      "D": "Através de análise de padrões de execução e recomendações de índices"
    },
    "correctAnswer": "D",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 26,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Dashboards estáticos",
      "B": "Apenas logs de erro",
      "C": "Query Profile e Spark UI",
      "D": "Documentação de referência"
    },
    "correctAnswer": "C",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 27,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Sempre usa o maior cluster disponível",
      "B": "Apenas baseado em tamanho de dados",
      "C": "Através de análise de padrões de workload e recomendações automáticas",
      "D": "Aleatoriamente entre opções disponíveis"
    },
    "correctAnswer": "C",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 28,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Apenas performance de leitura",
      "B": "Simplificar decisões de particionamento e organização de dados",
      "C": "Apenas segurança",
      "D": "Apenas documentação"
    },
    "correctAnswer": "B",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 29,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Através de análise de padrões de execução e recomendações de índices",
      "B": "Apenas aumentando a memória do cluster",
      "C": "Forçando o uso de cache em todas as queries",
      "D": "Removendo dados desnecessários automaticamente"
    },
    "correctAnswer": "A",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 30,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Dashboards estáticos",
      "B": "Apenas logs de erro",
      "C": "Documentação de referência",
      "D": "Query Profile e Spark UI"
    },
    "correctAnswer": "D",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 31,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Sempre usa o maior cluster disponível",
      "B": "Através de análise de padrões de workload e recomendações automáticas",
      "C": "Apenas baseado em tamanho de dados",
      "D": "Aleatoriamente entre opções disponíveis"
    },
    "correctAnswer": "B",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 32,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Apenas segurança",
      "B": "Apenas performance de leitura",
      "C": "Apenas documentação",
      "D": "Simplificar decisões de particionamento e organização de dados"
    },
    "correctAnswer": "D",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 33,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Removendo dados desnecessários automaticamente",
      "B": "Através de análise de padrões de execução e recomendações de índices",
      "C": "Forçando o uso de cache em todas as queries",
      "D": "Apenas aumentando a memória do cluster"
    },
    "correctAnswer": "B",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 34,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Documentação de referência",
      "B": "Dashboards estáticos",
      "C": "Query Profile e Spark UI",
      "D": "Apenas logs de erro"
    },
    "correctAnswer": "C",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 35,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Apenas baseado em tamanho de dados",
      "B": "Através de análise de padrões de workload e recomendações automáticas",
      "C": "Sempre usa o maior cluster disponível",
      "D": "Aleatoriamente entre opções disponíveis"
    },
    "correctAnswer": "B",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 36,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Apenas segurança",
      "B": "Apenas documentação",
      "C": "Simplificar decisões de particionamento e organização de dados",
      "D": "Apenas performance de leitura"
    },
    "correctAnswer": "C",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 37,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Removendo dados desnecessários automaticamente",
      "B": "Através de análise de padrões de execução e recomendações de índices",
      "C": "Forçando o uso de cache em todas as queries",
      "D": "Apenas aumentando a memória do cluster"
    },
    "correctAnswer": "B",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 38,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Dashboards estáticos",
      "B": "Apenas logs de erro",
      "C": "Query Profile e Spark UI",
      "D": "Documentação de referência"
    },
    "correctAnswer": "C",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 39,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Sempre usa o maior cluster disponível",
      "B": "Apenas baseado em tamanho de dados",
      "C": "Aleatoriamente entre opções disponíveis",
      "D": "Através de análise de padrões de workload e recomendações automáticas"
    },
    "correctAnswer": "D",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 40,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Simplificar decisões de particionamento e organização de dados",
      "B": "Apenas segurança",
      "C": "Apenas performance de leitura",
      "D": "Apenas documentação"
    },
    "correctAnswer": "A",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 41,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Forçando o uso de cache em todas as queries",
      "B": "Através de análise de padrões de execução e recomendações de índices",
      "C": "Removendo dados desnecessários automaticamente",
      "D": "Apenas aumentando a memória do cluster"
    },
    "correctAnswer": "B",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 42,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Apenas logs de erro",
      "B": "Documentação de referência",
      "C": "Query Profile e Spark UI",
      "D": "Dashboards estáticos"
    },
    "correctAnswer": "C",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 43,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Através de análise de padrões de workload e recomendações automáticas",
      "B": "Apenas baseado em tamanho de dados",
      "C": "Aleatoriamente entre opções disponíveis",
      "D": "Sempre usa o maior cluster disponível"
    },
    "correctAnswer": "A",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 44,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Apenas segurança",
      "B": "Simplificar decisões de particionamento e organização de dados",
      "C": "Apenas performance de leitura",
      "D": "Apenas documentação"
    },
    "correctAnswer": "B",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 45,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Removendo dados desnecessários automaticamente",
      "B": "Apenas aumentando a memória do cluster",
      "C": "Forçando o uso de cache em todas as queries",
      "D": "Através de análise de padrões de execução e recomendações de índices"
    },
    "correctAnswer": "D",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 46,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Apenas logs de erro",
      "B": "Documentação de referência",
      "C": "Dashboards estáticos",
      "D": "Query Profile e Spark UI"
    },
    "correctAnswer": "D",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 47,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Apenas baseado em tamanho de dados",
      "B": "Aleatoriamente entre opções disponíveis",
      "C": "Sempre usa o maior cluster disponível",
      "D": "Através de análise de padrões de workload e recomendações automáticas"
    },
    "correctAnswer": "D",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 48,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Apenas documentação",
      "B": "Apenas performance de leitura",
      "C": "Simplificar decisões de particionamento e organização de dados",
      "D": "Apenas segurança"
    },
    "correctAnswer": "C",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 49,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Removendo dados desnecessários automaticamente",
      "B": "Forçando o uso de cache em todas as queries",
      "C": "Apenas aumentando a memória do cluster",
      "D": "Através de análise de padrões de execução e recomendações de índices"
    },
    "correctAnswer": "D",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 50,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Dashboards estáticos",
      "B": "Documentação de referência",
      "C": "Query Profile e Spark UI",
      "D": "Apenas logs de erro"
    },
    "correctAnswer": "C",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 51,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Apenas baseado em tamanho de dados",
      "B": "Através de análise de padrões de workload e recomendações automáticas",
      "C": "Aleatoriamente entre opções disponíveis",
      "D": "Sempre usa o maior cluster disponível"
    },
    "correctAnswer": "B",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 52,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Apenas performance de leitura",
      "B": "Apenas documentação",
      "C": "Simplificar decisões de particionamento e organização de dados",
      "D": "Apenas segurança"
    },
    "correctAnswer": "C",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 53,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Forçando o uso de cache em todas as queries",
      "B": "Apenas aumentando a memória do cluster",
      "C": "Removendo dados desnecessários automaticamente",
      "D": "Através de análise de padrões de execução e recomendações de índices"
    },
    "correctAnswer": "D",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 54,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Documentação de referência",
      "B": "Dashboards estáticos",
      "C": "Query Profile e Spark UI",
      "D": "Apenas logs de erro"
    },
    "correctAnswer": "C",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 55,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Sempre usa o maior cluster disponível",
      "B": "Aleatoriamente entre opções disponíveis",
      "C": "Apenas baseado em tamanho de dados",
      "D": "Através de análise de padrões de workload e recomendações automáticas"
    },
    "correctAnswer": "D",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 56,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Apenas documentação",
      "B": "Apenas performance de leitura",
      "C": "Apenas segurança",
      "D": "Simplificar decisões de particionamento e organização de dados"
    },
    "correctAnswer": "D",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 57,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Apenas aumentando a memória do cluster",
      "B": "Forçando o uso de cache em todas as queries",
      "C": "Removendo dados desnecessários automaticamente",
      "D": "Através de análise de padrões de execução e recomendações de índices"
    },
    "correctAnswer": "D",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 58,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Query Profile e Spark UI",
      "B": "Apenas logs de erro",
      "C": "Dashboards estáticos",
      "D": "Documentação de referência"
    },
    "correctAnswer": "A",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 59,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Através de análise de padrões de workload e recomendações automáticas",
      "B": "Apenas baseado em tamanho de dados",
      "C": "Aleatoriamente entre opções disponíveis",
      "D": "Sempre usa o maior cluster disponível"
    },
    "correctAnswer": "A",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 60,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Apenas segurança",
      "B": "Apenas documentação",
      "C": "Simplificar decisões de particionamento e organização de dados",
      "D": "Apenas performance de leitura"
    },
    "correctAnswer": "C",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 61,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Através de análise de padrões de execução e recomendações de índices",
      "B": "Apenas aumentando a memória do cluster",
      "C": "Removendo dados desnecessários automaticamente",
      "D": "Forçando o uso de cache em todas as queries"
    },
    "correctAnswer": "A",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 62,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Documentação de referência",
      "B": "Query Profile e Spark UI",
      "C": "Dashboards estáticos",
      "D": "Apenas logs de erro"
    },
    "correctAnswer": "B",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 63,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Através de análise de padrões de workload e recomendações automáticas",
      "B": "Sempre usa o maior cluster disponível",
      "C": "Aleatoriamente entre opções disponíveis",
      "D": "Apenas baseado em tamanho de dados"
    },
    "correctAnswer": "A",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 64,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Apenas segurança",
      "B": "Apenas performance de leitura",
      "C": "Simplificar decisões de particionamento e organização de dados",
      "D": "Apenas documentação"
    },
    "correctAnswer": "C",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 65,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Apenas aumentando a memória do cluster",
      "B": "Forçando o uso de cache em todas as queries",
      "C": "Removendo dados desnecessários automaticamente",
      "D": "Através de análise de padrões de execução e recomendações de índices"
    },
    "correctAnswer": "D",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 66,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Documentação de referência",
      "B": "Query Profile e Spark UI",
      "C": "Dashboards estáticos",
      "D": "Apenas logs de erro"
    },
    "correctAnswer": "B",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 67,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Aleatoriamente entre opções disponíveis",
      "B": "Apenas baseado em tamanho de dados",
      "C": "Sempre usa o maior cluster disponível",
      "D": "Através de análise de padrões de workload e recomendações automáticas"
    },
    "correctAnswer": "D",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 68,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Simplificar decisões de particionamento e organização de dados",
      "B": "Apenas performance de leitura",
      "C": "Apenas documentação",
      "D": "Apenas segurança"
    },
    "correctAnswer": "A",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 69,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Forçando o uso de cache em todas as queries",
      "B": "Removendo dados desnecessários automaticamente",
      "C": "Através de análise de padrões de execução e recomendações de índices",
      "D": "Apenas aumentando a memória do cluster"
    },
    "correctAnswer": "C",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 70,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Apenas logs de erro",
      "B": "Documentação de referência",
      "C": "Query Profile e Spark UI",
      "D": "Dashboards estáticos"
    },
    "correctAnswer": "C",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 71,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Aleatoriamente entre opções disponíveis",
      "B": "Através de análise de padrões de workload e recomendações automáticas",
      "C": "Apenas baseado em tamanho de dados",
      "D": "Sempre usa o maior cluster disponível"
    },
    "correctAnswer": "B",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 72,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Simplificar decisões de particionamento e organização de dados",
      "B": "Apenas documentação",
      "C": "Apenas segurança",
      "D": "Apenas performance de leitura"
    },
    "correctAnswer": "A",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 73,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Através de análise de padrões de execução e recomendações de índices",
      "B": "Apenas aumentando a memória do cluster",
      "C": "Removendo dados desnecessários automaticamente",
      "D": "Forçando o uso de cache em todas as queries"
    },
    "correctAnswer": "A",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 74,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Dashboards estáticos",
      "B": "Query Profile e Spark UI",
      "C": "Documentação de referência",
      "D": "Apenas logs de erro"
    },
    "correctAnswer": "B",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 75,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Apenas baseado em tamanho de dados",
      "B": "Sempre usa o maior cluster disponível",
      "C": "Aleatoriamente entre opções disponíveis",
      "D": "Através de análise de padrões de workload e recomendações automáticas"
    },
    "correctAnswer": "D",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 76,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Simplificar decisões de particionamento e organização de dados",
      "B": "Apenas documentação",
      "C": "Apenas segurança",
      "D": "Apenas performance de leitura"
    },
    "correctAnswer": "A",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 77,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Forçando o uso de cache em todas as queries",
      "B": "Apenas aumentando a memória do cluster",
      "C": "Removendo dados desnecessários automaticamente",
      "D": "Através de análise de padrões de execução e recomendações de índices"
    },
    "correctAnswer": "D",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 78,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Documentação de referência",
      "B": "Apenas logs de erro",
      "C": "Query Profile e Spark UI",
      "D": "Dashboards estáticos"
    },
    "correctAnswer": "C",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 79,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Sempre usa o maior cluster disponível",
      "B": "Através de análise de padrões de workload e recomendações automáticas",
      "C": "Aleatoriamente entre opções disponíveis",
      "D": "Apenas baseado em tamanho de dados"
    },
    "correctAnswer": "B",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 80,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Apenas performance de leitura",
      "B": "Simplificar decisões de particionamento e organização de dados",
      "C": "Apenas documentação",
      "D": "Apenas segurança"
    },
    "correctAnswer": "B",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 81,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Forçando o uso de cache em todas as queries",
      "B": "Através de análise de padrões de execução e recomendações de índices",
      "C": "Apenas aumentando a memória do cluster",
      "D": "Removendo dados desnecessários automaticamente"
    },
    "correctAnswer": "B",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 82,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Dashboards estáticos",
      "B": "Apenas logs de erro",
      "C": "Documentação de referência",
      "D": "Query Profile e Spark UI"
    },
    "correctAnswer": "D",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 83,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Apenas baseado em tamanho de dados",
      "B": "Através de análise de padrões de workload e recomendações automáticas",
      "C": "Sempre usa o maior cluster disponível",
      "D": "Aleatoriamente entre opções disponíveis"
    },
    "correctAnswer": "B",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 84,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Apenas documentação",
      "B": "Simplificar decisões de particionamento e organização de dados",
      "C": "Apenas segurança",
      "D": "Apenas performance de leitura"
    },
    "correctAnswer": "B",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 85,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Forçando o uso de cache em todas as queries",
      "B": "Através de análise de padrões de execução e recomendações de índices",
      "C": "Removendo dados desnecessários automaticamente",
      "D": "Apenas aumentando a memória do cluster"
    },
    "correctAnswer": "B",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 86,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Dashboards estáticos",
      "B": "Query Profile e Spark UI",
      "C": "Apenas logs de erro",
      "D": "Documentação de referência"
    },
    "correctAnswer": "B",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 87,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Sempre usa o maior cluster disponível",
      "B": "Aleatoriamente entre opções disponíveis",
      "C": "Através de análise de padrões de workload e recomendações automáticas",
      "D": "Apenas baseado em tamanho de dados"
    },
    "correctAnswer": "C",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 88,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Simplificar decisões de particionamento e organização de dados",
      "B": "Apenas segurança",
      "C": "Apenas performance de leitura",
      "D": "Apenas documentação"
    },
    "correctAnswer": "A",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 89,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Apenas aumentando a memória do cluster",
      "B": "Através de análise de padrões de execução e recomendações de índices",
      "C": "Forçando o uso de cache em todas as queries",
      "D": "Removendo dados desnecessários automaticamente"
    },
    "correctAnswer": "B",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 90,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Dashboards estáticos",
      "B": "Apenas logs de erro",
      "C": "Query Profile e Spark UI",
      "D": "Documentação de referência"
    },
    "correctAnswer": "C",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 91,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Aleatoriamente entre opções disponíveis",
      "B": "Através de análise de padrões de workload e recomendações automáticas",
      "C": "Sempre usa o maior cluster disponível",
      "D": "Apenas baseado em tamanho de dados"
    },
    "correctAnswer": "B",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 92,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Simplificar decisões de particionamento e organização de dados",
      "B": "Apenas segurança",
      "C": "Apenas performance de leitura",
      "D": "Apenas documentação"
    },
    "correctAnswer": "A",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 93,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Forçando o uso de cache em todas as queries",
      "B": "Removendo dados desnecessários automaticamente",
      "C": "Apenas aumentando a memória do cluster",
      "D": "Através de análise de padrões de execução e recomendações de índices"
    },
    "correctAnswer": "D",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 94,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Query Profile e Spark UI",
      "B": "Dashboards estáticos",
      "C": "Documentação de referência",
      "D": "Apenas logs de erro"
    },
    "correctAnswer": "A",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 95,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Sempre usa o maior cluster disponível",
      "B": "Aleatoriamente entre opções disponíveis",
      "C": "Através de análise de padrões de workload e recomendações automáticas",
      "D": "Apenas baseado em tamanho de dados"
    },
    "correctAnswer": "C",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 96,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Simplificar decisões de particionamento e organização de dados",
      "B": "Apenas documentação",
      "C": "Apenas performance de leitura",
      "D": "Apenas segurança"
    },
    "correctAnswer": "A",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 97,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Removendo dados desnecessários automaticamente",
      "B": "Através de análise de padrões de execução e recomendações de índices",
      "C": "Apenas aumentando a memória do cluster",
      "D": "Forçando o uso de cache em todas as queries"
    },
    "correctAnswer": "B",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 98,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Dashboards estáticos",
      "B": "Documentação de referência",
      "C": "Query Profile e Spark UI",
      "D": "Apenas logs de erro"
    },
    "correctAnswer": "C",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 99,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Sempre usa o maior cluster disponível",
      "B": "Apenas baseado em tamanho de dados",
      "C": "Através de análise de padrões de workload e recomendações automáticas",
      "D": "Aleatoriamente entre opções disponíveis"
    },
    "correctAnswer": "C",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 100,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Apenas segurança",
      "B": "Simplificar decisões de particionamento e organização de dados",
      "C": "Apenas documentação",
      "D": "Apenas performance de leitura"
    },
    "correctAnswer": "B",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 101,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Removendo dados desnecessários automaticamente",
      "B": "Através de análise de padrões de execução e recomendações de índices",
      "C": "Forçando o uso de cache em todas as queries",
      "D": "Apenas aumentando a memória do cluster"
    },
    "correctAnswer": "B",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 102,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Apenas logs de erro",
      "B": "Dashboards estáticos",
      "C": "Documentação de referência",
      "D": "Query Profile e Spark UI"
    },
    "correctAnswer": "D",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 103,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Apenas baseado em tamanho de dados",
      "B": "Aleatoriamente entre opções disponíveis",
      "C": "Sempre usa o maior cluster disponível",
      "D": "Através de análise de padrões de workload e recomendações automáticas"
    },
    "correctAnswer": "D",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 104,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Apenas performance de leitura",
      "B": "Simplificar decisões de particionamento e organização de dados",
      "C": "Apenas documentação",
      "D": "Apenas segurança"
    },
    "correctAnswer": "B",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 105,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Removendo dados desnecessários automaticamente",
      "B": "Apenas aumentando a memória do cluster",
      "C": "Através de análise de padrões de execução e recomendações de índices",
      "D": "Forçando o uso de cache em todas as queries"
    },
    "correctAnswer": "C",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 106,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Apenas logs de erro",
      "B": "Query Profile e Spark UI",
      "C": "Dashboards estáticos",
      "D": "Documentação de referência"
    },
    "correctAnswer": "B",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 107,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Sempre usa o maior cluster disponível",
      "B": "Através de análise de padrões de workload e recomendações automáticas",
      "C": "Aleatoriamente entre opções disponíveis",
      "D": "Apenas baseado em tamanho de dados"
    },
    "correctAnswer": "B",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 108,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Apenas performance de leitura",
      "B": "Apenas documentação",
      "C": "Simplificar decisões de particionamento e organização de dados",
      "D": "Apenas segurança"
    },
    "correctAnswer": "C",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 109,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Através de análise de padrões de execução e recomendações de índices",
      "B": "Apenas aumentando a memória do cluster",
      "C": "Forçando o uso de cache em todas as queries",
      "D": "Removendo dados desnecessários automaticamente"
    },
    "correctAnswer": "A",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 110,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Apenas logs de erro",
      "B": "Query Profile e Spark UI",
      "C": "Dashboards estáticos",
      "D": "Documentação de referência"
    },
    "correctAnswer": "B",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 111,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Apenas baseado em tamanho de dados",
      "B": "Sempre usa o maior cluster disponível",
      "C": "Aleatoriamente entre opções disponíveis",
      "D": "Através de análise de padrões de workload e recomendações automáticas"
    },
    "correctAnswer": "D",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 112,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Simplificar decisões de particionamento e organização de dados",
      "B": "Apenas performance de leitura",
      "C": "Apenas documentação",
      "D": "Apenas segurança"
    },
    "correctAnswer": "A",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 113,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Removendo dados desnecessários automaticamente",
      "B": "Através de análise de padrões de execução e recomendações de índices",
      "C": "Apenas aumentando a memória do cluster",
      "D": "Forçando o uso de cache em todas as queries"
    },
    "correctAnswer": "B",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 114,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Query Profile e Spark UI",
      "B": "Dashboards estáticos",
      "C": "Documentação de referência",
      "D": "Apenas logs de erro"
    },
    "correctAnswer": "A",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 115,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Sempre usa o maior cluster disponível",
      "B": "Aleatoriamente entre opções disponíveis",
      "C": "Apenas baseado em tamanho de dados",
      "D": "Através de análise de padrões de workload e recomendações automáticas"
    },
    "correctAnswer": "D",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 116,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Simplificar decisões de particionamento e organização de dados",
      "B": "Apenas segurança",
      "C": "Apenas documentação",
      "D": "Apenas performance de leitura"
    },
    "correctAnswer": "A",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 117,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Forçando o uso de cache em todas as queries",
      "B": "Apenas aumentando a memória do cluster",
      "C": "Removendo dados desnecessários automaticamente",
      "D": "Através de análise de padrões de execução e recomendações de índices"
    },
    "correctAnswer": "D",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 118,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Dashboards estáticos",
      "B": "Query Profile e Spark UI",
      "C": "Apenas logs de erro",
      "D": "Documentação de referência"
    },
    "correctAnswer": "B",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 119,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Sempre usa o maior cluster disponível",
      "B": "Aleatoriamente entre opções disponíveis",
      "C": "Apenas baseado em tamanho de dados",
      "D": "Através de análise de padrões de workload e recomendações automáticas"
    },
    "correctAnswer": "D",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 120,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Apenas segurança",
      "B": "Simplificar decisões de particionamento e organização de dados",
      "C": "Apenas performance de leitura",
      "D": "Apenas documentação"
    },
    "correctAnswer": "B",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 121,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Através de análise de padrões de execução e recomendações de índices",
      "B": "Forçando o uso de cache em todas as queries",
      "C": "Removendo dados desnecessários automaticamente",
      "D": "Apenas aumentando a memória do cluster"
    },
    "correctAnswer": "A",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 122,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Apenas logs de erro",
      "B": "Documentação de referência",
      "C": "Dashboards estáticos",
      "D": "Query Profile e Spark UI"
    },
    "correctAnswer": "D",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 123,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Aleatoriamente entre opções disponíveis",
      "B": "Através de análise de padrões de workload e recomendações automáticas",
      "C": "Apenas baseado em tamanho de dados",
      "D": "Sempre usa o maior cluster disponível"
    },
    "correctAnswer": "B",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 124,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Simplificar decisões de particionamento e organização de dados",
      "B": "Apenas segurança",
      "C": "Apenas documentação",
      "D": "Apenas performance de leitura"
    },
    "correctAnswer": "A",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 125,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Apenas aumentando a memória do cluster",
      "B": "Através de análise de padrões de execução e recomendações de índices",
      "C": "Forçando o uso de cache em todas as queries",
      "D": "Removendo dados desnecessários automaticamente"
    },
    "correctAnswer": "B",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 126,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Dashboards estáticos",
      "B": "Query Profile e Spark UI",
      "C": "Documentação de referência",
      "D": "Apenas logs de erro"
    },
    "correctAnswer": "B",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 127,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Apenas baseado em tamanho de dados",
      "B": "Através de análise de padrões de workload e recomendações automáticas",
      "C": "Sempre usa o maior cluster disponível",
      "D": "Aleatoriamente entre opções disponíveis"
    },
    "correctAnswer": "B",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 128,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Apenas segurança",
      "B": "Apenas performance de leitura",
      "C": "Apenas documentação",
      "D": "Simplificar decisões de particionamento e organização de dados"
    },
    "correctAnswer": "D",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 129,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Forçando o uso de cache em todas as queries",
      "B": "Apenas aumentando a memória do cluster",
      "C": "Através de análise de padrões de execução e recomendações de índices",
      "D": "Removendo dados desnecessários automaticamente"
    },
    "correctAnswer": "C",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 130,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Query Profile e Spark UI",
      "B": "Documentação de referência",
      "C": "Apenas logs de erro",
      "D": "Dashboards estáticos"
    },
    "correctAnswer": "A",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 131,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Sempre usa o maior cluster disponível",
      "B": "Aleatoriamente entre opções disponíveis",
      "C": "Apenas baseado em tamanho de dados",
      "D": "Através de análise de padrões de workload e recomendações automáticas"
    },
    "correctAnswer": "D",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 132,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Apenas performance de leitura",
      "B": "Apenas documentação",
      "C": "Simplificar decisões de particionamento e organização de dados",
      "D": "Apenas segurança"
    },
    "correctAnswer": "C",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 133,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Removendo dados desnecessários automaticamente",
      "B": "Apenas aumentando a memória do cluster",
      "C": "Através de análise de padrões de execução e recomendações de índices",
      "D": "Forçando o uso de cache em todas as queries"
    },
    "correctAnswer": "C",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 134,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Documentação de referência",
      "B": "Dashboards estáticos",
      "C": "Apenas logs de erro",
      "D": "Query Profile e Spark UI"
    },
    "correctAnswer": "D",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 135,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Apenas baseado em tamanho de dados",
      "B": "Sempre usa o maior cluster disponível",
      "C": "Através de análise de padrões de workload e recomendações automáticas",
      "D": "Aleatoriamente entre opções disponíveis"
    },
    "correctAnswer": "C",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 136,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Apenas segurança",
      "B": "Apenas performance de leitura",
      "C": "Apenas documentação",
      "D": "Simplificar decisões de particionamento e organização de dados"
    },
    "correctAnswer": "D",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 137,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Removendo dados desnecessários automaticamente",
      "B": "Através de análise de padrões de execução e recomendações de índices",
      "C": "Apenas aumentando a memória do cluster",
      "D": "Forçando o uso de cache em todas as queries"
    },
    "correctAnswer": "B",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 138,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Documentação de referência",
      "B": "Apenas logs de erro",
      "C": "Query Profile e Spark UI",
      "D": "Dashboards estáticos"
    },
    "correctAnswer": "C",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 139,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Aleatoriamente entre opções disponíveis",
      "B": "Apenas baseado em tamanho de dados",
      "C": "Sempre usa o maior cluster disponível",
      "D": "Através de análise de padrões de workload e recomendações automáticas"
    },
    "correctAnswer": "D",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 140,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Apenas performance de leitura",
      "B": "Apenas segurança",
      "C": "Apenas documentação",
      "D": "Simplificar decisões de particionamento e organização de dados"
    },
    "correctAnswer": "D",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 141,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Apenas aumentando a memória do cluster",
      "B": "Através de análise de padrões de execução e recomendações de índices",
      "C": "Forçando o uso de cache em todas as queries",
      "D": "Removendo dados desnecessários automaticamente"
    },
    "correctAnswer": "B",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 142,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Apenas logs de erro",
      "B": "Query Profile e Spark UI",
      "C": "Documentação de referência",
      "D": "Dashboards estáticos"
    },
    "correctAnswer": "B",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 143,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Apenas baseado em tamanho de dados",
      "B": "Sempre usa o maior cluster disponível",
      "C": "Através de análise de padrões de workload e recomendações automáticas",
      "D": "Aleatoriamente entre opções disponíveis"
    },
    "correctAnswer": "C",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 144,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Simplificar decisões de particionamento e organização de dados",
      "B": "Apenas performance de leitura",
      "C": "Apenas documentação",
      "D": "Apenas segurança"
    },
    "correctAnswer": "A",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 145,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Através de análise de padrões de execução e recomendações de índices",
      "B": "Forçando o uso de cache em todas as queries",
      "C": "Apenas aumentando a memória do cluster",
      "D": "Removendo dados desnecessários automaticamente"
    },
    "correctAnswer": "A",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 146,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Apenas logs de erro",
      "B": "Documentação de referência",
      "C": "Dashboards estáticos",
      "D": "Query Profile e Spark UI"
    },
    "correctAnswer": "D",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 147,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Através de análise de padrões de workload e recomendações automáticas",
      "B": "Aleatoriamente entre opções disponíveis",
      "C": "Sempre usa o maior cluster disponível",
      "D": "Apenas baseado em tamanho de dados"
    },
    "correctAnswer": "A",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 148,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Apenas segurança",
      "B": "Simplificar decisões de particionamento e organização de dados",
      "C": "Apenas performance de leitura",
      "D": "Apenas documentação"
    },
    "correctAnswer": "B",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 149,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Forçando o uso de cache em todas as queries",
      "B": "Através de análise de padrões de execução e recomendações de índices",
      "C": "Apenas aumentando a memória do cluster",
      "D": "Removendo dados desnecessários automaticamente"
    },
    "correctAnswer": "B",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 150,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Query Profile e Spark UI",
      "B": "Apenas logs de erro",
      "C": "Documentação de referência",
      "D": "Dashboards estáticos"
    },
    "correctAnswer": "A",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 151,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Sempre usa o maior cluster disponível",
      "B": "Apenas baseado em tamanho de dados",
      "C": "Através de análise de padrões de workload e recomendações automáticas",
      "D": "Aleatoriamente entre opções disponíveis"
    },
    "correctAnswer": "C",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 152,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Apenas documentação",
      "B": "Apenas segurança",
      "C": "Apenas performance de leitura",
      "D": "Simplificar decisões de particionamento e organização de dados"
    },
    "correctAnswer": "D",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 153,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Forçando o uso de cache em todas as queries",
      "B": "Removendo dados desnecessários automaticamente",
      "C": "Através de análise de padrões de execução e recomendações de índices",
      "D": "Apenas aumentando a memória do cluster"
    },
    "correctAnswer": "C",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 154,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Documentação de referência",
      "B": "Apenas logs de erro",
      "C": "Query Profile e Spark UI",
      "D": "Dashboards estáticos"
    },
    "correctAnswer": "C",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 155,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Através de análise de padrões de workload e recomendações automáticas",
      "B": "Aleatoriamente entre opções disponíveis",
      "C": "Apenas baseado em tamanho de dados",
      "D": "Sempre usa o maior cluster disponível"
    },
    "correctAnswer": "A",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 156,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Apenas performance de leitura",
      "B": "Simplificar decisões de particionamento e organização de dados",
      "C": "Apenas documentação",
      "D": "Apenas segurança"
    },
    "correctAnswer": "B",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 157,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Forçando o uso de cache em todas as queries",
      "B": "Através de análise de padrões de execução e recomendações de índices",
      "C": "Apenas aumentando a memória do cluster",
      "D": "Removendo dados desnecessários automaticamente"
    },
    "correctAnswer": "B",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 158,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Apenas logs de erro",
      "B": "Dashboards estáticos",
      "C": "Query Profile e Spark UI",
      "D": "Documentação de referência"
    },
    "correctAnswer": "C",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 159,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Apenas baseado em tamanho de dados",
      "B": "Aleatoriamente entre opções disponíveis",
      "C": "Sempre usa o maior cluster disponível",
      "D": "Através de análise de padrões de workload e recomendações automáticas"
    },
    "correctAnswer": "D",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 160,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Apenas segurança",
      "B": "Apenas documentação",
      "C": "Apenas performance de leitura",
      "D": "Simplificar decisões de particionamento e organização de dados"
    },
    "correctAnswer": "D",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 161,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Removendo dados desnecessários automaticamente",
      "B": "Através de análise de padrões de execução e recomendações de índices",
      "C": "Apenas aumentando a memória do cluster",
      "D": "Forçando o uso de cache em todas as queries"
    },
    "correctAnswer": "B",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 162,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Documentação de referência",
      "B": "Query Profile e Spark UI",
      "C": "Apenas logs de erro",
      "D": "Dashboards estáticos"
    },
    "correctAnswer": "B",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 163,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Apenas baseado em tamanho de dados",
      "B": "Sempre usa o maior cluster disponível",
      "C": "Através de análise de padrões de workload e recomendações automáticas",
      "D": "Aleatoriamente entre opções disponíveis"
    },
    "correctAnswer": "C",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 164,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Apenas documentação",
      "B": "Apenas performance de leitura",
      "C": "Apenas segurança",
      "D": "Simplificar decisões de particionamento e organização de dados"
    },
    "correctAnswer": "D",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 165,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Apenas aumentando a memória do cluster",
      "B": "Forçando o uso de cache em todas as queries",
      "C": "Através de análise de padrões de execução e recomendações de índices",
      "D": "Removendo dados desnecessários automaticamente"
    },
    "correctAnswer": "C",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 166,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Apenas logs de erro",
      "B": "Query Profile e Spark UI",
      "C": "Documentação de referência",
      "D": "Dashboards estáticos"
    },
    "correctAnswer": "B",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 167,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Sempre usa o maior cluster disponível",
      "B": "Aleatoriamente entre opções disponíveis",
      "C": "Através de análise de padrões de workload e recomendações automáticas",
      "D": "Apenas baseado em tamanho de dados"
    },
    "correctAnswer": "C",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 168,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Apenas performance de leitura",
      "B": "Apenas documentação",
      "C": "Simplificar decisões de particionamento e organização de dados",
      "D": "Apenas segurança"
    },
    "correctAnswer": "C",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 169,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Forçando o uso de cache em todas as queries",
      "B": "Através de análise de padrões de execução e recomendações de índices",
      "C": "Apenas aumentando a memória do cluster",
      "D": "Removendo dados desnecessários automaticamente"
    },
    "correctAnswer": "B",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 170,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Dashboards estáticos",
      "B": "Apenas logs de erro",
      "C": "Query Profile e Spark UI",
      "D": "Documentação de referência"
    },
    "correctAnswer": "C",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 171,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Através de análise de padrões de workload e recomendações automáticas",
      "B": "Aleatoriamente entre opções disponíveis",
      "C": "Sempre usa o maior cluster disponível",
      "D": "Apenas baseado em tamanho de dados"
    },
    "correctAnswer": "A",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 172,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Apenas documentação",
      "B": "Apenas segurança",
      "C": "Simplificar decisões de particionamento e organização de dados",
      "D": "Apenas performance de leitura"
    },
    "correctAnswer": "C",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 173,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Apenas aumentando a memória do cluster",
      "B": "Forçando o uso de cache em todas as queries",
      "C": "Removendo dados desnecessários automaticamente",
      "D": "Através de análise de padrões de execução e recomendações de índices"
    },
    "correctAnswer": "D",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 174,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Documentação de referência",
      "B": "Dashboards estáticos",
      "C": "Apenas logs de erro",
      "D": "Query Profile e Spark UI"
    },
    "correctAnswer": "D",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 175,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Aleatoriamente entre opções disponíveis",
      "B": "Apenas baseado em tamanho de dados",
      "C": "Sempre usa o maior cluster disponível",
      "D": "Através de análise de padrões de workload e recomendações automáticas"
    },
    "correctAnswer": "D",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 176,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Simplificar decisões de particionamento e organização de dados",
      "B": "Apenas segurança",
      "C": "Apenas performance de leitura",
      "D": "Apenas documentação"
    },
    "correctAnswer": "A",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 177,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Removendo dados desnecessários automaticamente",
      "B": "Através de análise de padrões de execução e recomendações de índices",
      "C": "Forçando o uso de cache em todas as queries",
      "D": "Apenas aumentando a memória do cluster"
    },
    "correctAnswer": "B",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 178,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Apenas logs de erro",
      "B": "Dashboards estáticos",
      "C": "Query Profile e Spark UI",
      "D": "Documentação de referência"
    },
    "correctAnswer": "C",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 179,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Aleatoriamente entre opções disponíveis",
      "B": "Apenas baseado em tamanho de dados",
      "C": "Através de análise de padrões de workload e recomendações automáticas",
      "D": "Sempre usa o maior cluster disponível"
    },
    "correctAnswer": "C",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 180,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Apenas performance de leitura",
      "B": "Apenas documentação",
      "C": "Simplificar decisões de particionamento e organização de dados",
      "D": "Apenas segurança"
    },
    "correctAnswer": "C",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 181,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Forçando o uso de cache em todas as queries",
      "B": "Apenas aumentando a memória do cluster",
      "C": "Através de análise de padrões de execução e recomendações de índices",
      "D": "Removendo dados desnecessários automaticamente"
    },
    "correctAnswer": "C",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 182,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Query Profile e Spark UI",
      "B": "Apenas logs de erro",
      "C": "Documentação de referência",
      "D": "Dashboards estáticos"
    },
    "correctAnswer": "A",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 183,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Apenas baseado em tamanho de dados",
      "B": "Aleatoriamente entre opções disponíveis",
      "C": "Sempre usa o maior cluster disponível",
      "D": "Através de análise de padrões de workload e recomendações automáticas"
    },
    "correctAnswer": "D",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 184,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Simplificar decisões de particionamento e organização de dados",
      "B": "Apenas segurança",
      "C": "Apenas documentação",
      "D": "Apenas performance de leitura"
    },
    "correctAnswer": "A",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 185,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Apenas aumentando a memória do cluster",
      "B": "Através de análise de padrões de execução e recomendações de índices",
      "C": "Forçando o uso de cache em todas as queries",
      "D": "Removendo dados desnecessários automaticamente"
    },
    "correctAnswer": "B",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 186,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Query Profile e Spark UI",
      "B": "Apenas logs de erro",
      "C": "Dashboards estáticos",
      "D": "Documentação de referência"
    },
    "correctAnswer": "A",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 187,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Sempre usa o maior cluster disponível",
      "B": "Aleatoriamente entre opções disponíveis",
      "C": "Apenas baseado em tamanho de dados",
      "D": "Através de análise de padrões de workload e recomendações automáticas"
    },
    "correctAnswer": "D",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 188,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Apenas documentação",
      "B": "Apenas performance de leitura",
      "C": "Apenas segurança",
      "D": "Simplificar decisões de particionamento e organização de dados"
    },
    "correctAnswer": "D",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 189,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Removendo dados desnecessários automaticamente",
      "B": "Forçando o uso de cache em todas as queries",
      "C": "Através de análise de padrões de execução e recomendações de índices",
      "D": "Apenas aumentando a memória do cluster"
    },
    "correctAnswer": "C",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 190,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Apenas logs de erro",
      "B": "Documentação de referência",
      "C": "Dashboards estáticos",
      "D": "Query Profile e Spark UI"
    },
    "correctAnswer": "D",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 191,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Aleatoriamente entre opções disponíveis",
      "B": "Sempre usa o maior cluster disponível",
      "C": "Apenas baseado em tamanho de dados",
      "D": "Através de análise de padrões de workload e recomendações automáticas"
    },
    "correctAnswer": "D",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 192,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Apenas performance de leitura",
      "B": "Apenas documentação",
      "C": "Simplificar decisões de particionamento e organização de dados",
      "D": "Apenas segurança"
    },
    "correctAnswer": "C",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 193,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Removendo dados desnecessários automaticamente",
      "B": "Através de análise de padrões de execução e recomendações de índices",
      "C": "Forçando o uso de cache em todas as queries",
      "D": "Apenas aumentando a memória do cluster"
    },
    "correctAnswer": "B",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 194,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Apenas logs de erro",
      "B": "Query Profile e Spark UI",
      "C": "Dashboards estáticos",
      "D": "Documentação de referência"
    },
    "correctAnswer": "B",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 195,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Através de análise de padrões de workload e recomendações automáticas",
      "B": "Apenas baseado em tamanho de dados",
      "C": "Sempre usa o maior cluster disponível",
      "D": "Aleatoriamente entre opções disponíveis"
    },
    "correctAnswer": "A",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 196,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Simplificar decisões de particionamento e organização de dados",
      "B": "Apenas segurança",
      "C": "Apenas performance de leitura",
      "D": "Apenas documentação"
    },
    "correctAnswer": "A",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 197,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Forçando o uso de cache em todas as queries",
      "B": "Através de análise de padrões de execução e recomendações de índices",
      "C": "Removendo dados desnecessários automaticamente",
      "D": "Apenas aumentando a memória do cluster"
    },
    "correctAnswer": "B",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 198,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Apenas logs de erro",
      "B": "Query Profile e Spark UI",
      "C": "Documentação de referência",
      "D": "Dashboards estáticos"
    },
    "correctAnswer": "B",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 199,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Através de análise de padrões de workload e recomendações automáticas",
      "B": "Apenas baseado em tamanho de dados",
      "C": "Sempre usa o maior cluster disponível",
      "D": "Aleatoriamente entre opções disponíveis"
    },
    "correctAnswer": "A",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 200,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Apenas segurança",
      "B": "Apenas performance de leitura",
      "C": "Simplificar decisões de particionamento e organização de dados",
      "D": "Apenas documentação"
    },
    "correctAnswer": "C",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 201,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Através de análise de padrões de execução e recomendações de índices",
      "B": "Forçando o uso de cache em todas as queries",
      "C": "Apenas aumentando a memória do cluster",
      "D": "Removendo dados desnecessários automaticamente"
    },
    "correctAnswer": "A",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 202,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Apenas logs de erro",
      "B": "Dashboards estáticos",
      "C": "Documentação de referência",
      "D": "Query Profile e Spark UI"
    },
    "correctAnswer": "D",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 203,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Apenas baseado em tamanho de dados",
      "B": "Através de análise de padrões de workload e recomendações automáticas",
      "C": "Aleatoriamente entre opções disponíveis",
      "D": "Sempre usa o maior cluster disponível"
    },
    "correctAnswer": "B",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 204,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Apenas segurança",
      "B": "Apenas documentação",
      "C": "Simplificar decisões de particionamento e organização de dados",
      "D": "Apenas performance de leitura"
    },
    "correctAnswer": "C",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 205,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Apenas aumentando a memória do cluster",
      "B": "Removendo dados desnecessários automaticamente",
      "C": "Através de análise de padrões de execução e recomendações de índices",
      "D": "Forçando o uso de cache em todas as queries"
    },
    "correctAnswer": "C",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 206,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Documentação de referência",
      "B": "Dashboards estáticos",
      "C": "Query Profile e Spark UI",
      "D": "Apenas logs de erro"
    },
    "correctAnswer": "C",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 207,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Apenas baseado em tamanho de dados",
      "B": "Sempre usa o maior cluster disponível",
      "C": "Através de análise de padrões de workload e recomendações automáticas",
      "D": "Aleatoriamente entre opções disponíveis"
    },
    "correctAnswer": "C",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 208,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Apenas performance de leitura",
      "B": "Apenas documentação",
      "C": "Apenas segurança",
      "D": "Simplificar decisões de particionamento e organização de dados"
    },
    "correctAnswer": "D",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 209,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Removendo dados desnecessários automaticamente",
      "B": "Apenas aumentando a memória do cluster",
      "C": "Forçando o uso de cache em todas as queries",
      "D": "Através de análise de padrões de execução e recomendações de índices"
    },
    "correctAnswer": "D",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 210,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Query Profile e Spark UI",
      "B": "Documentação de referência",
      "C": "Dashboards estáticos",
      "D": "Apenas logs de erro"
    },
    "correctAnswer": "A",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 211,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Através de análise de padrões de workload e recomendações automáticas",
      "B": "Apenas baseado em tamanho de dados",
      "C": "Aleatoriamente entre opções disponíveis",
      "D": "Sempre usa o maior cluster disponível"
    },
    "correctAnswer": "A",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 212,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Apenas documentação",
      "B": "Apenas segurança",
      "C": "Simplificar decisões de particionamento e organização de dados",
      "D": "Apenas performance de leitura"
    },
    "correctAnswer": "C",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 213,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Apenas aumentando a memória do cluster",
      "B": "Através de análise de padrões de execução e recomendações de índices",
      "C": "Forçando o uso de cache em todas as queries",
      "D": "Removendo dados desnecessários automaticamente"
    },
    "correctAnswer": "B",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 214,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Query Profile e Spark UI",
      "B": "Dashboards estáticos",
      "C": "Apenas logs de erro",
      "D": "Documentação de referência"
    },
    "correctAnswer": "A",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 215,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Aleatoriamente entre opções disponíveis",
      "B": "Sempre usa o maior cluster disponível",
      "C": "Através de análise de padrões de workload e recomendações automáticas",
      "D": "Apenas baseado em tamanho de dados"
    },
    "correctAnswer": "C",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 216,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Apenas documentação",
      "B": "Apenas segurança",
      "C": "Simplificar decisões de particionamento e organização de dados",
      "D": "Apenas performance de leitura"
    },
    "correctAnswer": "C",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 217,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Através de análise de padrões de execução e recomendações de índices",
      "B": "Removendo dados desnecessários automaticamente",
      "C": "Apenas aumentando a memória do cluster",
      "D": "Forçando o uso de cache em todas as queries"
    },
    "correctAnswer": "A",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 218,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Apenas logs de erro",
      "B": "Query Profile e Spark UI",
      "C": "Documentação de referência",
      "D": "Dashboards estáticos"
    },
    "correctAnswer": "B",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 219,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Através de análise de padrões de workload e recomendações automáticas",
      "B": "Sempre usa o maior cluster disponível",
      "C": "Apenas baseado em tamanho de dados",
      "D": "Aleatoriamente entre opções disponíveis"
    },
    "correctAnswer": "A",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 220,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Apenas performance de leitura",
      "B": "Apenas segurança",
      "C": "Simplificar decisões de particionamento e organização de dados",
      "D": "Apenas documentação"
    },
    "correctAnswer": "C",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 221,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Apenas aumentando a memória do cluster",
      "B": "Através de análise de padrões de execução e recomendações de índices",
      "C": "Forçando o uso de cache em todas as queries",
      "D": "Removendo dados desnecessários automaticamente"
    },
    "correctAnswer": "B",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 222,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Documentação de referência",
      "B": "Query Profile e Spark UI",
      "C": "Dashboards estáticos",
      "D": "Apenas logs de erro"
    },
    "correctAnswer": "B",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 223,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Aleatoriamente entre opções disponíveis",
      "B": "Sempre usa o maior cluster disponível",
      "C": "Através de análise de padrões de workload e recomendações automáticas",
      "D": "Apenas baseado em tamanho de dados"
    },
    "correctAnswer": "C",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 224,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Simplificar decisões de particionamento e organização de dados",
      "B": "Apenas performance de leitura",
      "C": "Apenas documentação",
      "D": "Apenas segurança"
    },
    "correctAnswer": "A",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 225,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Forçando o uso de cache em todas as queries",
      "B": "Removendo dados desnecessários automaticamente",
      "C": "Apenas aumentando a memória do cluster",
      "D": "Através de análise de padrões de execução e recomendações de índices"
    },
    "correctAnswer": "D",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 226,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Apenas logs de erro",
      "B": "Dashboards estáticos",
      "C": "Documentação de referência",
      "D": "Query Profile e Spark UI"
    },
    "correctAnswer": "D",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 227,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Sempre usa o maior cluster disponível",
      "B": "Aleatoriamente entre opções disponíveis",
      "C": "Apenas baseado em tamanho de dados",
      "D": "Através de análise de padrões de workload e recomendações automáticas"
    },
    "correctAnswer": "D",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 228,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Apenas segurança",
      "B": "Apenas performance de leitura",
      "C": "Simplificar decisões de particionamento e organização de dados",
      "D": "Apenas documentação"
    },
    "correctAnswer": "C",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 229,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Apenas aumentando a memória do cluster",
      "B": "Removendo dados desnecessários automaticamente",
      "C": "Forçando o uso de cache em todas as queries",
      "D": "Através de análise de padrões de execução e recomendações de índices"
    },
    "correctAnswer": "D",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 230,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Dashboards estáticos",
      "B": "Documentação de referência",
      "C": "Apenas logs de erro",
      "D": "Query Profile e Spark UI"
    },
    "correctAnswer": "D",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 231,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Sempre usa o maior cluster disponível",
      "B": "Aleatoriamente entre opções disponíveis",
      "C": "Através de análise de padrões de workload e recomendações automáticas",
      "D": "Apenas baseado em tamanho de dados"
    },
    "correctAnswer": "C",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 232,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Apenas documentação",
      "B": "Apenas performance de leitura",
      "C": "Apenas segurança",
      "D": "Simplificar decisões de particionamento e organização de dados"
    },
    "correctAnswer": "D",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 233,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Através de análise de padrões de execução e recomendações de índices",
      "B": "Apenas aumentando a memória do cluster",
      "C": "Removendo dados desnecessários automaticamente",
      "D": "Forçando o uso de cache em todas as queries"
    },
    "correctAnswer": "A",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 234,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Dashboards estáticos",
      "B": "Documentação de referência",
      "C": "Query Profile e Spark UI",
      "D": "Apenas logs de erro"
    },
    "correctAnswer": "C",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 235,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Apenas baseado em tamanho de dados",
      "B": "Aleatoriamente entre opções disponíveis",
      "C": "Sempre usa o maior cluster disponível",
      "D": "Através de análise de padrões de workload e recomendações automáticas"
    },
    "correctAnswer": "D",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 236,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Simplificar decisões de particionamento e organização de dados",
      "B": "Apenas documentação",
      "C": "Apenas performance de leitura",
      "D": "Apenas segurança"
    },
    "correctAnswer": "A",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 237,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Apenas aumentando a memória do cluster",
      "B": "Forçando o uso de cache em todas as queries",
      "C": "Removendo dados desnecessários automaticamente",
      "D": "Através de análise de padrões de execução e recomendações de índices"
    },
    "correctAnswer": "D",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 238,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Dashboards estáticos",
      "B": "Apenas logs de erro",
      "C": "Query Profile e Spark UI",
      "D": "Documentação de referência"
    },
    "correctAnswer": "C",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 239,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Apenas baseado em tamanho de dados",
      "B": "Aleatoriamente entre opções disponíveis",
      "C": "Através de análise de padrões de workload e recomendações automáticas",
      "D": "Sempre usa o maior cluster disponível"
    },
    "correctAnswer": "C",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 240,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Apenas documentação",
      "B": "Simplificar decisões de particionamento e organização de dados",
      "C": "Apenas segurança",
      "D": "Apenas performance de leitura"
    },
    "correctAnswer": "B",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 241,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Apenas aumentando a memória do cluster",
      "B": "Removendo dados desnecessários automaticamente",
      "C": "Forçando o uso de cache em todas as queries",
      "D": "Através de análise de padrões de execução e recomendações de índices"
    },
    "correctAnswer": "D",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 242,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Dashboards estáticos",
      "B": "Apenas logs de erro",
      "C": "Documentação de referência",
      "D": "Query Profile e Spark UI"
    },
    "correctAnswer": "D",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 243,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Através de análise de padrões de workload e recomendações automáticas",
      "B": "Sempre usa o maior cluster disponível",
      "C": "Aleatoriamente entre opções disponíveis",
      "D": "Apenas baseado em tamanho de dados"
    },
    "correctAnswer": "A",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 244,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Apenas performance de leitura",
      "B": "Apenas segurança",
      "C": "Simplificar decisões de particionamento e organização de dados",
      "D": "Apenas documentação"
    },
    "correctAnswer": "C",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 245,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Através de análise de padrões de execução e recomendações de índices",
      "B": "Removendo dados desnecessários automaticamente",
      "C": "Forçando o uso de cache em todas as queries",
      "D": "Apenas aumentando a memória do cluster"
    },
    "correctAnswer": "A",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 246,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Dashboards estáticos",
      "B": "Documentação de referência",
      "C": "Query Profile e Spark UI",
      "D": "Apenas logs de erro"
    },
    "correctAnswer": "C",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 247,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Apenas baseado em tamanho de dados",
      "B": "Sempre usa o maior cluster disponível",
      "C": "Através de análise de padrões de workload e recomendações automáticas",
      "D": "Aleatoriamente entre opções disponíveis"
    },
    "correctAnswer": "C",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 248,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Apenas documentação",
      "B": "Apenas segurança",
      "C": "Apenas performance de leitura",
      "D": "Simplificar decisões de particionamento e organização de dados"
    },
    "correctAnswer": "D",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 249,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Através de análise de padrões de execução e recomendações de índices",
      "B": "Removendo dados desnecessários automaticamente",
      "C": "Forçando o uso de cache em todas as queries",
      "D": "Apenas aumentando a memória do cluster"
    },
    "correctAnswer": "A",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 250,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Dashboards estáticos",
      "B": "Apenas logs de erro",
      "C": "Query Profile e Spark UI",
      "D": "Documentação de referência"
    },
    "correctAnswer": "C",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 251,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Sempre usa o maior cluster disponível",
      "B": "Aleatoriamente entre opções disponíveis",
      "C": "Apenas baseado em tamanho de dados",
      "D": "Através de análise de padrões de workload e recomendações automáticas"
    },
    "correctAnswer": "D",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 252,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Apenas documentação",
      "B": "Apenas performance de leitura",
      "C": "Simplificar decisões de particionamento e organização de dados",
      "D": "Apenas segurança"
    },
    "correctAnswer": "C",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 253,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Através de análise de padrões de execução e recomendações de índices",
      "B": "Forçando o uso de cache em todas as queries",
      "C": "Apenas aumentando a memória do cluster",
      "D": "Removendo dados desnecessários automaticamente"
    },
    "correctAnswer": "A",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 254,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Documentação de referência",
      "B": "Query Profile e Spark UI",
      "C": "Apenas logs de erro",
      "D": "Dashboards estáticos"
    },
    "correctAnswer": "B",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 255,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Apenas baseado em tamanho de dados",
      "B": "Através de análise de padrões de workload e recomendações automáticas",
      "C": "Aleatoriamente entre opções disponíveis",
      "D": "Sempre usa o maior cluster disponível"
    },
    "correctAnswer": "B",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 256,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Simplificar decisões de particionamento e organização de dados",
      "B": "Apenas performance de leitura",
      "C": "Apenas documentação",
      "D": "Apenas segurança"
    },
    "correctAnswer": "A",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 257,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Através de análise de padrões de execução e recomendações de índices",
      "B": "Removendo dados desnecessários automaticamente",
      "C": "Forçando o uso de cache em todas as queries",
      "D": "Apenas aumentando a memória do cluster"
    },
    "correctAnswer": "A",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 258,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Apenas logs de erro",
      "B": "Dashboards estáticos",
      "C": "Documentação de referência",
      "D": "Query Profile e Spark UI"
    },
    "correctAnswer": "D",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 259,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Através de análise de padrões de workload e recomendações automáticas",
      "B": "Aleatoriamente entre opções disponíveis",
      "C": "Apenas baseado em tamanho de dados",
      "D": "Sempre usa o maior cluster disponível"
    },
    "correctAnswer": "A",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 260,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Apenas performance de leitura",
      "B": "Simplificar decisões de particionamento e organização de dados",
      "C": "Apenas documentação",
      "D": "Apenas segurança"
    },
    "correctAnswer": "B",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 261,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Forçando o uso de cache em todas as queries",
      "B": "Através de análise de padrões de execução e recomendações de índices",
      "C": "Apenas aumentando a memória do cluster",
      "D": "Removendo dados desnecessários automaticamente"
    },
    "correctAnswer": "B",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 262,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Query Profile e Spark UI",
      "B": "Dashboards estáticos",
      "C": "Documentação de referência",
      "D": "Apenas logs de erro"
    },
    "correctAnswer": "A",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 263,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Através de análise de padrões de workload e recomendações automáticas",
      "B": "Aleatoriamente entre opções disponíveis",
      "C": "Apenas baseado em tamanho de dados",
      "D": "Sempre usa o maior cluster disponível"
    },
    "correctAnswer": "A",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 264,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Apenas segurança",
      "B": "Apenas documentação",
      "C": "Simplificar decisões de particionamento e organização de dados",
      "D": "Apenas performance de leitura"
    },
    "correctAnswer": "C",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 265,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Forçando o uso de cache em todas as queries",
      "B": "Através de análise de padrões de execução e recomendações de índices",
      "C": "Apenas aumentando a memória do cluster",
      "D": "Removendo dados desnecessários automaticamente"
    },
    "correctAnswer": "B",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 266,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Apenas logs de erro",
      "B": "Dashboards estáticos",
      "C": "Query Profile e Spark UI",
      "D": "Documentação de referência"
    },
    "correctAnswer": "C",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 267,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Através de análise de padrões de workload e recomendações automáticas",
      "B": "Sempre usa o maior cluster disponível",
      "C": "Apenas baseado em tamanho de dados",
      "D": "Aleatoriamente entre opções disponíveis"
    },
    "correctAnswer": "A",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 268,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Apenas segurança",
      "B": "Apenas performance de leitura",
      "C": "Apenas documentação",
      "D": "Simplificar decisões de particionamento e organização de dados"
    },
    "correctAnswer": "D",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 269,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Removendo dados desnecessários automaticamente",
      "B": "Forçando o uso de cache em todas as queries",
      "C": "Através de análise de padrões de execução e recomendações de índices",
      "D": "Apenas aumentando a memória do cluster"
    },
    "correctAnswer": "C",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 270,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Documentação de referência",
      "B": "Query Profile e Spark UI",
      "C": "Apenas logs de erro",
      "D": "Dashboards estáticos"
    },
    "correctAnswer": "B",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 271,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Apenas baseado em tamanho de dados",
      "B": "Sempre usa o maior cluster disponível",
      "C": "Através de análise de padrões de workload e recomendações automáticas",
      "D": "Aleatoriamente entre opções disponíveis"
    },
    "correctAnswer": "C",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 272,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Apenas segurança",
      "B": "Simplificar decisões de particionamento e organização de dados",
      "C": "Apenas documentação",
      "D": "Apenas performance de leitura"
    },
    "correctAnswer": "B",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 273,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Forçando o uso de cache em todas as queries",
      "B": "Através de análise de padrões de execução e recomendações de índices",
      "C": "Removendo dados desnecessários automaticamente",
      "D": "Apenas aumentando a memória do cluster"
    },
    "correctAnswer": "B",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 274,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Apenas logs de erro",
      "B": "Query Profile e Spark UI",
      "C": "Documentação de referência",
      "D": "Dashboards estáticos"
    },
    "correctAnswer": "B",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 275,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Através de análise de padrões de workload e recomendações automáticas",
      "B": "Aleatoriamente entre opções disponíveis",
      "C": "Apenas baseado em tamanho de dados",
      "D": "Sempre usa o maior cluster disponível"
    },
    "correctAnswer": "A",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 276,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Apenas documentação",
      "B": "Apenas performance de leitura",
      "C": "Apenas segurança",
      "D": "Simplificar decisões de particionamento e organização de dados"
    },
    "correctAnswer": "D",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 277,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Forçando o uso de cache em todas as queries",
      "B": "Através de análise de padrões de execução e recomendações de índices",
      "C": "Apenas aumentando a memória do cluster",
      "D": "Removendo dados desnecessários automaticamente"
    },
    "correctAnswer": "B",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 278,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Apenas logs de erro",
      "B": "Documentação de referência",
      "C": "Dashboards estáticos",
      "D": "Query Profile e Spark UI"
    },
    "correctAnswer": "D",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 279,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Através de análise de padrões de workload e recomendações automáticas",
      "B": "Aleatoriamente entre opções disponíveis",
      "C": "Apenas baseado em tamanho de dados",
      "D": "Sempre usa o maior cluster disponível"
    },
    "correctAnswer": "A",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 280,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Simplificar decisões de particionamento e organização de dados",
      "B": "Apenas documentação",
      "C": "Apenas performance de leitura",
      "D": "Apenas segurança"
    },
    "correctAnswer": "A",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 281,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Apenas aumentando a memória do cluster",
      "B": "Forçando o uso de cache em todas as queries",
      "C": "Através de análise de padrões de execução e recomendações de índices",
      "D": "Removendo dados desnecessários automaticamente"
    },
    "correctAnswer": "C",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 282,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Query Profile e Spark UI",
      "B": "Documentação de referência",
      "C": "Apenas logs de erro",
      "D": "Dashboards estáticos"
    },
    "correctAnswer": "A",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 283,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Apenas baseado em tamanho de dados",
      "B": "Sempre usa o maior cluster disponível",
      "C": "Através de análise de padrões de workload e recomendações automáticas",
      "D": "Aleatoriamente entre opções disponíveis"
    },
    "correctAnswer": "C",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 284,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Simplificar decisões de particionamento e organização de dados",
      "B": "Apenas segurança",
      "C": "Apenas performance de leitura",
      "D": "Apenas documentação"
    },
    "correctAnswer": "A",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 285,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Através de análise de padrões de execução e recomendações de índices",
      "B": "Removendo dados desnecessários automaticamente",
      "C": "Forçando o uso de cache em todas as queries",
      "D": "Apenas aumentando a memória do cluster"
    },
    "correctAnswer": "A",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 286,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Query Profile e Spark UI",
      "B": "Apenas logs de erro",
      "C": "Documentação de referência",
      "D": "Dashboards estáticos"
    },
    "correctAnswer": "A",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 287,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Através de análise de padrões de workload e recomendações automáticas",
      "B": "Sempre usa o maior cluster disponível",
      "C": "Aleatoriamente entre opções disponíveis",
      "D": "Apenas baseado em tamanho de dados"
    },
    "correctAnswer": "A",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 288,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Apenas performance de leitura",
      "B": "Apenas segurança",
      "C": "Apenas documentação",
      "D": "Simplificar decisões de particionamento e organização de dados"
    },
    "correctAnswer": "D",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 289,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Removendo dados desnecessários automaticamente",
      "B": "Forçando o uso de cache em todas as queries",
      "C": "Apenas aumentando a memória do cluster",
      "D": "Através de análise de padrões de execução e recomendações de índices"
    },
    "correctAnswer": "D",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 290,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Apenas logs de erro",
      "B": "Documentação de referência",
      "C": "Query Profile e Spark UI",
      "D": "Dashboards estáticos"
    },
    "correctAnswer": "C",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 291,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Aleatoriamente entre opções disponíveis",
      "B": "Através de análise de padrões de workload e recomendações automáticas",
      "C": "Apenas baseado em tamanho de dados",
      "D": "Sempre usa o maior cluster disponível"
    },
    "correctAnswer": "B",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 292,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Simplificar decisões de particionamento e organização de dados",
      "B": "Apenas documentação",
      "C": "Apenas segurança",
      "D": "Apenas performance de leitura"
    },
    "correctAnswer": "A",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 293,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Apenas aumentando a memória do cluster",
      "B": "Removendo dados desnecessários automaticamente",
      "C": "Através de análise de padrões de execução e recomendações de índices",
      "D": "Forçando o uso de cache em todas as queries"
    },
    "correctAnswer": "C",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 294,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Documentação de referência",
      "B": "Apenas logs de erro",
      "C": "Query Profile e Spark UI",
      "D": "Dashboards estáticos"
    },
    "correctAnswer": "C",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 295,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Apenas baseado em tamanho de dados",
      "B": "Sempre usa o maior cluster disponível",
      "C": "Através de análise de padrões de workload e recomendações automáticas",
      "D": "Aleatoriamente entre opções disponíveis"
    },
    "correctAnswer": "C",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 296,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Apenas segurança",
      "B": "Apenas performance de leitura",
      "C": "Simplificar decisões de particionamento e organização de dados",
      "D": "Apenas documentação"
    },
    "correctAnswer": "C",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 297,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform otimiza a performance de queries através de análise automática?",
    "options": {
      "A": "Apenas aumentando a memória do cluster",
      "B": "Forçando o uso de cache em todas as queries",
      "C": "Através de análise de padrões de execução e recomendações de índices",
      "D": "Removendo dados desnecessários automaticamente"
    },
    "correctAnswer": "C",
    "rationale": "O Databricks Intelligence Platform analisa padrões de execução de queries e fornece recomendações automáticas para otimização, incluindo sugestões de índices e reorganização de dados.",
    "tip": "Foco em análise inteligente e recomendações automáticas para otimização."
  },
  {
    "id": 298,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Qual ferramenta é mais útil para diagnosticar gargalos de performance em queries?",
    "options": {
      "A": "Dashboards estáticos",
      "B": "Apenas logs de erro",
      "C": "Query Profile e Spark UI",
      "D": "Documentação de referência"
    },
    "correctAnswer": "C",
    "rationale": "Query Profile e Spark UI são ferramentas principais para entender e diagnosticar gargalos de performance em queries.",
    "tip": "Use ferramentas de profiling para identificar problemas de performance."
  },
  {
    "id": 299,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Como o Databricks Intelligence Platform identifica o compute apropriado para um workload?",
    "options": {
      "A": "Aleatoriamente entre opções disponíveis",
      "B": "Apenas baseado em tamanho de dados",
      "C": "Através de análise de padrões de workload e recomendações automáticas",
      "D": "Sempre usa o maior cluster disponível"
    },
    "correctAnswer": "C",
    "rationale": "O platform analisa características do workload e recomenda o tipo de compute (CPU, GPU, memória) mais apropriado.",
    "tip": "Pense em análise de padrões e recomendações baseadas em workload."
  },
  {
    "id": 300,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Qual é o principal benefício de usar o Databricks Intelligence Platform para layout de dados?",
    "options": {
      "A": "Apenas performance de leitura",
      "B": "Apenas documentação",
      "C": "Simplificar decisões de particionamento e organização de dados",
      "D": "Apenas segurança"
    },
    "correctAnswer": "C",
    "rationale": "O platform simplifica decisões complexas sobre como organizar e particionar dados para melhor performance.",
    "tip": "Foco em simplificação de decisões de layout de dados."
  },
  {
    "id": 301,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Apenas transferir dados entre clusters",
      "B": "Gerenciar permissões de usuários",
      "C": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "D": "Fazer backup de notebooks"
    },
    "correctAnswer": "C",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 302,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Eliminação de duplicatas",
      "B": "Schema Evolution automática e detecção de novos arquivos",
      "C": "Apenas compressão de dados",
      "D": "Criptografia automática"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 303,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Falha e interrompe a pipeline",
      "B": "Descarta automaticamente",
      "C": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "D": "Converte tudo para string"
    },
    "correctAnswer": "C",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 304,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "S3, ADLS, GCS, SFTP",
      "B": "Apenas S3",
      "C": "Apenas bancos de dados relacionais",
      "D": "Apenas APIs externas"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 305,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Não há diferença",
      "B": "COPY INTO é mais rápido",
      "C": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "D": "Auto Loader não funciona com Delta"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 306,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Apenas transferir dados entre clusters",
      "B": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "C": "Fazer backup de notebooks",
      "D": "Gerenciar permissões de usuários"
    },
    "correctAnswer": "B",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 307,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Eliminação de duplicatas",
      "B": "Schema Evolution automática e detecção de novos arquivos",
      "C": "Apenas compressão de dados",
      "D": "Criptografia automática"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 308,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "B": "Falha e interrompe a pipeline",
      "C": "Converte tudo para string",
      "D": "Descarta automaticamente"
    },
    "correctAnswer": "A",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 309,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "S3, ADLS, GCS, SFTP",
      "B": "Apenas APIs externas",
      "C": "Apenas bancos de dados relacionais",
      "D": "Apenas S3"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 310,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader não funciona com Delta",
      "B": "COPY INTO é mais rápido",
      "C": "Não há diferença",
      "D": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 311,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Apenas transferir dados entre clusters",
      "B": "Fazer backup de notebooks",
      "C": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "D": "Gerenciar permissões de usuários"
    },
    "correctAnswer": "C",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 312,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Schema Evolution automática e detecção de novos arquivos",
      "B": "Apenas compressão de dados",
      "C": "Eliminação de duplicatas",
      "D": "Criptografia automática"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 313,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Falha e interrompe a pipeline",
      "B": "Descarta automaticamente",
      "C": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "D": "Converte tudo para string"
    },
    "correctAnswer": "C",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 314,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas bancos de dados relacionais",
      "B": "Apenas S3",
      "C": "S3, ADLS, GCS, SFTP",
      "D": "Apenas APIs externas"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 315,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "B": "Auto Loader não funciona com Delta",
      "C": "Não há diferença",
      "D": "COPY INTO é mais rápido"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 316,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Apenas transferir dados entre clusters",
      "B": "Fazer backup de notebooks",
      "C": "Gerenciar permissões de usuários",
      "D": "Conectar IDEs locais ao Databricks para desenvolvimento remoto"
    },
    "correctAnswer": "D",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 317,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Apenas compressão de dados",
      "B": "Schema Evolution automática e detecção de novos arquivos",
      "C": "Criptografia automática",
      "D": "Eliminação de duplicatas"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 318,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "B": "Descarta automaticamente",
      "C": "Converte tudo para string",
      "D": "Falha e interrompe a pipeline"
    },
    "correctAnswer": "A",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 319,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas bancos de dados relacionais",
      "B": "Apenas APIs externas",
      "C": "S3, ADLS, GCS, SFTP",
      "D": "Apenas S3"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 320,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "B": "COPY INTO é mais rápido",
      "C": "Não há diferença",
      "D": "Auto Loader não funciona com Delta"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 321,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Fazer backup de notebooks",
      "B": "Gerenciar permissões de usuários",
      "C": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "D": "Apenas transferir dados entre clusters"
    },
    "correctAnswer": "C",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 322,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Schema Evolution automática e detecção de novos arquivos",
      "B": "Apenas compressão de dados",
      "C": "Criptografia automática",
      "D": "Eliminação de duplicatas"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 323,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Falha e interrompe a pipeline",
      "B": "Converte tudo para string",
      "C": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "D": "Descarta automaticamente"
    },
    "correctAnswer": "C",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 324,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas S3",
      "B": "Apenas APIs externas",
      "C": "Apenas bancos de dados relacionais",
      "D": "S3, ADLS, GCS, SFTP"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 325,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "B": "Auto Loader não funciona com Delta",
      "C": "Não há diferença",
      "D": "COPY INTO é mais rápido"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 326,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "B": "Apenas transferir dados entre clusters",
      "C": "Fazer backup de notebooks",
      "D": "Gerenciar permissões de usuários"
    },
    "correctAnswer": "A",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 327,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Eliminação de duplicatas",
      "B": "Criptografia automática",
      "C": "Schema Evolution automática e detecção de novos arquivos",
      "D": "Apenas compressão de dados"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 328,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Descarta automaticamente",
      "B": "Falha e interrompe a pipeline",
      "C": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "D": "Converte tudo para string"
    },
    "correctAnswer": "C",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 329,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "S3, ADLS, GCS, SFTP",
      "B": "Apenas APIs externas",
      "C": "Apenas bancos de dados relacionais",
      "D": "Apenas S3"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 330,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Não há diferença",
      "B": "Auto Loader não funciona com Delta",
      "C": "COPY INTO é mais rápido",
      "D": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 331,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "B": "Fazer backup de notebooks",
      "C": "Apenas transferir dados entre clusters",
      "D": "Gerenciar permissões de usuários"
    },
    "correctAnswer": "A",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 332,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Schema Evolution automática e detecção de novos arquivos",
      "B": "Apenas compressão de dados",
      "C": "Eliminação de duplicatas",
      "D": "Criptografia automática"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 333,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Converte tudo para string",
      "B": "Descarta automaticamente",
      "C": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "D": "Falha e interrompe a pipeline"
    },
    "correctAnswer": "C",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 334,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "S3, ADLS, GCS, SFTP",
      "B": "Apenas APIs externas",
      "C": "Apenas bancos de dados relacionais",
      "D": "Apenas S3"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 335,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Não há diferença",
      "B": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "C": "Auto Loader não funciona com Delta",
      "D": "COPY INTO é mais rápido"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 336,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "B": "Gerenciar permissões de usuários",
      "C": "Fazer backup de notebooks",
      "D": "Apenas transferir dados entre clusters"
    },
    "correctAnswer": "A",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 337,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Criptografia automática",
      "B": "Apenas compressão de dados",
      "C": "Schema Evolution automática e detecção de novos arquivos",
      "D": "Eliminação de duplicatas"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 338,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Falha e interrompe a pipeline",
      "B": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "C": "Descarta automaticamente",
      "D": "Converte tudo para string"
    },
    "correctAnswer": "B",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 339,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "S3, ADLS, GCS, SFTP",
      "B": "Apenas APIs externas",
      "C": "Apenas bancos de dados relacionais",
      "D": "Apenas S3"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 340,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "COPY INTO é mais rápido",
      "B": "Não há diferença",
      "C": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "D": "Auto Loader não funciona com Delta"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 341,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "B": "Gerenciar permissões de usuários",
      "C": "Fazer backup de notebooks",
      "D": "Apenas transferir dados entre clusters"
    },
    "correctAnswer": "A",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 342,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Eliminação de duplicatas",
      "B": "Apenas compressão de dados",
      "C": "Schema Evolution automática e detecção de novos arquivos",
      "D": "Criptografia automática"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 343,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Descarta automaticamente",
      "B": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "C": "Falha e interrompe a pipeline",
      "D": "Converte tudo para string"
    },
    "correctAnswer": "B",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 344,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas APIs externas",
      "B": "Apenas bancos de dados relacionais",
      "C": "Apenas S3",
      "D": "S3, ADLS, GCS, SFTP"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 345,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Não há diferença",
      "B": "Auto Loader não funciona com Delta",
      "C": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "D": "COPY INTO é mais rápido"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 346,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Gerenciar permissões de usuários",
      "B": "Apenas transferir dados entre clusters",
      "C": "Fazer backup de notebooks",
      "D": "Conectar IDEs locais ao Databricks para desenvolvimento remoto"
    },
    "correctAnswer": "D",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 347,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Schema Evolution automática e detecção de novos arquivos",
      "B": "Criptografia automática",
      "C": "Apenas compressão de dados",
      "D": "Eliminação de duplicatas"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 348,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Converte tudo para string",
      "B": "Falha e interrompe a pipeline",
      "C": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "D": "Descarta automaticamente"
    },
    "correctAnswer": "C",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 349,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas S3",
      "B": "S3, ADLS, GCS, SFTP",
      "C": "Apenas APIs externas",
      "D": "Apenas bancos de dados relacionais"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 350,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "B": "COPY INTO é mais rápido",
      "C": "Não há diferença",
      "D": "Auto Loader não funciona com Delta"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 351,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Apenas transferir dados entre clusters",
      "B": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "C": "Gerenciar permissões de usuários",
      "D": "Fazer backup de notebooks"
    },
    "correctAnswer": "B",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 352,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Eliminação de duplicatas",
      "B": "Schema Evolution automática e detecção de novos arquivos",
      "C": "Apenas compressão de dados",
      "D": "Criptografia automática"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 353,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "B": "Descarta automaticamente",
      "C": "Converte tudo para string",
      "D": "Falha e interrompe a pipeline"
    },
    "correctAnswer": "A",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 354,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas bancos de dados relacionais",
      "B": "Apenas APIs externas",
      "C": "S3, ADLS, GCS, SFTP",
      "D": "Apenas S3"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 355,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Não há diferença",
      "B": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "C": "COPY INTO é mais rápido",
      "D": "Auto Loader não funciona com Delta"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 356,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Gerenciar permissões de usuários",
      "B": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "C": "Apenas transferir dados entre clusters",
      "D": "Fazer backup de notebooks"
    },
    "correctAnswer": "B",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 357,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Schema Evolution automática e detecção de novos arquivos",
      "B": "Eliminação de duplicatas",
      "C": "Criptografia automática",
      "D": "Apenas compressão de dados"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 358,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Descarta automaticamente",
      "B": "Falha e interrompe a pipeline",
      "C": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "D": "Converte tudo para string"
    },
    "correctAnswer": "C",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 359,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas S3",
      "B": "Apenas bancos de dados relacionais",
      "C": "S3, ADLS, GCS, SFTP",
      "D": "Apenas APIs externas"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 360,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "B": "Não há diferença",
      "C": "Auto Loader não funciona com Delta",
      "D": "COPY INTO é mais rápido"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 361,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Apenas transferir dados entre clusters",
      "B": "Fazer backup de notebooks",
      "C": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "D": "Gerenciar permissões de usuários"
    },
    "correctAnswer": "C",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 362,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Apenas compressão de dados",
      "B": "Criptografia automática",
      "C": "Schema Evolution automática e detecção de novos arquivos",
      "D": "Eliminação de duplicatas"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 363,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Falha e interrompe a pipeline",
      "B": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "C": "Converte tudo para string",
      "D": "Descarta automaticamente"
    },
    "correctAnswer": "B",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 364,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "S3, ADLS, GCS, SFTP",
      "B": "Apenas S3",
      "C": "Apenas bancos de dados relacionais",
      "D": "Apenas APIs externas"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 365,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Não há diferença",
      "B": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "C": "Auto Loader não funciona com Delta",
      "D": "COPY INTO é mais rápido"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 366,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Apenas transferir dados entre clusters",
      "B": "Gerenciar permissões de usuários",
      "C": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "D": "Fazer backup de notebooks"
    },
    "correctAnswer": "C",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 367,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Apenas compressão de dados",
      "B": "Eliminação de duplicatas",
      "C": "Schema Evolution automática e detecção de novos arquivos",
      "D": "Criptografia automática"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 368,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "B": "Falha e interrompe a pipeline",
      "C": "Converte tudo para string",
      "D": "Descarta automaticamente"
    },
    "correctAnswer": "A",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 369,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas bancos de dados relacionais",
      "B": "S3, ADLS, GCS, SFTP",
      "C": "Apenas S3",
      "D": "Apenas APIs externas"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 370,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader não funciona com Delta",
      "B": "Não há diferença",
      "C": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "D": "COPY INTO é mais rápido"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 371,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "B": "Fazer backup de notebooks",
      "C": "Apenas transferir dados entre clusters",
      "D": "Gerenciar permissões de usuários"
    },
    "correctAnswer": "A",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 372,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Schema Evolution automática e detecção de novos arquivos",
      "B": "Criptografia automática",
      "C": "Apenas compressão de dados",
      "D": "Eliminação de duplicatas"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 373,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Descarta automaticamente",
      "B": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "C": "Converte tudo para string",
      "D": "Falha e interrompe a pipeline"
    },
    "correctAnswer": "B",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 374,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas bancos de dados relacionais",
      "B": "S3, ADLS, GCS, SFTP",
      "C": "Apenas APIs externas",
      "D": "Apenas S3"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 375,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "COPY INTO é mais rápido",
      "B": "Não há diferença",
      "C": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "D": "Auto Loader não funciona com Delta"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 376,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Apenas transferir dados entre clusters",
      "B": "Gerenciar permissões de usuários",
      "C": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "D": "Fazer backup de notebooks"
    },
    "correctAnswer": "C",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 377,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Apenas compressão de dados",
      "B": "Criptografia automática",
      "C": "Schema Evolution automática e detecção de novos arquivos",
      "D": "Eliminação de duplicatas"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 378,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Falha e interrompe a pipeline",
      "B": "Descarta automaticamente",
      "C": "Converte tudo para string",
      "D": "Usa 'rescuedDataColumn' para capturar dados problemáticos"
    },
    "correctAnswer": "D",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 379,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas bancos de dados relacionais",
      "B": "Apenas APIs externas",
      "C": "Apenas S3",
      "D": "S3, ADLS, GCS, SFTP"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 380,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader não funciona com Delta",
      "B": "COPY INTO é mais rápido",
      "C": "Não há diferença",
      "D": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 381,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Fazer backup de notebooks",
      "B": "Apenas transferir dados entre clusters",
      "C": "Gerenciar permissões de usuários",
      "D": "Conectar IDEs locais ao Databricks para desenvolvimento remoto"
    },
    "correctAnswer": "D",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 382,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Apenas compressão de dados",
      "B": "Criptografia automática",
      "C": "Eliminação de duplicatas",
      "D": "Schema Evolution automática e detecção de novos arquivos"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 383,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "B": "Falha e interrompe a pipeline",
      "C": "Descarta automaticamente",
      "D": "Converte tudo para string"
    },
    "correctAnswer": "A",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 384,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "S3, ADLS, GCS, SFTP",
      "B": "Apenas bancos de dados relacionais",
      "C": "Apenas APIs externas",
      "D": "Apenas S3"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 385,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "B": "Não há diferença",
      "C": "COPY INTO é mais rápido",
      "D": "Auto Loader não funciona com Delta"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 386,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "B": "Gerenciar permissões de usuários",
      "C": "Apenas transferir dados entre clusters",
      "D": "Fazer backup de notebooks"
    },
    "correctAnswer": "A",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 387,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Eliminação de duplicatas",
      "B": "Criptografia automática",
      "C": "Schema Evolution automática e detecção de novos arquivos",
      "D": "Apenas compressão de dados"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 388,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Descarta automaticamente",
      "B": "Converte tudo para string",
      "C": "Falha e interrompe a pipeline",
      "D": "Usa 'rescuedDataColumn' para capturar dados problemáticos"
    },
    "correctAnswer": "D",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 389,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas S3",
      "B": "Apenas APIs externas",
      "C": "S3, ADLS, GCS, SFTP",
      "D": "Apenas bancos de dados relacionais"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 390,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "COPY INTO é mais rápido",
      "B": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "C": "Auto Loader não funciona com Delta",
      "D": "Não há diferença"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 391,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Apenas transferir dados entre clusters",
      "B": "Gerenciar permissões de usuários",
      "C": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "D": "Fazer backup de notebooks"
    },
    "correctAnswer": "C",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 392,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Schema Evolution automática e detecção de novos arquivos",
      "B": "Eliminação de duplicatas",
      "C": "Apenas compressão de dados",
      "D": "Criptografia automática"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 393,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Descarta automaticamente",
      "B": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "C": "Converte tudo para string",
      "D": "Falha e interrompe a pipeline"
    },
    "correctAnswer": "B",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 394,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas APIs externas",
      "B": "Apenas bancos de dados relacionais",
      "C": "Apenas S3",
      "D": "S3, ADLS, GCS, SFTP"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 395,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "B": "Auto Loader não funciona com Delta",
      "C": "Não há diferença",
      "D": "COPY INTO é mais rápido"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 396,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "B": "Fazer backup de notebooks",
      "C": "Gerenciar permissões de usuários",
      "D": "Apenas transferir dados entre clusters"
    },
    "correctAnswer": "A",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 397,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Criptografia automática",
      "B": "Schema Evolution automática e detecção de novos arquivos",
      "C": "Apenas compressão de dados",
      "D": "Eliminação de duplicatas"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 398,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "B": "Falha e interrompe a pipeline",
      "C": "Descarta automaticamente",
      "D": "Converte tudo para string"
    },
    "correctAnswer": "A",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 399,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas bancos de dados relacionais",
      "B": "S3, ADLS, GCS, SFTP",
      "C": "Apenas APIs externas",
      "D": "Apenas S3"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 400,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "B": "Não há diferença",
      "C": "Auto Loader não funciona com Delta",
      "D": "COPY INTO é mais rápido"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 401,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Fazer backup de notebooks",
      "B": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "C": "Gerenciar permissões de usuários",
      "D": "Apenas transferir dados entre clusters"
    },
    "correctAnswer": "B",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 402,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Apenas compressão de dados",
      "B": "Criptografia automática",
      "C": "Schema Evolution automática e detecção de novos arquivos",
      "D": "Eliminação de duplicatas"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 403,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Converte tudo para string",
      "B": "Falha e interrompe a pipeline",
      "C": "Descarta automaticamente",
      "D": "Usa 'rescuedDataColumn' para capturar dados problemáticos"
    },
    "correctAnswer": "D",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 404,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "S3, ADLS, GCS, SFTP",
      "B": "Apenas APIs externas",
      "C": "Apenas bancos de dados relacionais",
      "D": "Apenas S3"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 405,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "B": "Não há diferença",
      "C": "COPY INTO é mais rápido",
      "D": "Auto Loader não funciona com Delta"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 406,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Apenas transferir dados entre clusters",
      "B": "Gerenciar permissões de usuários",
      "C": "Fazer backup de notebooks",
      "D": "Conectar IDEs locais ao Databricks para desenvolvimento remoto"
    },
    "correctAnswer": "D",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 407,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Apenas compressão de dados",
      "B": "Criptografia automática",
      "C": "Schema Evolution automática e detecção de novos arquivos",
      "D": "Eliminação de duplicatas"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 408,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Converte tudo para string",
      "B": "Falha e interrompe a pipeline",
      "C": "Descarta automaticamente",
      "D": "Usa 'rescuedDataColumn' para capturar dados problemáticos"
    },
    "correctAnswer": "D",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 409,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas APIs externas",
      "B": "Apenas bancos de dados relacionais",
      "C": "Apenas S3",
      "D": "S3, ADLS, GCS, SFTP"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 410,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Não há diferença",
      "B": "Auto Loader não funciona com Delta",
      "C": "COPY INTO é mais rápido",
      "D": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 411,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "B": "Fazer backup de notebooks",
      "C": "Apenas transferir dados entre clusters",
      "D": "Gerenciar permissões de usuários"
    },
    "correctAnswer": "A",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 412,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Schema Evolution automática e detecção de novos arquivos",
      "B": "Apenas compressão de dados",
      "C": "Criptografia automática",
      "D": "Eliminação de duplicatas"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 413,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Falha e interrompe a pipeline",
      "B": "Descarta automaticamente",
      "C": "Converte tudo para string",
      "D": "Usa 'rescuedDataColumn' para capturar dados problemáticos"
    },
    "correctAnswer": "D",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 414,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas bancos de dados relacionais",
      "B": "Apenas S3",
      "C": "S3, ADLS, GCS, SFTP",
      "D": "Apenas APIs externas"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 415,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Não há diferença",
      "B": "COPY INTO é mais rápido",
      "C": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "D": "Auto Loader não funciona com Delta"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 416,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Gerenciar permissões de usuários",
      "B": "Apenas transferir dados entre clusters",
      "C": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "D": "Fazer backup de notebooks"
    },
    "correctAnswer": "C",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 417,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Schema Evolution automática e detecção de novos arquivos",
      "B": "Apenas compressão de dados",
      "C": "Eliminação de duplicatas",
      "D": "Criptografia automática"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 418,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Descarta automaticamente",
      "B": "Converte tudo para string",
      "C": "Falha e interrompe a pipeline",
      "D": "Usa 'rescuedDataColumn' para capturar dados problemáticos"
    },
    "correctAnswer": "D",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 419,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas bancos de dados relacionais",
      "B": "S3, ADLS, GCS, SFTP",
      "C": "Apenas APIs externas",
      "D": "Apenas S3"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 420,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Não há diferença",
      "B": "COPY INTO é mais rápido",
      "C": "Auto Loader não funciona com Delta",
      "D": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 421,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Apenas transferir dados entre clusters",
      "B": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "C": "Fazer backup de notebooks",
      "D": "Gerenciar permissões de usuários"
    },
    "correctAnswer": "B",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 422,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Schema Evolution automática e detecção de novos arquivos",
      "B": "Eliminação de duplicatas",
      "C": "Apenas compressão de dados",
      "D": "Criptografia automática"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 423,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Descarta automaticamente",
      "B": "Falha e interrompe a pipeline",
      "C": "Converte tudo para string",
      "D": "Usa 'rescuedDataColumn' para capturar dados problemáticos"
    },
    "correctAnswer": "D",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 424,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas S3",
      "B": "Apenas APIs externas",
      "C": "S3, ADLS, GCS, SFTP",
      "D": "Apenas bancos de dados relacionais"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 425,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "COPY INTO é mais rápido",
      "B": "Não há diferença",
      "C": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "D": "Auto Loader não funciona com Delta"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 426,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Gerenciar permissões de usuários",
      "B": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "C": "Fazer backup de notebooks",
      "D": "Apenas transferir dados entre clusters"
    },
    "correctAnswer": "B",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 427,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Schema Evolution automática e detecção de novos arquivos",
      "B": "Criptografia automática",
      "C": "Apenas compressão de dados",
      "D": "Eliminação de duplicatas"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 428,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Converte tudo para string",
      "B": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "C": "Descarta automaticamente",
      "D": "Falha e interrompe a pipeline"
    },
    "correctAnswer": "B",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 429,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas bancos de dados relacionais",
      "B": "Apenas S3",
      "C": "Apenas APIs externas",
      "D": "S3, ADLS, GCS, SFTP"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 430,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "COPY INTO é mais rápido",
      "B": "Auto Loader não funciona com Delta",
      "C": "Não há diferença",
      "D": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 431,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Apenas transferir dados entre clusters",
      "B": "Fazer backup de notebooks",
      "C": "Gerenciar permissões de usuários",
      "D": "Conectar IDEs locais ao Databricks para desenvolvimento remoto"
    },
    "correctAnswer": "D",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 432,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Apenas compressão de dados",
      "B": "Eliminação de duplicatas",
      "C": "Criptografia automática",
      "D": "Schema Evolution automática e detecção de novos arquivos"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 433,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Converte tudo para string",
      "B": "Descarta automaticamente",
      "C": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "D": "Falha e interrompe a pipeline"
    },
    "correctAnswer": "C",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 434,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas bancos de dados relacionais",
      "B": "Apenas APIs externas",
      "C": "S3, ADLS, GCS, SFTP",
      "D": "Apenas S3"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 435,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "B": "COPY INTO é mais rápido",
      "C": "Não há diferença",
      "D": "Auto Loader não funciona com Delta"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 436,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "B": "Gerenciar permissões de usuários",
      "C": "Apenas transferir dados entre clusters",
      "D": "Fazer backup de notebooks"
    },
    "correctAnswer": "A",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 437,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Eliminação de duplicatas",
      "B": "Schema Evolution automática e detecção de novos arquivos",
      "C": "Criptografia automática",
      "D": "Apenas compressão de dados"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 438,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Converte tudo para string",
      "B": "Falha e interrompe a pipeline",
      "C": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "D": "Descarta automaticamente"
    },
    "correctAnswer": "C",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 439,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "S3, ADLS, GCS, SFTP",
      "B": "Apenas bancos de dados relacionais",
      "C": "Apenas APIs externas",
      "D": "Apenas S3"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 440,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Não há diferença",
      "B": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "C": "COPY INTO é mais rápido",
      "D": "Auto Loader não funciona com Delta"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 441,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Gerenciar permissões de usuários",
      "B": "Apenas transferir dados entre clusters",
      "C": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "D": "Fazer backup de notebooks"
    },
    "correctAnswer": "C",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 442,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Schema Evolution automática e detecção de novos arquivos",
      "B": "Eliminação de duplicatas",
      "C": "Apenas compressão de dados",
      "D": "Criptografia automática"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 443,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Falha e interrompe a pipeline",
      "B": "Converte tudo para string",
      "C": "Descarta automaticamente",
      "D": "Usa 'rescuedDataColumn' para capturar dados problemáticos"
    },
    "correctAnswer": "D",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 444,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas S3",
      "B": "Apenas APIs externas",
      "C": "S3, ADLS, GCS, SFTP",
      "D": "Apenas bancos de dados relacionais"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 445,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Não há diferença",
      "B": "Auto Loader não funciona com Delta",
      "C": "COPY INTO é mais rápido",
      "D": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 446,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Apenas transferir dados entre clusters",
      "B": "Fazer backup de notebooks",
      "C": "Gerenciar permissões de usuários",
      "D": "Conectar IDEs locais ao Databricks para desenvolvimento remoto"
    },
    "correctAnswer": "D",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 447,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Eliminação de duplicatas",
      "B": "Schema Evolution automática e detecção de novos arquivos",
      "C": "Apenas compressão de dados",
      "D": "Criptografia automática"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 448,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Converte tudo para string",
      "B": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "C": "Falha e interrompe a pipeline",
      "D": "Descarta automaticamente"
    },
    "correctAnswer": "B",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 449,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas APIs externas",
      "B": "S3, ADLS, GCS, SFTP",
      "C": "Apenas S3",
      "D": "Apenas bancos de dados relacionais"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 450,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Não há diferença",
      "B": "COPY INTO é mais rápido",
      "C": "Auto Loader não funciona com Delta",
      "D": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 451,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Fazer backup de notebooks",
      "B": "Apenas transferir dados entre clusters",
      "C": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "D": "Gerenciar permissões de usuários"
    },
    "correctAnswer": "C",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 452,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Criptografia automática",
      "B": "Eliminação de duplicatas",
      "C": "Apenas compressão de dados",
      "D": "Schema Evolution automática e detecção de novos arquivos"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 453,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Falha e interrompe a pipeline",
      "B": "Descarta automaticamente",
      "C": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "D": "Converte tudo para string"
    },
    "correctAnswer": "C",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 454,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "S3, ADLS, GCS, SFTP",
      "B": "Apenas S3",
      "C": "Apenas APIs externas",
      "D": "Apenas bancos de dados relacionais"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 455,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "COPY INTO é mais rápido",
      "B": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "C": "Não há diferença",
      "D": "Auto Loader não funciona com Delta"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 456,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "B": "Fazer backup de notebooks",
      "C": "Gerenciar permissões de usuários",
      "D": "Apenas transferir dados entre clusters"
    },
    "correctAnswer": "A",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 457,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Apenas compressão de dados",
      "B": "Eliminação de duplicatas",
      "C": "Criptografia automática",
      "D": "Schema Evolution automática e detecção de novos arquivos"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 458,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Descarta automaticamente",
      "B": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "C": "Converte tudo para string",
      "D": "Falha e interrompe a pipeline"
    },
    "correctAnswer": "B",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 459,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas APIs externas",
      "B": "Apenas bancos de dados relacionais",
      "C": "S3, ADLS, GCS, SFTP",
      "D": "Apenas S3"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 460,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Não há diferença",
      "B": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "C": "COPY INTO é mais rápido",
      "D": "Auto Loader não funciona com Delta"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 461,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Apenas transferir dados entre clusters",
      "B": "Fazer backup de notebooks",
      "C": "Gerenciar permissões de usuários",
      "D": "Conectar IDEs locais ao Databricks para desenvolvimento remoto"
    },
    "correctAnswer": "D",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 462,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Schema Evolution automática e detecção de novos arquivos",
      "B": "Eliminação de duplicatas",
      "C": "Apenas compressão de dados",
      "D": "Criptografia automática"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 463,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Falha e interrompe a pipeline",
      "B": "Converte tudo para string",
      "C": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "D": "Descarta automaticamente"
    },
    "correctAnswer": "C",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 464,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas bancos de dados relacionais",
      "B": "Apenas S3",
      "C": "Apenas APIs externas",
      "D": "S3, ADLS, GCS, SFTP"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 465,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "B": "Auto Loader não funciona com Delta",
      "C": "COPY INTO é mais rápido",
      "D": "Não há diferença"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 466,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "B": "Apenas transferir dados entre clusters",
      "C": "Gerenciar permissões de usuários",
      "D": "Fazer backup de notebooks"
    },
    "correctAnswer": "A",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 467,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Criptografia automática",
      "B": "Schema Evolution automática e detecção de novos arquivos",
      "C": "Eliminação de duplicatas",
      "D": "Apenas compressão de dados"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 468,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Descarta automaticamente",
      "B": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "C": "Falha e interrompe a pipeline",
      "D": "Converte tudo para string"
    },
    "correctAnswer": "B",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 469,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas S3",
      "B": "S3, ADLS, GCS, SFTP",
      "C": "Apenas bancos de dados relacionais",
      "D": "Apenas APIs externas"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 470,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader não funciona com Delta",
      "B": "Não há diferença",
      "C": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "D": "COPY INTO é mais rápido"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 471,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "B": "Apenas transferir dados entre clusters",
      "C": "Gerenciar permissões de usuários",
      "D": "Fazer backup de notebooks"
    },
    "correctAnswer": "A",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 472,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Schema Evolution automática e detecção de novos arquivos",
      "B": "Eliminação de duplicatas",
      "C": "Apenas compressão de dados",
      "D": "Criptografia automática"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 473,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Falha e interrompe a pipeline",
      "B": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "C": "Converte tudo para string",
      "D": "Descarta automaticamente"
    },
    "correctAnswer": "B",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 474,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas bancos de dados relacionais",
      "B": "Apenas APIs externas",
      "C": "S3, ADLS, GCS, SFTP",
      "D": "Apenas S3"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 475,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "COPY INTO é mais rápido",
      "B": "Não há diferença",
      "C": "Auto Loader não funciona com Delta",
      "D": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 476,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Gerenciar permissões de usuários",
      "B": "Fazer backup de notebooks",
      "C": "Apenas transferir dados entre clusters",
      "D": "Conectar IDEs locais ao Databricks para desenvolvimento remoto"
    },
    "correctAnswer": "D",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 477,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Eliminação de duplicatas",
      "B": "Schema Evolution automática e detecção de novos arquivos",
      "C": "Criptografia automática",
      "D": "Apenas compressão de dados"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 478,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Descarta automaticamente",
      "B": "Converte tudo para string",
      "C": "Falha e interrompe a pipeline",
      "D": "Usa 'rescuedDataColumn' para capturar dados problemáticos"
    },
    "correctAnswer": "D",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 479,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "S3, ADLS, GCS, SFTP",
      "B": "Apenas S3",
      "C": "Apenas bancos de dados relacionais",
      "D": "Apenas APIs externas"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 480,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "B": "COPY INTO é mais rápido",
      "C": "Não há diferença",
      "D": "Auto Loader não funciona com Delta"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 481,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "B": "Gerenciar permissões de usuários",
      "C": "Apenas transferir dados entre clusters",
      "D": "Fazer backup de notebooks"
    },
    "correctAnswer": "A",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 482,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Apenas compressão de dados",
      "B": "Criptografia automática",
      "C": "Eliminação de duplicatas",
      "D": "Schema Evolution automática e detecção de novos arquivos"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 483,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Falha e interrompe a pipeline",
      "B": "Converte tudo para string",
      "C": "Descarta automaticamente",
      "D": "Usa 'rescuedDataColumn' para capturar dados problemáticos"
    },
    "correctAnswer": "D",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 484,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas S3",
      "B": "S3, ADLS, GCS, SFTP",
      "C": "Apenas bancos de dados relacionais",
      "D": "Apenas APIs externas"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 485,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "COPY INTO é mais rápido",
      "B": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "C": "Auto Loader não funciona com Delta",
      "D": "Não há diferença"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 486,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Apenas transferir dados entre clusters",
      "B": "Fazer backup de notebooks",
      "C": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "D": "Gerenciar permissões de usuários"
    },
    "correctAnswer": "C",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 487,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Criptografia automática",
      "B": "Schema Evolution automática e detecção de novos arquivos",
      "C": "Eliminação de duplicatas",
      "D": "Apenas compressão de dados"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 488,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "B": "Falha e interrompe a pipeline",
      "C": "Converte tudo para string",
      "D": "Descarta automaticamente"
    },
    "correctAnswer": "A",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 489,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "S3, ADLS, GCS, SFTP",
      "B": "Apenas bancos de dados relacionais",
      "C": "Apenas S3",
      "D": "Apenas APIs externas"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 490,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "COPY INTO é mais rápido",
      "B": "Auto Loader não funciona com Delta",
      "C": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "D": "Não há diferença"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 491,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "B": "Fazer backup de notebooks",
      "C": "Apenas transferir dados entre clusters",
      "D": "Gerenciar permissões de usuários"
    },
    "correctAnswer": "A",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 492,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Apenas compressão de dados",
      "B": "Eliminação de duplicatas",
      "C": "Criptografia automática",
      "D": "Schema Evolution automática e detecção de novos arquivos"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 493,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Converte tudo para string",
      "B": "Falha e interrompe a pipeline",
      "C": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "D": "Descarta automaticamente"
    },
    "correctAnswer": "C",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 494,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas APIs externas",
      "B": "S3, ADLS, GCS, SFTP",
      "C": "Apenas bancos de dados relacionais",
      "D": "Apenas S3"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 495,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader não funciona com Delta",
      "B": "Não há diferença",
      "C": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "D": "COPY INTO é mais rápido"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 496,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "B": "Fazer backup de notebooks",
      "C": "Apenas transferir dados entre clusters",
      "D": "Gerenciar permissões de usuários"
    },
    "correctAnswer": "A",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 497,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Eliminação de duplicatas",
      "B": "Criptografia automática",
      "C": "Schema Evolution automática e detecção de novos arquivos",
      "D": "Apenas compressão de dados"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 498,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Converte tudo para string",
      "B": "Descarta automaticamente",
      "C": "Falha e interrompe a pipeline",
      "D": "Usa 'rescuedDataColumn' para capturar dados problemáticos"
    },
    "correctAnswer": "D",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 499,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas bancos de dados relacionais",
      "B": "Apenas S3",
      "C": "S3, ADLS, GCS, SFTP",
      "D": "Apenas APIs externas"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 500,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Não há diferença",
      "B": "Auto Loader não funciona com Delta",
      "C": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "D": "COPY INTO é mais rápido"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 501,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Apenas transferir dados entre clusters",
      "B": "Gerenciar permissões de usuários",
      "C": "Fazer backup de notebooks",
      "D": "Conectar IDEs locais ao Databricks para desenvolvimento remoto"
    },
    "correctAnswer": "D",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 502,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Eliminação de duplicatas",
      "B": "Criptografia automática",
      "C": "Schema Evolution automática e detecção de novos arquivos",
      "D": "Apenas compressão de dados"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 503,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Descarta automaticamente",
      "B": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "C": "Converte tudo para string",
      "D": "Falha e interrompe a pipeline"
    },
    "correctAnswer": "B",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 504,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "S3, ADLS, GCS, SFTP",
      "B": "Apenas S3",
      "C": "Apenas APIs externas",
      "D": "Apenas bancos de dados relacionais"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 505,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Não há diferença",
      "B": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "C": "COPY INTO é mais rápido",
      "D": "Auto Loader não funciona com Delta"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 506,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Gerenciar permissões de usuários",
      "B": "Fazer backup de notebooks",
      "C": "Apenas transferir dados entre clusters",
      "D": "Conectar IDEs locais ao Databricks para desenvolvimento remoto"
    },
    "correctAnswer": "D",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 507,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Schema Evolution automática e detecção de novos arquivos",
      "B": "Criptografia automática",
      "C": "Eliminação de duplicatas",
      "D": "Apenas compressão de dados"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 508,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Descarta automaticamente",
      "B": "Converte tudo para string",
      "C": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "D": "Falha e interrompe a pipeline"
    },
    "correctAnswer": "C",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 509,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas S3",
      "B": "S3, ADLS, GCS, SFTP",
      "C": "Apenas APIs externas",
      "D": "Apenas bancos de dados relacionais"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 510,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "B": "Não há diferença",
      "C": "Auto Loader não funciona com Delta",
      "D": "COPY INTO é mais rápido"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 511,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Apenas transferir dados entre clusters",
      "B": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "C": "Fazer backup de notebooks",
      "D": "Gerenciar permissões de usuários"
    },
    "correctAnswer": "B",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 512,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Apenas compressão de dados",
      "B": "Schema Evolution automática e detecção de novos arquivos",
      "C": "Criptografia automática",
      "D": "Eliminação de duplicatas"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 513,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Converte tudo para string",
      "B": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "C": "Descarta automaticamente",
      "D": "Falha e interrompe a pipeline"
    },
    "correctAnswer": "B",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 514,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas APIs externas",
      "B": "Apenas bancos de dados relacionais",
      "C": "S3, ADLS, GCS, SFTP",
      "D": "Apenas S3"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 515,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader não funciona com Delta",
      "B": "COPY INTO é mais rápido",
      "C": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "D": "Não há diferença"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 516,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Fazer backup de notebooks",
      "B": "Apenas transferir dados entre clusters",
      "C": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "D": "Gerenciar permissões de usuários"
    },
    "correctAnswer": "C",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 517,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Apenas compressão de dados",
      "B": "Eliminação de duplicatas",
      "C": "Schema Evolution automática e detecção de novos arquivos",
      "D": "Criptografia automática"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 518,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Converte tudo para string",
      "B": "Descarta automaticamente",
      "C": "Falha e interrompe a pipeline",
      "D": "Usa 'rescuedDataColumn' para capturar dados problemáticos"
    },
    "correctAnswer": "D",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 519,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas APIs externas",
      "B": "Apenas bancos de dados relacionais",
      "C": "Apenas S3",
      "D": "S3, ADLS, GCS, SFTP"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 520,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "COPY INTO é mais rápido",
      "B": "Não há diferença",
      "C": "Auto Loader não funciona com Delta",
      "D": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 521,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "B": "Fazer backup de notebooks",
      "C": "Gerenciar permissões de usuários",
      "D": "Apenas transferir dados entre clusters"
    },
    "correctAnswer": "A",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 522,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Apenas compressão de dados",
      "B": "Eliminação de duplicatas",
      "C": "Schema Evolution automática e detecção de novos arquivos",
      "D": "Criptografia automática"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 523,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "B": "Falha e interrompe a pipeline",
      "C": "Converte tudo para string",
      "D": "Descarta automaticamente"
    },
    "correctAnswer": "A",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 524,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "S3, ADLS, GCS, SFTP",
      "B": "Apenas bancos de dados relacionais",
      "C": "Apenas APIs externas",
      "D": "Apenas S3"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 525,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "B": "Não há diferença",
      "C": "Auto Loader não funciona com Delta",
      "D": "COPY INTO é mais rápido"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 526,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Apenas transferir dados entre clusters",
      "B": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "C": "Fazer backup de notebooks",
      "D": "Gerenciar permissões de usuários"
    },
    "correctAnswer": "B",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 527,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Schema Evolution automática e detecção de novos arquivos",
      "B": "Eliminação de duplicatas",
      "C": "Criptografia automática",
      "D": "Apenas compressão de dados"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 528,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "B": "Descarta automaticamente",
      "C": "Converte tudo para string",
      "D": "Falha e interrompe a pipeline"
    },
    "correctAnswer": "A",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 529,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas APIs externas",
      "B": "S3, ADLS, GCS, SFTP",
      "C": "Apenas bancos de dados relacionais",
      "D": "Apenas S3"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 530,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "COPY INTO é mais rápido",
      "B": "Auto Loader não funciona com Delta",
      "C": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "D": "Não há diferença"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 531,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Fazer backup de notebooks",
      "B": "Apenas transferir dados entre clusters",
      "C": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "D": "Gerenciar permissões de usuários"
    },
    "correctAnswer": "C",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 532,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Criptografia automática",
      "B": "Schema Evolution automática e detecção de novos arquivos",
      "C": "Eliminação de duplicatas",
      "D": "Apenas compressão de dados"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 533,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Falha e interrompe a pipeline",
      "B": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "C": "Descarta automaticamente",
      "D": "Converte tudo para string"
    },
    "correctAnswer": "B",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 534,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas bancos de dados relacionais",
      "B": "Apenas APIs externas",
      "C": "Apenas S3",
      "D": "S3, ADLS, GCS, SFTP"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 535,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "B": "Auto Loader não funciona com Delta",
      "C": "COPY INTO é mais rápido",
      "D": "Não há diferença"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 536,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Apenas transferir dados entre clusters",
      "B": "Fazer backup de notebooks",
      "C": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "D": "Gerenciar permissões de usuários"
    },
    "correctAnswer": "C",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 537,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Criptografia automática",
      "B": "Eliminação de duplicatas",
      "C": "Apenas compressão de dados",
      "D": "Schema Evolution automática e detecção de novos arquivos"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 538,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Descarta automaticamente",
      "B": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "C": "Converte tudo para string",
      "D": "Falha e interrompe a pipeline"
    },
    "correctAnswer": "B",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 539,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "S3, ADLS, GCS, SFTP",
      "B": "Apenas bancos de dados relacionais",
      "C": "Apenas S3",
      "D": "Apenas APIs externas"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 540,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader não funciona com Delta",
      "B": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "C": "COPY INTO é mais rápido",
      "D": "Não há diferença"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 541,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Fazer backup de notebooks",
      "B": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "C": "Gerenciar permissões de usuários",
      "D": "Apenas transferir dados entre clusters"
    },
    "correctAnswer": "B",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 542,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Schema Evolution automática e detecção de novos arquivos",
      "B": "Criptografia automática",
      "C": "Eliminação de duplicatas",
      "D": "Apenas compressão de dados"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 543,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Descarta automaticamente",
      "B": "Converte tudo para string",
      "C": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "D": "Falha e interrompe a pipeline"
    },
    "correctAnswer": "C",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 544,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas APIs externas",
      "B": "Apenas S3",
      "C": "Apenas bancos de dados relacionais",
      "D": "S3, ADLS, GCS, SFTP"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 545,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "COPY INTO é mais rápido",
      "B": "Não há diferença",
      "C": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "D": "Auto Loader não funciona com Delta"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 546,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Apenas transferir dados entre clusters",
      "B": "Fazer backup de notebooks",
      "C": "Gerenciar permissões de usuários",
      "D": "Conectar IDEs locais ao Databricks para desenvolvimento remoto"
    },
    "correctAnswer": "D",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 547,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Eliminação de duplicatas",
      "B": "Apenas compressão de dados",
      "C": "Schema Evolution automática e detecção de novos arquivos",
      "D": "Criptografia automática"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 548,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "B": "Converte tudo para string",
      "C": "Descarta automaticamente",
      "D": "Falha e interrompe a pipeline"
    },
    "correctAnswer": "A",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 549,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "S3, ADLS, GCS, SFTP",
      "B": "Apenas S3",
      "C": "Apenas bancos de dados relacionais",
      "D": "Apenas APIs externas"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 550,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "B": "Auto Loader não funciona com Delta",
      "C": "COPY INTO é mais rápido",
      "D": "Não há diferença"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 551,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Apenas transferir dados entre clusters",
      "B": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "C": "Fazer backup de notebooks",
      "D": "Gerenciar permissões de usuários"
    },
    "correctAnswer": "B",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 552,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Apenas compressão de dados",
      "B": "Criptografia automática",
      "C": "Eliminação de duplicatas",
      "D": "Schema Evolution automática e detecção de novos arquivos"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 553,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Falha e interrompe a pipeline",
      "B": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "C": "Descarta automaticamente",
      "D": "Converte tudo para string"
    },
    "correctAnswer": "B",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 554,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas S3",
      "B": "S3, ADLS, GCS, SFTP",
      "C": "Apenas bancos de dados relacionais",
      "D": "Apenas APIs externas"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 555,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "B": "COPY INTO é mais rápido",
      "C": "Não há diferença",
      "D": "Auto Loader não funciona com Delta"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 556,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Apenas transferir dados entre clusters",
      "B": "Gerenciar permissões de usuários",
      "C": "Fazer backup de notebooks",
      "D": "Conectar IDEs locais ao Databricks para desenvolvimento remoto"
    },
    "correctAnswer": "D",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 557,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Criptografia automática",
      "B": "Eliminação de duplicatas",
      "C": "Schema Evolution automática e detecção de novos arquivos",
      "D": "Apenas compressão de dados"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 558,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "B": "Falha e interrompe a pipeline",
      "C": "Descarta automaticamente",
      "D": "Converte tudo para string"
    },
    "correctAnswer": "A",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 559,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "S3, ADLS, GCS, SFTP",
      "B": "Apenas bancos de dados relacionais",
      "C": "Apenas APIs externas",
      "D": "Apenas S3"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 560,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Não há diferença",
      "B": "Auto Loader não funciona com Delta",
      "C": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "D": "COPY INTO é mais rápido"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 561,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Apenas transferir dados entre clusters",
      "B": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "C": "Fazer backup de notebooks",
      "D": "Gerenciar permissões de usuários"
    },
    "correctAnswer": "B",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 562,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Schema Evolution automática e detecção de novos arquivos",
      "B": "Apenas compressão de dados",
      "C": "Criptografia automática",
      "D": "Eliminação de duplicatas"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 563,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Falha e interrompe a pipeline",
      "B": "Converte tudo para string",
      "C": "Descarta automaticamente",
      "D": "Usa 'rescuedDataColumn' para capturar dados problemáticos"
    },
    "correctAnswer": "D",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 564,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas S3",
      "B": "Apenas bancos de dados relacionais",
      "C": "S3, ADLS, GCS, SFTP",
      "D": "Apenas APIs externas"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 565,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "COPY INTO é mais rápido",
      "B": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "C": "Não há diferença",
      "D": "Auto Loader não funciona com Delta"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 566,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "B": "Gerenciar permissões de usuários",
      "C": "Apenas transferir dados entre clusters",
      "D": "Fazer backup de notebooks"
    },
    "correctAnswer": "A",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 567,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Apenas compressão de dados",
      "B": "Eliminação de duplicatas",
      "C": "Criptografia automática",
      "D": "Schema Evolution automática e detecção de novos arquivos"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 568,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Descarta automaticamente",
      "B": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "C": "Falha e interrompe a pipeline",
      "D": "Converte tudo para string"
    },
    "correctAnswer": "B",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 569,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "S3, ADLS, GCS, SFTP",
      "B": "Apenas S3",
      "C": "Apenas APIs externas",
      "D": "Apenas bancos de dados relacionais"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 570,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "COPY INTO é mais rápido",
      "B": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "C": "Não há diferença",
      "D": "Auto Loader não funciona com Delta"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 571,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "B": "Apenas transferir dados entre clusters",
      "C": "Gerenciar permissões de usuários",
      "D": "Fazer backup de notebooks"
    },
    "correctAnswer": "A",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 572,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Apenas compressão de dados",
      "B": "Criptografia automática",
      "C": "Eliminação de duplicatas",
      "D": "Schema Evolution automática e detecção de novos arquivos"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 573,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Converte tudo para string",
      "B": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "C": "Falha e interrompe a pipeline",
      "D": "Descarta automaticamente"
    },
    "correctAnswer": "B",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 574,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas APIs externas",
      "B": "Apenas bancos de dados relacionais",
      "C": "S3, ADLS, GCS, SFTP",
      "D": "Apenas S3"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 575,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "COPY INTO é mais rápido",
      "B": "Não há diferença",
      "C": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "D": "Auto Loader não funciona com Delta"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 576,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Fazer backup de notebooks",
      "B": "Gerenciar permissões de usuários",
      "C": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "D": "Apenas transferir dados entre clusters"
    },
    "correctAnswer": "C",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 577,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Eliminação de duplicatas",
      "B": "Criptografia automática",
      "C": "Schema Evolution automática e detecção de novos arquivos",
      "D": "Apenas compressão de dados"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 578,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "B": "Converte tudo para string",
      "C": "Falha e interrompe a pipeline",
      "D": "Descarta automaticamente"
    },
    "correctAnswer": "A",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 579,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas APIs externas",
      "B": "S3, ADLS, GCS, SFTP",
      "C": "Apenas bancos de dados relacionais",
      "D": "Apenas S3"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 580,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "B": "Auto Loader não funciona com Delta",
      "C": "COPY INTO é mais rápido",
      "D": "Não há diferença"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 581,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Apenas transferir dados entre clusters",
      "B": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "C": "Fazer backup de notebooks",
      "D": "Gerenciar permissões de usuários"
    },
    "correctAnswer": "B",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 582,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Schema Evolution automática e detecção de novos arquivos",
      "B": "Criptografia automática",
      "C": "Apenas compressão de dados",
      "D": "Eliminação de duplicatas"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 583,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "B": "Descarta automaticamente",
      "C": "Falha e interrompe a pipeline",
      "D": "Converte tudo para string"
    },
    "correctAnswer": "A",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 584,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas bancos de dados relacionais",
      "B": "Apenas APIs externas",
      "C": "S3, ADLS, GCS, SFTP",
      "D": "Apenas S3"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 585,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Não há diferença",
      "B": "COPY INTO é mais rápido",
      "C": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "D": "Auto Loader não funciona com Delta"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 586,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Gerenciar permissões de usuários",
      "B": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "C": "Fazer backup de notebooks",
      "D": "Apenas transferir dados entre clusters"
    },
    "correctAnswer": "B",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 587,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Eliminação de duplicatas",
      "B": "Criptografia automática",
      "C": "Schema Evolution automática e detecção de novos arquivos",
      "D": "Apenas compressão de dados"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 588,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Falha e interrompe a pipeline",
      "B": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "C": "Descarta automaticamente",
      "D": "Converte tudo para string"
    },
    "correctAnswer": "B",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 589,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas S3",
      "B": "Apenas bancos de dados relacionais",
      "C": "S3, ADLS, GCS, SFTP",
      "D": "Apenas APIs externas"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 590,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader não funciona com Delta",
      "B": "Não há diferença",
      "C": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "D": "COPY INTO é mais rápido"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 591,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Gerenciar permissões de usuários",
      "B": "Apenas transferir dados entre clusters",
      "C": "Fazer backup de notebooks",
      "D": "Conectar IDEs locais ao Databricks para desenvolvimento remoto"
    },
    "correctAnswer": "D",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 592,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Eliminação de duplicatas",
      "B": "Apenas compressão de dados",
      "C": "Criptografia automática",
      "D": "Schema Evolution automática e detecção de novos arquivos"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 593,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Falha e interrompe a pipeline",
      "B": "Converte tudo para string",
      "C": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "D": "Descarta automaticamente"
    },
    "correctAnswer": "C",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 594,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas S3",
      "B": "Apenas bancos de dados relacionais",
      "C": "S3, ADLS, GCS, SFTP",
      "D": "Apenas APIs externas"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 595,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "COPY INTO é mais rápido",
      "B": "Não há diferença",
      "C": "Auto Loader não funciona com Delta",
      "D": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 596,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Fazer backup de notebooks",
      "B": "Apenas transferir dados entre clusters",
      "C": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "D": "Gerenciar permissões de usuários"
    },
    "correctAnswer": "C",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 597,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Eliminação de duplicatas",
      "B": "Criptografia automática",
      "C": "Schema Evolution automática e detecção de novos arquivos",
      "D": "Apenas compressão de dados"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 598,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Descarta automaticamente",
      "B": "Converte tudo para string",
      "C": "Falha e interrompe a pipeline",
      "D": "Usa 'rescuedDataColumn' para capturar dados problemáticos"
    },
    "correctAnswer": "D",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 599,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "S3, ADLS, GCS, SFTP",
      "B": "Apenas S3",
      "C": "Apenas APIs externas",
      "D": "Apenas bancos de dados relacionais"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 600,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader não funciona com Delta",
      "B": "Não há diferença",
      "C": "COPY INTO é mais rápido",
      "D": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 601,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "B": "Fazer backup de notebooks",
      "C": "Gerenciar permissões de usuários",
      "D": "Apenas transferir dados entre clusters"
    },
    "correctAnswer": "A",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 602,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Schema Evolution automática e detecção de novos arquivos",
      "B": "Eliminação de duplicatas",
      "C": "Criptografia automática",
      "D": "Apenas compressão de dados"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 603,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Converte tudo para string",
      "B": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "C": "Falha e interrompe a pipeline",
      "D": "Descarta automaticamente"
    },
    "correctAnswer": "B",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 604,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas APIs externas",
      "B": "S3, ADLS, GCS, SFTP",
      "C": "Apenas bancos de dados relacionais",
      "D": "Apenas S3"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 605,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "COPY INTO é mais rápido",
      "B": "Não há diferença",
      "C": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "D": "Auto Loader não funciona com Delta"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 606,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Gerenciar permissões de usuários",
      "B": "Apenas transferir dados entre clusters",
      "C": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "D": "Fazer backup de notebooks"
    },
    "correctAnswer": "C",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 607,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Criptografia automática",
      "B": "Schema Evolution automática e detecção de novos arquivos",
      "C": "Eliminação de duplicatas",
      "D": "Apenas compressão de dados"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 608,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Descarta automaticamente",
      "B": "Converte tudo para string",
      "C": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "D": "Falha e interrompe a pipeline"
    },
    "correctAnswer": "C",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 609,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "S3, ADLS, GCS, SFTP",
      "B": "Apenas S3",
      "C": "Apenas bancos de dados relacionais",
      "D": "Apenas APIs externas"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 610,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Não há diferença",
      "B": "COPY INTO é mais rápido",
      "C": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "D": "Auto Loader não funciona com Delta"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 611,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Apenas transferir dados entre clusters",
      "B": "Fazer backup de notebooks",
      "C": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "D": "Gerenciar permissões de usuários"
    },
    "correctAnswer": "C",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 612,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Criptografia automática",
      "B": "Eliminação de duplicatas",
      "C": "Schema Evolution automática e detecção de novos arquivos",
      "D": "Apenas compressão de dados"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 613,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Falha e interrompe a pipeline",
      "B": "Descarta automaticamente",
      "C": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "D": "Converte tudo para string"
    },
    "correctAnswer": "C",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 614,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas S3",
      "B": "S3, ADLS, GCS, SFTP",
      "C": "Apenas APIs externas",
      "D": "Apenas bancos de dados relacionais"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 615,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader não funciona com Delta",
      "B": "COPY INTO é mais rápido",
      "C": "Não há diferença",
      "D": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 616,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "B": "Fazer backup de notebooks",
      "C": "Apenas transferir dados entre clusters",
      "D": "Gerenciar permissões de usuários"
    },
    "correctAnswer": "A",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 617,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Schema Evolution automática e detecção de novos arquivos",
      "B": "Eliminação de duplicatas",
      "C": "Apenas compressão de dados",
      "D": "Criptografia automática"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 618,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Converte tudo para string",
      "B": "Descarta automaticamente",
      "C": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "D": "Falha e interrompe a pipeline"
    },
    "correctAnswer": "C",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 619,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "S3, ADLS, GCS, SFTP",
      "B": "Apenas S3",
      "C": "Apenas bancos de dados relacionais",
      "D": "Apenas APIs externas"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 620,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "COPY INTO é mais rápido",
      "B": "Não há diferença",
      "C": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "D": "Auto Loader não funciona com Delta"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 621,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Fazer backup de notebooks",
      "B": "Gerenciar permissões de usuários",
      "C": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "D": "Apenas transferir dados entre clusters"
    },
    "correctAnswer": "C",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 622,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Eliminação de duplicatas",
      "B": "Criptografia automática",
      "C": "Schema Evolution automática e detecção de novos arquivos",
      "D": "Apenas compressão de dados"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 623,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Converte tudo para string",
      "B": "Descarta automaticamente",
      "C": "Falha e interrompe a pipeline",
      "D": "Usa 'rescuedDataColumn' para capturar dados problemáticos"
    },
    "correctAnswer": "D",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 624,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas bancos de dados relacionais",
      "B": "S3, ADLS, GCS, SFTP",
      "C": "Apenas S3",
      "D": "Apenas APIs externas"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 625,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Não há diferença",
      "B": "COPY INTO é mais rápido",
      "C": "Auto Loader não funciona com Delta",
      "D": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 626,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Fazer backup de notebooks",
      "B": "Apenas transferir dados entre clusters",
      "C": "Gerenciar permissões de usuários",
      "D": "Conectar IDEs locais ao Databricks para desenvolvimento remoto"
    },
    "correctAnswer": "D",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 627,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Apenas compressão de dados",
      "B": "Schema Evolution automática e detecção de novos arquivos",
      "C": "Eliminação de duplicatas",
      "D": "Criptografia automática"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 628,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Descarta automaticamente",
      "B": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "C": "Falha e interrompe a pipeline",
      "D": "Converte tudo para string"
    },
    "correctAnswer": "B",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 629,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "S3, ADLS, GCS, SFTP",
      "B": "Apenas bancos de dados relacionais",
      "C": "Apenas APIs externas",
      "D": "Apenas S3"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 630,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "B": "Não há diferença",
      "C": "COPY INTO é mais rápido",
      "D": "Auto Loader não funciona com Delta"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 631,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Gerenciar permissões de usuários",
      "B": "Fazer backup de notebooks",
      "C": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "D": "Apenas transferir dados entre clusters"
    },
    "correctAnswer": "C",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 632,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Schema Evolution automática e detecção de novos arquivos",
      "B": "Apenas compressão de dados",
      "C": "Eliminação de duplicatas",
      "D": "Criptografia automática"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 633,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Falha e interrompe a pipeline",
      "B": "Converte tudo para string",
      "C": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "D": "Descarta automaticamente"
    },
    "correctAnswer": "C",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 634,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas APIs externas",
      "B": "Apenas S3",
      "C": "S3, ADLS, GCS, SFTP",
      "D": "Apenas bancos de dados relacionais"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 635,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "B": "Não há diferença",
      "C": "COPY INTO é mais rápido",
      "D": "Auto Loader não funciona com Delta"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 636,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Apenas transferir dados entre clusters",
      "B": "Fazer backup de notebooks",
      "C": "Gerenciar permissões de usuários",
      "D": "Conectar IDEs locais ao Databricks para desenvolvimento remoto"
    },
    "correctAnswer": "D",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 637,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Criptografia automática",
      "B": "Eliminação de duplicatas",
      "C": "Apenas compressão de dados",
      "D": "Schema Evolution automática e detecção de novos arquivos"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 638,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Converte tudo para string",
      "B": "Descarta automaticamente",
      "C": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "D": "Falha e interrompe a pipeline"
    },
    "correctAnswer": "C",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 639,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas bancos de dados relacionais",
      "B": "S3, ADLS, GCS, SFTP",
      "C": "Apenas APIs externas",
      "D": "Apenas S3"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 640,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "B": "Auto Loader não funciona com Delta",
      "C": "Não há diferença",
      "D": "COPY INTO é mais rápido"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 641,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "B": "Apenas transferir dados entre clusters",
      "C": "Gerenciar permissões de usuários",
      "D": "Fazer backup de notebooks"
    },
    "correctAnswer": "A",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 642,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Eliminação de duplicatas",
      "B": "Apenas compressão de dados",
      "C": "Criptografia automática",
      "D": "Schema Evolution automática e detecção de novos arquivos"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 643,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Descarta automaticamente",
      "B": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "C": "Falha e interrompe a pipeline",
      "D": "Converte tudo para string"
    },
    "correctAnswer": "B",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 644,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas S3",
      "B": "Apenas APIs externas",
      "C": "S3, ADLS, GCS, SFTP",
      "D": "Apenas bancos de dados relacionais"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 645,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Não há diferença",
      "B": "Auto Loader não funciona com Delta",
      "C": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "D": "COPY INTO é mais rápido"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 646,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Gerenciar permissões de usuários",
      "B": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "C": "Fazer backup de notebooks",
      "D": "Apenas transferir dados entre clusters"
    },
    "correctAnswer": "B",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 647,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Eliminação de duplicatas",
      "B": "Apenas compressão de dados",
      "C": "Schema Evolution automática e detecção de novos arquivos",
      "D": "Criptografia automática"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 648,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Falha e interrompe a pipeline",
      "B": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "C": "Converte tudo para string",
      "D": "Descarta automaticamente"
    },
    "correctAnswer": "B",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 649,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas APIs externas",
      "B": "S3, ADLS, GCS, SFTP",
      "C": "Apenas S3",
      "D": "Apenas bancos de dados relacionais"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 650,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "B": "COPY INTO é mais rápido",
      "C": "Não há diferença",
      "D": "Auto Loader não funciona com Delta"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 651,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Apenas transferir dados entre clusters",
      "B": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "C": "Gerenciar permissões de usuários",
      "D": "Fazer backup de notebooks"
    },
    "correctAnswer": "B",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 652,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Apenas compressão de dados",
      "B": "Criptografia automática",
      "C": "Schema Evolution automática e detecção de novos arquivos",
      "D": "Eliminação de duplicatas"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 653,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Descarta automaticamente",
      "B": "Converte tudo para string",
      "C": "Falha e interrompe a pipeline",
      "D": "Usa 'rescuedDataColumn' para capturar dados problemáticos"
    },
    "correctAnswer": "D",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 654,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas bancos de dados relacionais",
      "B": "Apenas S3",
      "C": "Apenas APIs externas",
      "D": "S3, ADLS, GCS, SFTP"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 655,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Não há diferença",
      "B": "COPY INTO é mais rápido",
      "C": "Auto Loader não funciona com Delta",
      "D": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 656,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Gerenciar permissões de usuários",
      "B": "Fazer backup de notebooks",
      "C": "Apenas transferir dados entre clusters",
      "D": "Conectar IDEs locais ao Databricks para desenvolvimento remoto"
    },
    "correctAnswer": "D",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 657,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Criptografia automática",
      "B": "Schema Evolution automática e detecção de novos arquivos",
      "C": "Eliminação de duplicatas",
      "D": "Apenas compressão de dados"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 658,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Falha e interrompe a pipeline",
      "B": "Converte tudo para string",
      "C": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "D": "Descarta automaticamente"
    },
    "correctAnswer": "C",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 659,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "S3, ADLS, GCS, SFTP",
      "B": "Apenas APIs externas",
      "C": "Apenas S3",
      "D": "Apenas bancos de dados relacionais"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 660,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "COPY INTO é mais rápido",
      "B": "Não há diferença",
      "C": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "D": "Auto Loader não funciona com Delta"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 661,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Apenas transferir dados entre clusters",
      "B": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "C": "Gerenciar permissões de usuários",
      "D": "Fazer backup de notebooks"
    },
    "correctAnswer": "B",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 662,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Eliminação de duplicatas",
      "B": "Criptografia automática",
      "C": "Apenas compressão de dados",
      "D": "Schema Evolution automática e detecção de novos arquivos"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 663,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Falha e interrompe a pipeline",
      "B": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "C": "Converte tudo para string",
      "D": "Descarta automaticamente"
    },
    "correctAnswer": "B",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 664,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "S3, ADLS, GCS, SFTP",
      "B": "Apenas S3",
      "C": "Apenas bancos de dados relacionais",
      "D": "Apenas APIs externas"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 665,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "COPY INTO é mais rápido",
      "B": "Não há diferença",
      "C": "Auto Loader não funciona com Delta",
      "D": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 666,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Gerenciar permissões de usuários",
      "B": "Fazer backup de notebooks",
      "C": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "D": "Apenas transferir dados entre clusters"
    },
    "correctAnswer": "C",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 667,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Criptografia automática",
      "B": "Schema Evolution automática e detecção de novos arquivos",
      "C": "Eliminação de duplicatas",
      "D": "Apenas compressão de dados"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 668,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Falha e interrompe a pipeline",
      "B": "Descarta automaticamente",
      "C": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "D": "Converte tudo para string"
    },
    "correctAnswer": "C",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 669,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas S3",
      "B": "Apenas bancos de dados relacionais",
      "C": "Apenas APIs externas",
      "D": "S3, ADLS, GCS, SFTP"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 670,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Não há diferença",
      "B": "Auto Loader não funciona com Delta",
      "C": "COPY INTO é mais rápido",
      "D": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 671,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Gerenciar permissões de usuários",
      "B": "Fazer backup de notebooks",
      "C": "Apenas transferir dados entre clusters",
      "D": "Conectar IDEs locais ao Databricks para desenvolvimento remoto"
    },
    "correctAnswer": "D",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 672,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Schema Evolution automática e detecção de novos arquivos",
      "B": "Eliminação de duplicatas",
      "C": "Apenas compressão de dados",
      "D": "Criptografia automática"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 673,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Converte tudo para string",
      "B": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "C": "Descarta automaticamente",
      "D": "Falha e interrompe a pipeline"
    },
    "correctAnswer": "B",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 674,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "S3, ADLS, GCS, SFTP",
      "B": "Apenas bancos de dados relacionais",
      "C": "Apenas APIs externas",
      "D": "Apenas S3"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 675,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Não há diferença",
      "B": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "C": "COPY INTO é mais rápido",
      "D": "Auto Loader não funciona com Delta"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 676,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Fazer backup de notebooks",
      "B": "Apenas transferir dados entre clusters",
      "C": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "D": "Gerenciar permissões de usuários"
    },
    "correctAnswer": "C",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 677,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Eliminação de duplicatas",
      "B": "Criptografia automática",
      "C": "Apenas compressão de dados",
      "D": "Schema Evolution automática e detecção de novos arquivos"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 678,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Falha e interrompe a pipeline",
      "B": "Descarta automaticamente",
      "C": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "D": "Converte tudo para string"
    },
    "correctAnswer": "C",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 679,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "S3, ADLS, GCS, SFTP",
      "B": "Apenas bancos de dados relacionais",
      "C": "Apenas APIs externas",
      "D": "Apenas S3"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 680,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "COPY INTO é mais rápido",
      "B": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "C": "Não há diferença",
      "D": "Auto Loader não funciona com Delta"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 681,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Gerenciar permissões de usuários",
      "B": "Fazer backup de notebooks",
      "C": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "D": "Apenas transferir dados entre clusters"
    },
    "correctAnswer": "C",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 682,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Eliminação de duplicatas",
      "B": "Criptografia automática",
      "C": "Schema Evolution automática e detecção de novos arquivos",
      "D": "Apenas compressão de dados"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 683,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Falha e interrompe a pipeline",
      "B": "Converte tudo para string",
      "C": "Descarta automaticamente",
      "D": "Usa 'rescuedDataColumn' para capturar dados problemáticos"
    },
    "correctAnswer": "D",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 684,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "S3, ADLS, GCS, SFTP",
      "B": "Apenas bancos de dados relacionais",
      "C": "Apenas APIs externas",
      "D": "Apenas S3"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 685,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "COPY INTO é mais rápido",
      "B": "Não há diferença",
      "C": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "D": "Auto Loader não funciona com Delta"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 686,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "B": "Apenas transferir dados entre clusters",
      "C": "Gerenciar permissões de usuários",
      "D": "Fazer backup de notebooks"
    },
    "correctAnswer": "A",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 687,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Schema Evolution automática e detecção de novos arquivos",
      "B": "Eliminação de duplicatas",
      "C": "Criptografia automática",
      "D": "Apenas compressão de dados"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 688,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Converte tudo para string",
      "B": "Falha e interrompe a pipeline",
      "C": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "D": "Descarta automaticamente"
    },
    "correctAnswer": "C",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 689,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas S3",
      "B": "S3, ADLS, GCS, SFTP",
      "C": "Apenas APIs externas",
      "D": "Apenas bancos de dados relacionais"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 690,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "B": "COPY INTO é mais rápido",
      "C": "Auto Loader não funciona com Delta",
      "D": "Não há diferença"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 691,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "B": "Apenas transferir dados entre clusters",
      "C": "Gerenciar permissões de usuários",
      "D": "Fazer backup de notebooks"
    },
    "correctAnswer": "A",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 692,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Eliminação de duplicatas",
      "B": "Schema Evolution automática e detecção de novos arquivos",
      "C": "Apenas compressão de dados",
      "D": "Criptografia automática"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 693,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "B": "Converte tudo para string",
      "C": "Falha e interrompe a pipeline",
      "D": "Descarta automaticamente"
    },
    "correctAnswer": "A",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 694,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas bancos de dados relacionais",
      "B": "S3, ADLS, GCS, SFTP",
      "C": "Apenas S3",
      "D": "Apenas APIs externas"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 695,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "B": "COPY INTO é mais rápido",
      "C": "Auto Loader não funciona com Delta",
      "D": "Não há diferença"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 696,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "B": "Gerenciar permissões de usuários",
      "C": "Apenas transferir dados entre clusters",
      "D": "Fazer backup de notebooks"
    },
    "correctAnswer": "A",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 697,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Schema Evolution automática e detecção de novos arquivos",
      "B": "Eliminação de duplicatas",
      "C": "Apenas compressão de dados",
      "D": "Criptografia automática"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 698,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Converte tudo para string",
      "B": "Descarta automaticamente",
      "C": "Falha e interrompe a pipeline",
      "D": "Usa 'rescuedDataColumn' para capturar dados problemáticos"
    },
    "correctAnswer": "D",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 699,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas bancos de dados relacionais",
      "B": "Apenas APIs externas",
      "C": "S3, ADLS, GCS, SFTP",
      "D": "Apenas S3"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 700,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Não há diferença",
      "B": "Auto Loader não funciona com Delta",
      "C": "COPY INTO é mais rápido",
      "D": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 701,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Apenas transferir dados entre clusters",
      "B": "Gerenciar permissões de usuários",
      "C": "Fazer backup de notebooks",
      "D": "Conectar IDEs locais ao Databricks para desenvolvimento remoto"
    },
    "correctAnswer": "D",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 702,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Criptografia automática",
      "B": "Eliminação de duplicatas",
      "C": "Schema Evolution automática e detecção de novos arquivos",
      "D": "Apenas compressão de dados"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 703,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Converte tudo para string",
      "B": "Falha e interrompe a pipeline",
      "C": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "D": "Descarta automaticamente"
    },
    "correctAnswer": "C",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 704,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas bancos de dados relacionais",
      "B": "Apenas APIs externas",
      "C": "Apenas S3",
      "D": "S3, ADLS, GCS, SFTP"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 705,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Não há diferença",
      "B": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "C": "COPY INTO é mais rápido",
      "D": "Auto Loader não funciona com Delta"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 706,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Fazer backup de notebooks",
      "B": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "C": "Gerenciar permissões de usuários",
      "D": "Apenas transferir dados entre clusters"
    },
    "correctAnswer": "B",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 707,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Criptografia automática",
      "B": "Eliminação de duplicatas",
      "C": "Schema Evolution automática e detecção de novos arquivos",
      "D": "Apenas compressão de dados"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 708,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Converte tudo para string",
      "B": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "C": "Falha e interrompe a pipeline",
      "D": "Descarta automaticamente"
    },
    "correctAnswer": "B",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 709,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "S3, ADLS, GCS, SFTP",
      "B": "Apenas S3",
      "C": "Apenas bancos de dados relacionais",
      "D": "Apenas APIs externas"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 710,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Não há diferença",
      "B": "Auto Loader não funciona com Delta",
      "C": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "D": "COPY INTO é mais rápido"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 711,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Fazer backup de notebooks",
      "B": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "C": "Apenas transferir dados entre clusters",
      "D": "Gerenciar permissões de usuários"
    },
    "correctAnswer": "B",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 712,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Eliminação de duplicatas",
      "B": "Criptografia automática",
      "C": "Apenas compressão de dados",
      "D": "Schema Evolution automática e detecção de novos arquivos"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 713,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Converte tudo para string",
      "B": "Descarta automaticamente",
      "C": "Falha e interrompe a pipeline",
      "D": "Usa 'rescuedDataColumn' para capturar dados problemáticos"
    },
    "correctAnswer": "D",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 714,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas bancos de dados relacionais",
      "B": "Apenas S3",
      "C": "Apenas APIs externas",
      "D": "S3, ADLS, GCS, SFTP"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 715,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader não funciona com Delta",
      "B": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "C": "Não há diferença",
      "D": "COPY INTO é mais rápido"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 716,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Gerenciar permissões de usuários",
      "B": "Apenas transferir dados entre clusters",
      "C": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "D": "Fazer backup de notebooks"
    },
    "correctAnswer": "C",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 717,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Criptografia automática",
      "B": "Schema Evolution automática e detecção de novos arquivos",
      "C": "Apenas compressão de dados",
      "D": "Eliminação de duplicatas"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 718,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Falha e interrompe a pipeline",
      "B": "Descarta automaticamente",
      "C": "Converte tudo para string",
      "D": "Usa 'rescuedDataColumn' para capturar dados problemáticos"
    },
    "correctAnswer": "D",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 719,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "S3, ADLS, GCS, SFTP",
      "B": "Apenas APIs externas",
      "C": "Apenas S3",
      "D": "Apenas bancos de dados relacionais"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 720,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "COPY INTO é mais rápido",
      "B": "Auto Loader não funciona com Delta",
      "C": "Não há diferença",
      "D": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 721,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Fazer backup de notebooks",
      "B": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "C": "Apenas transferir dados entre clusters",
      "D": "Gerenciar permissões de usuários"
    },
    "correctAnswer": "B",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 722,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Apenas compressão de dados",
      "B": "Eliminação de duplicatas",
      "C": "Criptografia automática",
      "D": "Schema Evolution automática e detecção de novos arquivos"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 723,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Descarta automaticamente",
      "B": "Converte tudo para string",
      "C": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "D": "Falha e interrompe a pipeline"
    },
    "correctAnswer": "C",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 724,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas S3",
      "B": "Apenas APIs externas",
      "C": "Apenas bancos de dados relacionais",
      "D": "S3, ADLS, GCS, SFTP"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 725,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "COPY INTO é mais rápido",
      "B": "Auto Loader não funciona com Delta",
      "C": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "D": "Não há diferença"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 726,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Apenas transferir dados entre clusters",
      "B": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "C": "Gerenciar permissões de usuários",
      "D": "Fazer backup de notebooks"
    },
    "correctAnswer": "B",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 727,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Apenas compressão de dados",
      "B": "Eliminação de duplicatas",
      "C": "Schema Evolution automática e detecção de novos arquivos",
      "D": "Criptografia automática"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 728,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "B": "Converte tudo para string",
      "C": "Falha e interrompe a pipeline",
      "D": "Descarta automaticamente"
    },
    "correctAnswer": "A",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 729,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas S3",
      "B": "S3, ADLS, GCS, SFTP",
      "C": "Apenas APIs externas",
      "D": "Apenas bancos de dados relacionais"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 730,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Não há diferença",
      "B": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "C": "Auto Loader não funciona com Delta",
      "D": "COPY INTO é mais rápido"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 731,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Apenas transferir dados entre clusters",
      "B": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "C": "Gerenciar permissões de usuários",
      "D": "Fazer backup de notebooks"
    },
    "correctAnswer": "B",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 732,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Schema Evolution automática e detecção de novos arquivos",
      "B": "Apenas compressão de dados",
      "C": "Criptografia automática",
      "D": "Eliminação de duplicatas"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 733,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Descarta automaticamente",
      "B": "Falha e interrompe a pipeline",
      "C": "Converte tudo para string",
      "D": "Usa 'rescuedDataColumn' para capturar dados problemáticos"
    },
    "correctAnswer": "D",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 734,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas bancos de dados relacionais",
      "B": "Apenas APIs externas",
      "C": "Apenas S3",
      "D": "S3, ADLS, GCS, SFTP"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 735,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Não há diferença",
      "B": "COPY INTO é mais rápido",
      "C": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "D": "Auto Loader não funciona com Delta"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 736,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "B": "Gerenciar permissões de usuários",
      "C": "Fazer backup de notebooks",
      "D": "Apenas transferir dados entre clusters"
    },
    "correctAnswer": "A",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 737,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Eliminação de duplicatas",
      "B": "Criptografia automática",
      "C": "Apenas compressão de dados",
      "D": "Schema Evolution automática e detecção de novos arquivos"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 738,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "B": "Descarta automaticamente",
      "C": "Falha e interrompe a pipeline",
      "D": "Converte tudo para string"
    },
    "correctAnswer": "A",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 739,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "S3, ADLS, GCS, SFTP",
      "B": "Apenas bancos de dados relacionais",
      "C": "Apenas APIs externas",
      "D": "Apenas S3"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 740,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader não funciona com Delta",
      "B": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "C": "Não há diferença",
      "D": "COPY INTO é mais rápido"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 741,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "B": "Apenas transferir dados entre clusters",
      "C": "Fazer backup de notebooks",
      "D": "Gerenciar permissões de usuários"
    },
    "correctAnswer": "A",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 742,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Eliminação de duplicatas",
      "B": "Criptografia automática",
      "C": "Apenas compressão de dados",
      "D": "Schema Evolution automática e detecção de novos arquivos"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 743,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Descarta automaticamente",
      "B": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "C": "Converte tudo para string",
      "D": "Falha e interrompe a pipeline"
    },
    "correctAnswer": "B",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 744,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas bancos de dados relacionais",
      "B": "S3, ADLS, GCS, SFTP",
      "C": "Apenas APIs externas",
      "D": "Apenas S3"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 745,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Não há diferença",
      "B": "COPY INTO é mais rápido",
      "C": "Auto Loader não funciona com Delta",
      "D": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 746,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Gerenciar permissões de usuários",
      "B": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "C": "Fazer backup de notebooks",
      "D": "Apenas transferir dados entre clusters"
    },
    "correctAnswer": "B",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 747,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Eliminação de duplicatas",
      "B": "Schema Evolution automática e detecção de novos arquivos",
      "C": "Apenas compressão de dados",
      "D": "Criptografia automática"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 748,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Converte tudo para string",
      "B": "Falha e interrompe a pipeline",
      "C": "Descarta automaticamente",
      "D": "Usa 'rescuedDataColumn' para capturar dados problemáticos"
    },
    "correctAnswer": "D",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 749,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "S3, ADLS, GCS, SFTP",
      "B": "Apenas bancos de dados relacionais",
      "C": "Apenas S3",
      "D": "Apenas APIs externas"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 750,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Não há diferença",
      "B": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "C": "COPY INTO é mais rápido",
      "D": "Auto Loader não funciona com Delta"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 751,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Fazer backup de notebooks",
      "B": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "C": "Gerenciar permissões de usuários",
      "D": "Apenas transferir dados entre clusters"
    },
    "correctAnswer": "B",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 752,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Schema Evolution automática e detecção de novos arquivos",
      "B": "Eliminação de duplicatas",
      "C": "Apenas compressão de dados",
      "D": "Criptografia automática"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 753,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "B": "Falha e interrompe a pipeline",
      "C": "Converte tudo para string",
      "D": "Descarta automaticamente"
    },
    "correctAnswer": "A",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 754,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas APIs externas",
      "B": "Apenas bancos de dados relacionais",
      "C": "S3, ADLS, GCS, SFTP",
      "D": "Apenas S3"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 755,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader não funciona com Delta",
      "B": "COPY INTO é mais rápido",
      "C": "Não há diferença",
      "D": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 756,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Fazer backup de notebooks",
      "B": "Apenas transferir dados entre clusters",
      "C": "Gerenciar permissões de usuários",
      "D": "Conectar IDEs locais ao Databricks para desenvolvimento remoto"
    },
    "correctAnswer": "D",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 757,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Schema Evolution automática e detecção de novos arquivos",
      "B": "Criptografia automática",
      "C": "Eliminação de duplicatas",
      "D": "Apenas compressão de dados"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 758,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "B": "Falha e interrompe a pipeline",
      "C": "Descarta automaticamente",
      "D": "Converte tudo para string"
    },
    "correctAnswer": "A",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 759,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas APIs externas",
      "B": "Apenas bancos de dados relacionais",
      "C": "S3, ADLS, GCS, SFTP",
      "D": "Apenas S3"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 760,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "B": "Auto Loader não funciona com Delta",
      "C": "Não há diferença",
      "D": "COPY INTO é mais rápido"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 761,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Fazer backup de notebooks",
      "B": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "C": "Gerenciar permissões de usuários",
      "D": "Apenas transferir dados entre clusters"
    },
    "correctAnswer": "B",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 762,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Apenas compressão de dados",
      "B": "Eliminação de duplicatas",
      "C": "Schema Evolution automática e detecção de novos arquivos",
      "D": "Criptografia automática"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 763,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Converte tudo para string",
      "B": "Falha e interrompe a pipeline",
      "C": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "D": "Descarta automaticamente"
    },
    "correctAnswer": "C",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 764,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "S3, ADLS, GCS, SFTP",
      "B": "Apenas bancos de dados relacionais",
      "C": "Apenas APIs externas",
      "D": "Apenas S3"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 765,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "COPY INTO é mais rápido",
      "B": "Não há diferença",
      "C": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "D": "Auto Loader não funciona com Delta"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 766,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "B": "Fazer backup de notebooks",
      "C": "Apenas transferir dados entre clusters",
      "D": "Gerenciar permissões de usuários"
    },
    "correctAnswer": "A",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 767,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Schema Evolution automática e detecção de novos arquivos",
      "B": "Eliminação de duplicatas",
      "C": "Criptografia automática",
      "D": "Apenas compressão de dados"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 768,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Descarta automaticamente",
      "B": "Converte tudo para string",
      "C": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "D": "Falha e interrompe a pipeline"
    },
    "correctAnswer": "C",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 769,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas APIs externas",
      "B": "Apenas S3",
      "C": "Apenas bancos de dados relacionais",
      "D": "S3, ADLS, GCS, SFTP"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 770,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Não há diferença",
      "B": "COPY INTO é mais rápido",
      "C": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "D": "Auto Loader não funciona com Delta"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 771,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Fazer backup de notebooks",
      "B": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "C": "Apenas transferir dados entre clusters",
      "D": "Gerenciar permissões de usuários"
    },
    "correctAnswer": "B",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 772,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Criptografia automática",
      "B": "Schema Evolution automática e detecção de novos arquivos",
      "C": "Eliminação de duplicatas",
      "D": "Apenas compressão de dados"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 773,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Falha e interrompe a pipeline",
      "B": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "C": "Converte tudo para string",
      "D": "Descarta automaticamente"
    },
    "correctAnswer": "B",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 774,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "S3, ADLS, GCS, SFTP",
      "B": "Apenas bancos de dados relacionais",
      "C": "Apenas APIs externas",
      "D": "Apenas S3"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 775,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "COPY INTO é mais rápido",
      "B": "Não há diferença",
      "C": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "D": "Auto Loader não funciona com Delta"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 776,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Apenas transferir dados entre clusters",
      "B": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "C": "Gerenciar permissões de usuários",
      "D": "Fazer backup de notebooks"
    },
    "correctAnswer": "B",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 777,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Eliminação de duplicatas",
      "B": "Schema Evolution automática e detecção de novos arquivos",
      "C": "Apenas compressão de dados",
      "D": "Criptografia automática"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 778,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Falha e interrompe a pipeline",
      "B": "Descarta automaticamente",
      "C": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "D": "Converte tudo para string"
    },
    "correctAnswer": "C",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 779,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas APIs externas",
      "B": "S3, ADLS, GCS, SFTP",
      "C": "Apenas S3",
      "D": "Apenas bancos de dados relacionais"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 780,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "B": "COPY INTO é mais rápido",
      "C": "Auto Loader não funciona com Delta",
      "D": "Não há diferença"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 781,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Fazer backup de notebooks",
      "B": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "C": "Gerenciar permissões de usuários",
      "D": "Apenas transferir dados entre clusters"
    },
    "correctAnswer": "B",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 782,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Apenas compressão de dados",
      "B": "Criptografia automática",
      "C": "Schema Evolution automática e detecção de novos arquivos",
      "D": "Eliminação de duplicatas"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 783,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Falha e interrompe a pipeline",
      "B": "Converte tudo para string",
      "C": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "D": "Descarta automaticamente"
    },
    "correctAnswer": "C",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 784,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas bancos de dados relacionais",
      "B": "S3, ADLS, GCS, SFTP",
      "C": "Apenas S3",
      "D": "Apenas APIs externas"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 785,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "B": "Não há diferença",
      "C": "Auto Loader não funciona com Delta",
      "D": "COPY INTO é mais rápido"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 786,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "B": "Fazer backup de notebooks",
      "C": "Apenas transferir dados entre clusters",
      "D": "Gerenciar permissões de usuários"
    },
    "correctAnswer": "A",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 787,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Criptografia automática",
      "B": "Apenas compressão de dados",
      "C": "Schema Evolution automática e detecção de novos arquivos",
      "D": "Eliminação de duplicatas"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 788,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Falha e interrompe a pipeline",
      "B": "Converte tudo para string",
      "C": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "D": "Descarta automaticamente"
    },
    "correctAnswer": "C",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 789,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas S3",
      "B": "Apenas APIs externas",
      "C": "Apenas bancos de dados relacionais",
      "D": "S3, ADLS, GCS, SFTP"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 790,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Não há diferença",
      "B": "COPY INTO é mais rápido",
      "C": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "D": "Auto Loader não funciona com Delta"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 791,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Gerenciar permissões de usuários",
      "B": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "C": "Apenas transferir dados entre clusters",
      "D": "Fazer backup de notebooks"
    },
    "correctAnswer": "B",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 792,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Eliminação de duplicatas",
      "B": "Schema Evolution automática e detecção de novos arquivos",
      "C": "Criptografia automática",
      "D": "Apenas compressão de dados"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 793,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Converte tudo para string",
      "B": "Descarta automaticamente",
      "C": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "D": "Falha e interrompe a pipeline"
    },
    "correctAnswer": "C",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 794,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "S3, ADLS, GCS, SFTP",
      "B": "Apenas bancos de dados relacionais",
      "C": "Apenas S3",
      "D": "Apenas APIs externas"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 795,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Não há diferença",
      "B": "Auto Loader não funciona com Delta",
      "C": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "D": "COPY INTO é mais rápido"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 796,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Apenas transferir dados entre clusters",
      "B": "Gerenciar permissões de usuários",
      "C": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "D": "Fazer backup de notebooks"
    },
    "correctAnswer": "C",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 797,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Apenas compressão de dados",
      "B": "Criptografia automática",
      "C": "Schema Evolution automática e detecção de novos arquivos",
      "D": "Eliminação de duplicatas"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 798,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Descarta automaticamente",
      "B": "Converte tudo para string",
      "C": "Falha e interrompe a pipeline",
      "D": "Usa 'rescuedDataColumn' para capturar dados problemáticos"
    },
    "correctAnswer": "D",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 799,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "S3, ADLS, GCS, SFTP",
      "B": "Apenas bancos de dados relacionais",
      "C": "Apenas S3",
      "D": "Apenas APIs externas"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 800,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader não funciona com Delta",
      "B": "Não há diferença",
      "C": "COPY INTO é mais rápido",
      "D": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 801,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Apenas transferir dados entre clusters",
      "B": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "C": "Gerenciar permissões de usuários",
      "D": "Fazer backup de notebooks"
    },
    "correctAnswer": "B",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 802,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Criptografia automática",
      "B": "Eliminação de duplicatas",
      "C": "Schema Evolution automática e detecção de novos arquivos",
      "D": "Apenas compressão de dados"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 803,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Converte tudo para string",
      "B": "Descarta automaticamente",
      "C": "Falha e interrompe a pipeline",
      "D": "Usa 'rescuedDataColumn' para capturar dados problemáticos"
    },
    "correctAnswer": "D",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 804,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas bancos de dados relacionais",
      "B": "S3, ADLS, GCS, SFTP",
      "C": "Apenas APIs externas",
      "D": "Apenas S3"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 805,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "COPY INTO é mais rápido",
      "B": "Auto Loader não funciona com Delta",
      "C": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "D": "Não há diferença"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 806,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Apenas transferir dados entre clusters",
      "B": "Fazer backup de notebooks",
      "C": "Gerenciar permissões de usuários",
      "D": "Conectar IDEs locais ao Databricks para desenvolvimento remoto"
    },
    "correctAnswer": "D",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 807,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Schema Evolution automática e detecção de novos arquivos",
      "B": "Apenas compressão de dados",
      "C": "Criptografia automática",
      "D": "Eliminação de duplicatas"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 808,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Descarta automaticamente",
      "B": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "C": "Falha e interrompe a pipeline",
      "D": "Converte tudo para string"
    },
    "correctAnswer": "B",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 809,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas APIs externas",
      "B": "Apenas S3",
      "C": "Apenas bancos de dados relacionais",
      "D": "S3, ADLS, GCS, SFTP"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 810,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader não funciona com Delta",
      "B": "Não há diferença",
      "C": "COPY INTO é mais rápido",
      "D": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 811,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Gerenciar permissões de usuários",
      "B": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "C": "Apenas transferir dados entre clusters",
      "D": "Fazer backup de notebooks"
    },
    "correctAnswer": "B",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 812,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Schema Evolution automática e detecção de novos arquivos",
      "B": "Apenas compressão de dados",
      "C": "Eliminação de duplicatas",
      "D": "Criptografia automática"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 813,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Converte tudo para string",
      "B": "Falha e interrompe a pipeline",
      "C": "Descarta automaticamente",
      "D": "Usa 'rescuedDataColumn' para capturar dados problemáticos"
    },
    "correctAnswer": "D",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 814,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas bancos de dados relacionais",
      "B": "Apenas S3",
      "C": "Apenas APIs externas",
      "D": "S3, ADLS, GCS, SFTP"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 815,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "B": "Auto Loader não funciona com Delta",
      "C": "COPY INTO é mais rápido",
      "D": "Não há diferença"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 816,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "B": "Fazer backup de notebooks",
      "C": "Apenas transferir dados entre clusters",
      "D": "Gerenciar permissões de usuários"
    },
    "correctAnswer": "A",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 817,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Apenas compressão de dados",
      "B": "Eliminação de duplicatas",
      "C": "Criptografia automática",
      "D": "Schema Evolution automática e detecção de novos arquivos"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 818,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Converte tudo para string",
      "B": "Descarta automaticamente",
      "C": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "D": "Falha e interrompe a pipeline"
    },
    "correctAnswer": "C",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 819,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "S3, ADLS, GCS, SFTP",
      "B": "Apenas S3",
      "C": "Apenas bancos de dados relacionais",
      "D": "Apenas APIs externas"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 820,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader não funciona com Delta",
      "B": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "C": "Não há diferença",
      "D": "COPY INTO é mais rápido"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 821,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Fazer backup de notebooks",
      "B": "Gerenciar permissões de usuários",
      "C": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "D": "Apenas transferir dados entre clusters"
    },
    "correctAnswer": "C",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 822,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Schema Evolution automática e detecção de novos arquivos",
      "B": "Criptografia automática",
      "C": "Eliminação de duplicatas",
      "D": "Apenas compressão de dados"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 823,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Converte tudo para string",
      "B": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "C": "Falha e interrompe a pipeline",
      "D": "Descarta automaticamente"
    },
    "correctAnswer": "B",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 824,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas bancos de dados relacionais",
      "B": "S3, ADLS, GCS, SFTP",
      "C": "Apenas APIs externas",
      "D": "Apenas S3"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 825,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader não funciona com Delta",
      "B": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "C": "COPY INTO é mais rápido",
      "D": "Não há diferença"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 826,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Apenas transferir dados entre clusters",
      "B": "Gerenciar permissões de usuários",
      "C": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "D": "Fazer backup de notebooks"
    },
    "correctAnswer": "C",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 827,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Apenas compressão de dados",
      "B": "Criptografia automática",
      "C": "Eliminação de duplicatas",
      "D": "Schema Evolution automática e detecção de novos arquivos"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 828,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "B": "Falha e interrompe a pipeline",
      "C": "Descarta automaticamente",
      "D": "Converte tudo para string"
    },
    "correctAnswer": "A",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 829,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "S3, ADLS, GCS, SFTP",
      "B": "Apenas S3",
      "C": "Apenas APIs externas",
      "D": "Apenas bancos de dados relacionais"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 830,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Não há diferença",
      "B": "COPY INTO é mais rápido",
      "C": "Auto Loader não funciona com Delta",
      "D": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 831,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Fazer backup de notebooks",
      "B": "Apenas transferir dados entre clusters",
      "C": "Gerenciar permissões de usuários",
      "D": "Conectar IDEs locais ao Databricks para desenvolvimento remoto"
    },
    "correctAnswer": "D",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 832,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Eliminação de duplicatas",
      "B": "Criptografia automática",
      "C": "Apenas compressão de dados",
      "D": "Schema Evolution automática e detecção de novos arquivos"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 833,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Falha e interrompe a pipeline",
      "B": "Converte tudo para string",
      "C": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "D": "Descarta automaticamente"
    },
    "correctAnswer": "C",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 834,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas S3",
      "B": "Apenas bancos de dados relacionais",
      "C": "Apenas APIs externas",
      "D": "S3, ADLS, GCS, SFTP"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 835,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Não há diferença",
      "B": "COPY INTO é mais rápido",
      "C": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "D": "Auto Loader não funciona com Delta"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 836,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Fazer backup de notebooks",
      "B": "Apenas transferir dados entre clusters",
      "C": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "D": "Gerenciar permissões de usuários"
    },
    "correctAnswer": "C",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 837,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Criptografia automática",
      "B": "Eliminação de duplicatas",
      "C": "Schema Evolution automática e detecção de novos arquivos",
      "D": "Apenas compressão de dados"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 838,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Converte tudo para string",
      "B": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "C": "Descarta automaticamente",
      "D": "Falha e interrompe a pipeline"
    },
    "correctAnswer": "B",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 839,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas APIs externas",
      "B": "Apenas S3",
      "C": "Apenas bancos de dados relacionais",
      "D": "S3, ADLS, GCS, SFTP"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 840,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "COPY INTO é mais rápido",
      "B": "Não há diferença",
      "C": "Auto Loader não funciona com Delta",
      "D": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 841,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "B": "Gerenciar permissões de usuários",
      "C": "Fazer backup de notebooks",
      "D": "Apenas transferir dados entre clusters"
    },
    "correctAnswer": "A",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 842,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Schema Evolution automática e detecção de novos arquivos",
      "B": "Apenas compressão de dados",
      "C": "Eliminação de duplicatas",
      "D": "Criptografia automática"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 843,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Descarta automaticamente",
      "B": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "C": "Converte tudo para string",
      "D": "Falha e interrompe a pipeline"
    },
    "correctAnswer": "B",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 844,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas S3",
      "B": "S3, ADLS, GCS, SFTP",
      "C": "Apenas APIs externas",
      "D": "Apenas bancos de dados relacionais"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 845,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Não há diferença",
      "B": "COPY INTO é mais rápido",
      "C": "Auto Loader não funciona com Delta",
      "D": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 846,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Gerenciar permissões de usuários",
      "B": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "C": "Fazer backup de notebooks",
      "D": "Apenas transferir dados entre clusters"
    },
    "correctAnswer": "B",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 847,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Eliminação de duplicatas",
      "B": "Apenas compressão de dados",
      "C": "Schema Evolution automática e detecção de novos arquivos",
      "D": "Criptografia automática"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 848,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Converte tudo para string",
      "B": "Falha e interrompe a pipeline",
      "C": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "D": "Descarta automaticamente"
    },
    "correctAnswer": "C",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 849,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas APIs externas",
      "B": "S3, ADLS, GCS, SFTP",
      "C": "Apenas bancos de dados relacionais",
      "D": "Apenas S3"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 850,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Não há diferença",
      "B": "Auto Loader não funciona com Delta",
      "C": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "D": "COPY INTO é mais rápido"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 851,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "B": "Apenas transferir dados entre clusters",
      "C": "Fazer backup de notebooks",
      "D": "Gerenciar permissões de usuários"
    },
    "correctAnswer": "A",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 852,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Apenas compressão de dados",
      "B": "Criptografia automática",
      "C": "Schema Evolution automática e detecção de novos arquivos",
      "D": "Eliminação de duplicatas"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 853,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Converte tudo para string",
      "B": "Descarta automaticamente",
      "C": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "D": "Falha e interrompe a pipeline"
    },
    "correctAnswer": "C",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 854,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas APIs externas",
      "B": "Apenas bancos de dados relacionais",
      "C": "Apenas S3",
      "D": "S3, ADLS, GCS, SFTP"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 855,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "B": "COPY INTO é mais rápido",
      "C": "Auto Loader não funciona com Delta",
      "D": "Não há diferença"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 856,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Apenas transferir dados entre clusters",
      "B": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "C": "Fazer backup de notebooks",
      "D": "Gerenciar permissões de usuários"
    },
    "correctAnswer": "B",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 857,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Apenas compressão de dados",
      "B": "Schema Evolution automática e detecção de novos arquivos",
      "C": "Criptografia automática",
      "D": "Eliminação de duplicatas"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 858,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Falha e interrompe a pipeline",
      "B": "Converte tudo para string",
      "C": "Descarta automaticamente",
      "D": "Usa 'rescuedDataColumn' para capturar dados problemáticos"
    },
    "correctAnswer": "D",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 859,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas APIs externas",
      "B": "Apenas bancos de dados relacionais",
      "C": "Apenas S3",
      "D": "S3, ADLS, GCS, SFTP"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 860,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader não funciona com Delta",
      "B": "Não há diferença",
      "C": "COPY INTO é mais rápido",
      "D": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 861,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Fazer backup de notebooks",
      "B": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "C": "Apenas transferir dados entre clusters",
      "D": "Gerenciar permissões de usuários"
    },
    "correctAnswer": "B",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 862,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Apenas compressão de dados",
      "B": "Criptografia automática",
      "C": "Schema Evolution automática e detecção de novos arquivos",
      "D": "Eliminação de duplicatas"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 863,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Descarta automaticamente",
      "B": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "C": "Converte tudo para string",
      "D": "Falha e interrompe a pipeline"
    },
    "correctAnswer": "B",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 864,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas S3",
      "B": "S3, ADLS, GCS, SFTP",
      "C": "Apenas APIs externas",
      "D": "Apenas bancos de dados relacionais"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 865,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader não funciona com Delta",
      "B": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "C": "Não há diferença",
      "D": "COPY INTO é mais rápido"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 866,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Gerenciar permissões de usuários",
      "B": "Fazer backup de notebooks",
      "C": "Apenas transferir dados entre clusters",
      "D": "Conectar IDEs locais ao Databricks para desenvolvimento remoto"
    },
    "correctAnswer": "D",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 867,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Schema Evolution automática e detecção de novos arquivos",
      "B": "Eliminação de duplicatas",
      "C": "Apenas compressão de dados",
      "D": "Criptografia automática"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 868,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Converte tudo para string",
      "B": "Descarta automaticamente",
      "C": "Falha e interrompe a pipeline",
      "D": "Usa 'rescuedDataColumn' para capturar dados problemáticos"
    },
    "correctAnswer": "D",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 869,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas bancos de dados relacionais",
      "B": "Apenas S3",
      "C": "Apenas APIs externas",
      "D": "S3, ADLS, GCS, SFTP"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 870,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader não funciona com Delta",
      "B": "Não há diferença",
      "C": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "D": "COPY INTO é mais rápido"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 871,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "B": "Gerenciar permissões de usuários",
      "C": "Fazer backup de notebooks",
      "D": "Apenas transferir dados entre clusters"
    },
    "correctAnswer": "A",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 872,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Eliminação de duplicatas",
      "B": "Criptografia automática",
      "C": "Apenas compressão de dados",
      "D": "Schema Evolution automática e detecção de novos arquivos"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 873,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Converte tudo para string",
      "B": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "C": "Falha e interrompe a pipeline",
      "D": "Descarta automaticamente"
    },
    "correctAnswer": "B",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 874,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "S3, ADLS, GCS, SFTP",
      "B": "Apenas S3",
      "C": "Apenas bancos de dados relacionais",
      "D": "Apenas APIs externas"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 875,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "COPY INTO é mais rápido",
      "B": "Auto Loader não funciona com Delta",
      "C": "Não há diferença",
      "D": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 876,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Gerenciar permissões de usuários",
      "B": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "C": "Fazer backup de notebooks",
      "D": "Apenas transferir dados entre clusters"
    },
    "correctAnswer": "B",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 877,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Apenas compressão de dados",
      "B": "Criptografia automática",
      "C": "Schema Evolution automática e detecção de novos arquivos",
      "D": "Eliminação de duplicatas"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 878,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Converte tudo para string",
      "B": "Descarta automaticamente",
      "C": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "D": "Falha e interrompe a pipeline"
    },
    "correctAnswer": "C",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 879,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas bancos de dados relacionais",
      "B": "S3, ADLS, GCS, SFTP",
      "C": "Apenas S3",
      "D": "Apenas APIs externas"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 880,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "COPY INTO é mais rápido",
      "B": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "C": "Não há diferença",
      "D": "Auto Loader não funciona com Delta"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 881,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Gerenciar permissões de usuários",
      "B": "Fazer backup de notebooks",
      "C": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "D": "Apenas transferir dados entre clusters"
    },
    "correctAnswer": "C",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 882,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Criptografia automática",
      "B": "Apenas compressão de dados",
      "C": "Schema Evolution automática e detecção de novos arquivos",
      "D": "Eliminação de duplicatas"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 883,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Falha e interrompe a pipeline",
      "B": "Descarta automaticamente",
      "C": "Converte tudo para string",
      "D": "Usa 'rescuedDataColumn' para capturar dados problemáticos"
    },
    "correctAnswer": "D",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 884,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas S3",
      "B": "Apenas APIs externas",
      "C": "Apenas bancos de dados relacionais",
      "D": "S3, ADLS, GCS, SFTP"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 885,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "COPY INTO é mais rápido",
      "B": "Auto Loader não funciona com Delta",
      "C": "Não há diferença",
      "D": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 886,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Gerenciar permissões de usuários",
      "B": "Fazer backup de notebooks",
      "C": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "D": "Apenas transferir dados entre clusters"
    },
    "correctAnswer": "C",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 887,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Apenas compressão de dados",
      "B": "Criptografia automática",
      "C": "Schema Evolution automática e detecção de novos arquivos",
      "D": "Eliminação de duplicatas"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 888,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Descarta automaticamente",
      "B": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "C": "Converte tudo para string",
      "D": "Falha e interrompe a pipeline"
    },
    "correctAnswer": "B",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 889,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas S3",
      "B": "S3, ADLS, GCS, SFTP",
      "C": "Apenas APIs externas",
      "D": "Apenas bancos de dados relacionais"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 890,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "COPY INTO é mais rápido",
      "B": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "C": "Auto Loader não funciona com Delta",
      "D": "Não há diferença"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 891,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "B": "Gerenciar permissões de usuários",
      "C": "Apenas transferir dados entre clusters",
      "D": "Fazer backup de notebooks"
    },
    "correctAnswer": "A",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 892,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Apenas compressão de dados",
      "B": "Schema Evolution automática e detecção de novos arquivos",
      "C": "Criptografia automática",
      "D": "Eliminação de duplicatas"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 893,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Falha e interrompe a pipeline",
      "B": "Converte tudo para string",
      "C": "Descarta automaticamente",
      "D": "Usa 'rescuedDataColumn' para capturar dados problemáticos"
    },
    "correctAnswer": "D",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 894,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas APIs externas",
      "B": "S3, ADLS, GCS, SFTP",
      "C": "Apenas S3",
      "D": "Apenas bancos de dados relacionais"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 895,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader não funciona com Delta",
      "B": "COPY INTO é mais rápido",
      "C": "Não há diferença",
      "D": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 896,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "B": "Gerenciar permissões de usuários",
      "C": "Fazer backup de notebooks",
      "D": "Apenas transferir dados entre clusters"
    },
    "correctAnswer": "A",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 897,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Apenas compressão de dados",
      "B": "Criptografia automática",
      "C": "Eliminação de duplicatas",
      "D": "Schema Evolution automática e detecção de novos arquivos"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 898,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "B": "Descarta automaticamente",
      "C": "Converte tudo para string",
      "D": "Falha e interrompe a pipeline"
    },
    "correctAnswer": "A",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 899,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "S3, ADLS, GCS, SFTP",
      "B": "Apenas S3",
      "C": "Apenas bancos de dados relacionais",
      "D": "Apenas APIs externas"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 900,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Não há diferença",
      "B": "COPY INTO é mais rápido",
      "C": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "D": "Auto Loader não funciona com Delta"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 901,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Fazer backup de notebooks",
      "B": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "C": "Gerenciar permissões de usuários",
      "D": "Apenas transferir dados entre clusters"
    },
    "correctAnswer": "B",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 902,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Apenas compressão de dados",
      "B": "Eliminação de duplicatas",
      "C": "Criptografia automática",
      "D": "Schema Evolution automática e detecção de novos arquivos"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 903,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Descarta automaticamente",
      "B": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "C": "Falha e interrompe a pipeline",
      "D": "Converte tudo para string"
    },
    "correctAnswer": "B",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 904,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas S3",
      "B": "Apenas bancos de dados relacionais",
      "C": "Apenas APIs externas",
      "D": "S3, ADLS, GCS, SFTP"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 905,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "COPY INTO é mais rápido",
      "B": "Auto Loader não funciona com Delta",
      "C": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "D": "Não há diferença"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 906,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Gerenciar permissões de usuários",
      "B": "Fazer backup de notebooks",
      "C": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "D": "Apenas transferir dados entre clusters"
    },
    "correctAnswer": "C",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 907,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Schema Evolution automática e detecção de novos arquivos",
      "B": "Apenas compressão de dados",
      "C": "Criptografia automática",
      "D": "Eliminação de duplicatas"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 908,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Descarta automaticamente",
      "B": "Converte tudo para string",
      "C": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "D": "Falha e interrompe a pipeline"
    },
    "correctAnswer": "C",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 909,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "S3, ADLS, GCS, SFTP",
      "B": "Apenas bancos de dados relacionais",
      "C": "Apenas APIs externas",
      "D": "Apenas S3"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 910,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "COPY INTO é mais rápido",
      "B": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "C": "Não há diferença",
      "D": "Auto Loader não funciona com Delta"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 911,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Fazer backup de notebooks",
      "B": "Gerenciar permissões de usuários",
      "C": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "D": "Apenas transferir dados entre clusters"
    },
    "correctAnswer": "C",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 912,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Schema Evolution automática e detecção de novos arquivos",
      "B": "Apenas compressão de dados",
      "C": "Eliminação de duplicatas",
      "D": "Criptografia automática"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 913,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Falha e interrompe a pipeline",
      "B": "Converte tudo para string",
      "C": "Descarta automaticamente",
      "D": "Usa 'rescuedDataColumn' para capturar dados problemáticos"
    },
    "correctAnswer": "D",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 914,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas S3",
      "B": "Apenas bancos de dados relacionais",
      "C": "Apenas APIs externas",
      "D": "S3, ADLS, GCS, SFTP"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 915,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader não funciona com Delta",
      "B": "Não há diferença",
      "C": "COPY INTO é mais rápido",
      "D": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 916,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Apenas transferir dados entre clusters",
      "B": "Gerenciar permissões de usuários",
      "C": "Fazer backup de notebooks",
      "D": "Conectar IDEs locais ao Databricks para desenvolvimento remoto"
    },
    "correctAnswer": "D",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 917,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Apenas compressão de dados",
      "B": "Criptografia automática",
      "C": "Schema Evolution automática e detecção de novos arquivos",
      "D": "Eliminação de duplicatas"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 918,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Converte tudo para string",
      "B": "Descarta automaticamente",
      "C": "Falha e interrompe a pipeline",
      "D": "Usa 'rescuedDataColumn' para capturar dados problemáticos"
    },
    "correctAnswer": "D",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 919,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas S3",
      "B": "S3, ADLS, GCS, SFTP",
      "C": "Apenas bancos de dados relacionais",
      "D": "Apenas APIs externas"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 920,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Não há diferença",
      "B": "COPY INTO é mais rápido",
      "C": "Auto Loader não funciona com Delta",
      "D": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 921,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "B": "Gerenciar permissões de usuários",
      "C": "Fazer backup de notebooks",
      "D": "Apenas transferir dados entre clusters"
    },
    "correctAnswer": "A",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 922,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Criptografia automática",
      "B": "Schema Evolution automática e detecção de novos arquivos",
      "C": "Apenas compressão de dados",
      "D": "Eliminação de duplicatas"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 923,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Descarta automaticamente",
      "B": "Converte tudo para string",
      "C": "Falha e interrompe a pipeline",
      "D": "Usa 'rescuedDataColumn' para capturar dados problemáticos"
    },
    "correctAnswer": "D",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 924,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas bancos de dados relacionais",
      "B": "S3, ADLS, GCS, SFTP",
      "C": "Apenas S3",
      "D": "Apenas APIs externas"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 925,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader não funciona com Delta",
      "B": "Não há diferença",
      "C": "COPY INTO é mais rápido",
      "D": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 926,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "B": "Apenas transferir dados entre clusters",
      "C": "Gerenciar permissões de usuários",
      "D": "Fazer backup de notebooks"
    },
    "correctAnswer": "A",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 927,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Eliminação de duplicatas",
      "B": "Schema Evolution automática e detecção de novos arquivos",
      "C": "Apenas compressão de dados",
      "D": "Criptografia automática"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 928,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "B": "Descarta automaticamente",
      "C": "Converte tudo para string",
      "D": "Falha e interrompe a pipeline"
    },
    "correctAnswer": "A",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 929,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas APIs externas",
      "B": "Apenas bancos de dados relacionais",
      "C": "Apenas S3",
      "D": "S3, ADLS, GCS, SFTP"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 930,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "B": "Auto Loader não funciona com Delta",
      "C": "Não há diferença",
      "D": "COPY INTO é mais rápido"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 931,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "B": "Apenas transferir dados entre clusters",
      "C": "Fazer backup de notebooks",
      "D": "Gerenciar permissões de usuários"
    },
    "correctAnswer": "A",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 932,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Criptografia automática",
      "B": "Eliminação de duplicatas",
      "C": "Schema Evolution automática e detecção de novos arquivos",
      "D": "Apenas compressão de dados"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 933,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Descarta automaticamente",
      "B": "Converte tudo para string",
      "C": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "D": "Falha e interrompe a pipeline"
    },
    "correctAnswer": "C",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 934,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "S3, ADLS, GCS, SFTP",
      "B": "Apenas bancos de dados relacionais",
      "C": "Apenas APIs externas",
      "D": "Apenas S3"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 935,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Não há diferença",
      "B": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "C": "Auto Loader não funciona com Delta",
      "D": "COPY INTO é mais rápido"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 936,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Apenas transferir dados entre clusters",
      "B": "Fazer backup de notebooks",
      "C": "Gerenciar permissões de usuários",
      "D": "Conectar IDEs locais ao Databricks para desenvolvimento remoto"
    },
    "correctAnswer": "D",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 937,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Eliminação de duplicatas",
      "B": "Apenas compressão de dados",
      "C": "Criptografia automática",
      "D": "Schema Evolution automática e detecção de novos arquivos"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 938,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Falha e interrompe a pipeline",
      "B": "Descarta automaticamente",
      "C": "Converte tudo para string",
      "D": "Usa 'rescuedDataColumn' para capturar dados problemáticos"
    },
    "correctAnswer": "D",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 939,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas S3",
      "B": "S3, ADLS, GCS, SFTP",
      "C": "Apenas bancos de dados relacionais",
      "D": "Apenas APIs externas"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 940,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "B": "Não há diferença",
      "C": "Auto Loader não funciona com Delta",
      "D": "COPY INTO é mais rápido"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 941,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Fazer backup de notebooks",
      "B": "Apenas transferir dados entre clusters",
      "C": "Gerenciar permissões de usuários",
      "D": "Conectar IDEs locais ao Databricks para desenvolvimento remoto"
    },
    "correctAnswer": "D",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 942,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Eliminação de duplicatas",
      "B": "Apenas compressão de dados",
      "C": "Schema Evolution automática e detecção de novos arquivos",
      "D": "Criptografia automática"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 943,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Falha e interrompe a pipeline",
      "B": "Descarta automaticamente",
      "C": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "D": "Converte tudo para string"
    },
    "correctAnswer": "C",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 944,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas S3",
      "B": "Apenas bancos de dados relacionais",
      "C": "Apenas APIs externas",
      "D": "S3, ADLS, GCS, SFTP"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 945,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "COPY INTO é mais rápido",
      "B": "Auto Loader não funciona com Delta",
      "C": "Não há diferença",
      "D": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 946,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "B": "Fazer backup de notebooks",
      "C": "Apenas transferir dados entre clusters",
      "D": "Gerenciar permissões de usuários"
    },
    "correctAnswer": "A",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 947,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Criptografia automática",
      "B": "Eliminação de duplicatas",
      "C": "Schema Evolution automática e detecção de novos arquivos",
      "D": "Apenas compressão de dados"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 948,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Descarta automaticamente",
      "B": "Converte tudo para string",
      "C": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "D": "Falha e interrompe a pipeline"
    },
    "correctAnswer": "C",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 949,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas S3",
      "B": "Apenas APIs externas",
      "C": "S3, ADLS, GCS, SFTP",
      "D": "Apenas bancos de dados relacionais"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 950,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Não há diferença",
      "B": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "C": "COPY INTO é mais rápido",
      "D": "Auto Loader não funciona com Delta"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 951,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "B": "Gerenciar permissões de usuários",
      "C": "Fazer backup de notebooks",
      "D": "Apenas transferir dados entre clusters"
    },
    "correctAnswer": "A",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 952,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Schema Evolution automática e detecção de novos arquivos",
      "B": "Apenas compressão de dados",
      "C": "Eliminação de duplicatas",
      "D": "Criptografia automática"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 953,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "B": "Descarta automaticamente",
      "C": "Falha e interrompe a pipeline",
      "D": "Converte tudo para string"
    },
    "correctAnswer": "A",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 954,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas bancos de dados relacionais",
      "B": "S3, ADLS, GCS, SFTP",
      "C": "Apenas S3",
      "D": "Apenas APIs externas"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 955,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "COPY INTO é mais rápido",
      "B": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "C": "Não há diferença",
      "D": "Auto Loader não funciona com Delta"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 956,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Apenas transferir dados entre clusters",
      "B": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "C": "Fazer backup de notebooks",
      "D": "Gerenciar permissões de usuários"
    },
    "correctAnswer": "B",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 957,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Eliminação de duplicatas",
      "B": "Criptografia automática",
      "C": "Apenas compressão de dados",
      "D": "Schema Evolution automática e detecção de novos arquivos"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 958,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "B": "Converte tudo para string",
      "C": "Descarta automaticamente",
      "D": "Falha e interrompe a pipeline"
    },
    "correctAnswer": "A",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 959,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas APIs externas",
      "B": "Apenas bancos de dados relacionais",
      "C": "Apenas S3",
      "D": "S3, ADLS, GCS, SFTP"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 960,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader não funciona com Delta",
      "B": "COPY INTO é mais rápido",
      "C": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "D": "Não há diferença"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 961,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Fazer backup de notebooks",
      "B": "Apenas transferir dados entre clusters",
      "C": "Gerenciar permissões de usuários",
      "D": "Conectar IDEs locais ao Databricks para desenvolvimento remoto"
    },
    "correctAnswer": "D",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 962,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Apenas compressão de dados",
      "B": "Schema Evolution automática e detecção de novos arquivos",
      "C": "Eliminação de duplicatas",
      "D": "Criptografia automática"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 963,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Converte tudo para string",
      "B": "Descarta automaticamente",
      "C": "Falha e interrompe a pipeline",
      "D": "Usa 'rescuedDataColumn' para capturar dados problemáticos"
    },
    "correctAnswer": "D",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 964,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas S3",
      "B": "Apenas bancos de dados relacionais",
      "C": "Apenas APIs externas",
      "D": "S3, ADLS, GCS, SFTP"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 965,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "B": "COPY INTO é mais rápido",
      "C": "Não há diferença",
      "D": "Auto Loader não funciona com Delta"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 966,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "B": "Apenas transferir dados entre clusters",
      "C": "Gerenciar permissões de usuários",
      "D": "Fazer backup de notebooks"
    },
    "correctAnswer": "A",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 967,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Criptografia automática",
      "B": "Eliminação de duplicatas",
      "C": "Schema Evolution automática e detecção de novos arquivos",
      "D": "Apenas compressão de dados"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 968,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Falha e interrompe a pipeline",
      "B": "Descarta automaticamente",
      "C": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "D": "Converte tudo para string"
    },
    "correctAnswer": "C",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 969,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas APIs externas",
      "B": "Apenas S3",
      "C": "S3, ADLS, GCS, SFTP",
      "D": "Apenas bancos de dados relacionais"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 970,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "B": "Auto Loader não funciona com Delta",
      "C": "COPY INTO é mais rápido",
      "D": "Não há diferença"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 971,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Gerenciar permissões de usuários",
      "B": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "C": "Apenas transferir dados entre clusters",
      "D": "Fazer backup de notebooks"
    },
    "correctAnswer": "B",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 972,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Criptografia automática",
      "B": "Schema Evolution automática e detecção de novos arquivos",
      "C": "Apenas compressão de dados",
      "D": "Eliminação de duplicatas"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 973,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "B": "Descarta automaticamente",
      "C": "Falha e interrompe a pipeline",
      "D": "Converte tudo para string"
    },
    "correctAnswer": "A",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 974,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas bancos de dados relacionais",
      "B": "Apenas APIs externas",
      "C": "Apenas S3",
      "D": "S3, ADLS, GCS, SFTP"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 975,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Não há diferença",
      "B": "Auto Loader não funciona com Delta",
      "C": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "D": "COPY INTO é mais rápido"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 976,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Fazer backup de notebooks",
      "B": "Apenas transferir dados entre clusters",
      "C": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "D": "Gerenciar permissões de usuários"
    },
    "correctAnswer": "C",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 977,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Schema Evolution automática e detecção de novos arquivos",
      "B": "Criptografia automática",
      "C": "Apenas compressão de dados",
      "D": "Eliminação de duplicatas"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 978,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Converte tudo para string",
      "B": "Falha e interrompe a pipeline",
      "C": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "D": "Descarta automaticamente"
    },
    "correctAnswer": "C",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 979,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas APIs externas",
      "B": "Apenas bancos de dados relacionais",
      "C": "S3, ADLS, GCS, SFTP",
      "D": "Apenas S3"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 980,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "COPY INTO é mais rápido",
      "B": "Não há diferença",
      "C": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "D": "Auto Loader não funciona com Delta"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 981,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Fazer backup de notebooks",
      "B": "Gerenciar permissões de usuários",
      "C": "Apenas transferir dados entre clusters",
      "D": "Conectar IDEs locais ao Databricks para desenvolvimento remoto"
    },
    "correctAnswer": "D",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 982,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Schema Evolution automática e detecção de novos arquivos",
      "B": "Apenas compressão de dados",
      "C": "Eliminação de duplicatas",
      "D": "Criptografia automática"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 983,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Falha e interrompe a pipeline",
      "B": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "C": "Converte tudo para string",
      "D": "Descarta automaticamente"
    },
    "correctAnswer": "B",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 984,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas S3",
      "B": "Apenas bancos de dados relacionais",
      "C": "Apenas APIs externas",
      "D": "S3, ADLS, GCS, SFTP"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 985,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "COPY INTO é mais rápido",
      "B": "Auto Loader não funciona com Delta",
      "C": "Não há diferença",
      "D": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 986,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "B": "Gerenciar permissões de usuários",
      "C": "Apenas transferir dados entre clusters",
      "D": "Fazer backup de notebooks"
    },
    "correctAnswer": "A",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 987,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Criptografia automática",
      "B": "Eliminação de duplicatas",
      "C": "Apenas compressão de dados",
      "D": "Schema Evolution automática e detecção de novos arquivos"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 988,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Falha e interrompe a pipeline",
      "B": "Converte tudo para string",
      "C": "Descarta automaticamente",
      "D": "Usa 'rescuedDataColumn' para capturar dados problemáticos"
    },
    "correctAnswer": "D",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 989,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "S3, ADLS, GCS, SFTP",
      "B": "Apenas S3",
      "C": "Apenas bancos de dados relacionais",
      "D": "Apenas APIs externas"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 990,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Não há diferença",
      "B": "COPY INTO é mais rápido",
      "C": "Auto Loader não funciona com Delta",
      "D": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 991,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Fazer backup de notebooks",
      "B": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "C": "Apenas transferir dados entre clusters",
      "D": "Gerenciar permissões de usuários"
    },
    "correctAnswer": "B",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 992,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Eliminação de duplicatas",
      "B": "Apenas compressão de dados",
      "C": "Schema Evolution automática e detecção de novos arquivos",
      "D": "Criptografia automática"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 993,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "B": "Falha e interrompe a pipeline",
      "C": "Converte tudo para string",
      "D": "Descarta automaticamente"
    },
    "correctAnswer": "A",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 994,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas S3",
      "B": "S3, ADLS, GCS, SFTP",
      "C": "Apenas APIs externas",
      "D": "Apenas bancos de dados relacionais"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 995,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "COPY INTO é mais rápido",
      "B": "Auto Loader não funciona com Delta",
      "C": "Não há diferença",
      "D": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 996,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "B": "Gerenciar permissões de usuários",
      "C": "Apenas transferir dados entre clusters",
      "D": "Fazer backup de notebooks"
    },
    "correctAnswer": "A",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 997,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Eliminação de duplicatas",
      "B": "Apenas compressão de dados",
      "C": "Schema Evolution automática e detecção de novos arquivos",
      "D": "Criptografia automática"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 998,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Falha e interrompe a pipeline",
      "B": "Converte tudo para string",
      "C": "Descarta automaticamente",
      "D": "Usa 'rescuedDataColumn' para capturar dados problemáticos"
    },
    "correctAnswer": "D",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 999,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas S3",
      "B": "S3, ADLS, GCS, SFTP",
      "C": "Apenas bancos de dados relacionais",
      "D": "Apenas APIs externas"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 1000,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader não funciona com Delta",
      "B": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "C": "COPY INTO é mais rápido",
      "D": "Não há diferença"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 1001,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Gerenciar permissões de usuários",
      "B": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "C": "Apenas transferir dados entre clusters",
      "D": "Fazer backup de notebooks"
    },
    "correctAnswer": "B",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 1002,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Apenas compressão de dados",
      "B": "Eliminação de duplicatas",
      "C": "Criptografia automática",
      "D": "Schema Evolution automática e detecção de novos arquivos"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 1003,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Falha e interrompe a pipeline",
      "B": "Descarta automaticamente",
      "C": "Converte tudo para string",
      "D": "Usa 'rescuedDataColumn' para capturar dados problemáticos"
    },
    "correctAnswer": "D",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 1004,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas bancos de dados relacionais",
      "B": "S3, ADLS, GCS, SFTP",
      "C": "Apenas APIs externas",
      "D": "Apenas S3"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 1005,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader não funciona com Delta",
      "B": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "C": "Não há diferença",
      "D": "COPY INTO é mais rápido"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 1006,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "B": "Apenas transferir dados entre clusters",
      "C": "Gerenciar permissões de usuários",
      "D": "Fazer backup de notebooks"
    },
    "correctAnswer": "A",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 1007,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Eliminação de duplicatas",
      "B": "Schema Evolution automática e detecção de novos arquivos",
      "C": "Criptografia automática",
      "D": "Apenas compressão de dados"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 1008,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Falha e interrompe a pipeline",
      "B": "Descarta automaticamente",
      "C": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "D": "Converte tudo para string"
    },
    "correctAnswer": "C",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 1009,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas S3",
      "B": "Apenas bancos de dados relacionais",
      "C": "S3, ADLS, GCS, SFTP",
      "D": "Apenas APIs externas"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 1010,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Não há diferença",
      "B": "Auto Loader não funciona com Delta",
      "C": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "D": "COPY INTO é mais rápido"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 1011,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Fazer backup de notebooks",
      "B": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "C": "Gerenciar permissões de usuários",
      "D": "Apenas transferir dados entre clusters"
    },
    "correctAnswer": "B",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 1012,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Criptografia automática",
      "B": "Eliminação de duplicatas",
      "C": "Apenas compressão de dados",
      "D": "Schema Evolution automática e detecção de novos arquivos"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 1013,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "B": "Falha e interrompe a pipeline",
      "C": "Converte tudo para string",
      "D": "Descarta automaticamente"
    },
    "correctAnswer": "A",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 1014,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas bancos de dados relacionais",
      "B": "S3, ADLS, GCS, SFTP",
      "C": "Apenas APIs externas",
      "D": "Apenas S3"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 1015,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader não funciona com Delta",
      "B": "Não há diferença",
      "C": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "D": "COPY INTO é mais rápido"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 1016,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Gerenciar permissões de usuários",
      "B": "Apenas transferir dados entre clusters",
      "C": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "D": "Fazer backup de notebooks"
    },
    "correctAnswer": "C",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 1017,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Eliminação de duplicatas",
      "B": "Criptografia automática",
      "C": "Apenas compressão de dados",
      "D": "Schema Evolution automática e detecção de novos arquivos"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 1018,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Converte tudo para string",
      "B": "Falha e interrompe a pipeline",
      "C": "Descarta automaticamente",
      "D": "Usa 'rescuedDataColumn' para capturar dados problemáticos"
    },
    "correctAnswer": "D",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 1019,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas S3",
      "B": "Apenas APIs externas",
      "C": "Apenas bancos de dados relacionais",
      "D": "S3, ADLS, GCS, SFTP"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 1020,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "B": "Não há diferença",
      "C": "Auto Loader não funciona com Delta",
      "D": "COPY INTO é mais rápido"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 1021,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Fazer backup de notebooks",
      "B": "Gerenciar permissões de usuários",
      "C": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "D": "Apenas transferir dados entre clusters"
    },
    "correctAnswer": "C",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 1022,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Eliminação de duplicatas",
      "B": "Schema Evolution automática e detecção de novos arquivos",
      "C": "Criptografia automática",
      "D": "Apenas compressão de dados"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 1023,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "B": "Descarta automaticamente",
      "C": "Falha e interrompe a pipeline",
      "D": "Converte tudo para string"
    },
    "correctAnswer": "A",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 1024,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas S3",
      "B": "Apenas APIs externas",
      "C": "Apenas bancos de dados relacionais",
      "D": "S3, ADLS, GCS, SFTP"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 1025,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "COPY INTO é mais rápido",
      "B": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "C": "Auto Loader não funciona com Delta",
      "D": "Não há diferença"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 1026,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Gerenciar permissões de usuários",
      "B": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "C": "Fazer backup de notebooks",
      "D": "Apenas transferir dados entre clusters"
    },
    "correctAnswer": "B",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 1027,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Eliminação de duplicatas",
      "B": "Schema Evolution automática e detecção de novos arquivos",
      "C": "Criptografia automática",
      "D": "Apenas compressão de dados"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 1028,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "B": "Falha e interrompe a pipeline",
      "C": "Converte tudo para string",
      "D": "Descarta automaticamente"
    },
    "correctAnswer": "A",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 1029,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas APIs externas",
      "B": "S3, ADLS, GCS, SFTP",
      "C": "Apenas bancos de dados relacionais",
      "D": "Apenas S3"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 1030,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader não funciona com Delta",
      "B": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "C": "Não há diferença",
      "D": "COPY INTO é mais rápido"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 1031,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Fazer backup de notebooks",
      "B": "Gerenciar permissões de usuários",
      "C": "Apenas transferir dados entre clusters",
      "D": "Conectar IDEs locais ao Databricks para desenvolvimento remoto"
    },
    "correctAnswer": "D",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 1032,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Apenas compressão de dados",
      "B": "Schema Evolution automática e detecção de novos arquivos",
      "C": "Eliminação de duplicatas",
      "D": "Criptografia automática"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 1033,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Descarta automaticamente",
      "B": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "C": "Falha e interrompe a pipeline",
      "D": "Converte tudo para string"
    },
    "correctAnswer": "B",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 1034,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "S3, ADLS, GCS, SFTP",
      "B": "Apenas APIs externas",
      "C": "Apenas bancos de dados relacionais",
      "D": "Apenas S3"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 1035,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "COPY INTO é mais rápido",
      "B": "Não há diferença",
      "C": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "D": "Auto Loader não funciona com Delta"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 1036,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Gerenciar permissões de usuários",
      "B": "Fazer backup de notebooks",
      "C": "Apenas transferir dados entre clusters",
      "D": "Conectar IDEs locais ao Databricks para desenvolvimento remoto"
    },
    "correctAnswer": "D",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 1037,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Schema Evolution automática e detecção de novos arquivos",
      "B": "Apenas compressão de dados",
      "C": "Criptografia automática",
      "D": "Eliminação de duplicatas"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 1038,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Falha e interrompe a pipeline",
      "B": "Converte tudo para string",
      "C": "Descarta automaticamente",
      "D": "Usa 'rescuedDataColumn' para capturar dados problemáticos"
    },
    "correctAnswer": "D",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 1039,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas S3",
      "B": "Apenas bancos de dados relacionais",
      "C": "S3, ADLS, GCS, SFTP",
      "D": "Apenas APIs externas"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 1040,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "COPY INTO é mais rápido",
      "B": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "C": "Não há diferença",
      "D": "Auto Loader não funciona com Delta"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 1041,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Fazer backup de notebooks",
      "B": "Gerenciar permissões de usuários",
      "C": "Apenas transferir dados entre clusters",
      "D": "Conectar IDEs locais ao Databricks para desenvolvimento remoto"
    },
    "correctAnswer": "D",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 1042,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Eliminação de duplicatas",
      "B": "Criptografia automática",
      "C": "Schema Evolution automática e detecção de novos arquivos",
      "D": "Apenas compressão de dados"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 1043,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Descarta automaticamente",
      "B": "Falha e interrompe a pipeline",
      "C": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "D": "Converte tudo para string"
    },
    "correctAnswer": "C",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 1044,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas bancos de dados relacionais",
      "B": "S3, ADLS, GCS, SFTP",
      "C": "Apenas S3",
      "D": "Apenas APIs externas"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 1045,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Não há diferença",
      "B": "Auto Loader não funciona com Delta",
      "C": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "D": "COPY INTO é mais rápido"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 1046,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "B": "Gerenciar permissões de usuários",
      "C": "Fazer backup de notebooks",
      "D": "Apenas transferir dados entre clusters"
    },
    "correctAnswer": "A",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 1047,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Schema Evolution automática e detecção de novos arquivos",
      "B": "Criptografia automática",
      "C": "Eliminação de duplicatas",
      "D": "Apenas compressão de dados"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 1048,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Converte tudo para string",
      "B": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "C": "Falha e interrompe a pipeline",
      "D": "Descarta automaticamente"
    },
    "correctAnswer": "B",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 1049,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas S3",
      "B": "Apenas APIs externas",
      "C": "S3, ADLS, GCS, SFTP",
      "D": "Apenas bancos de dados relacionais"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 1050,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "COPY INTO é mais rápido",
      "B": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "C": "Não há diferença",
      "D": "Auto Loader não funciona com Delta"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 1051,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Fazer backup de notebooks",
      "B": "Apenas transferir dados entre clusters",
      "C": "Gerenciar permissões de usuários",
      "D": "Conectar IDEs locais ao Databricks para desenvolvimento remoto"
    },
    "correctAnswer": "D",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 1052,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Eliminação de duplicatas",
      "B": "Apenas compressão de dados",
      "C": "Schema Evolution automática e detecção de novos arquivos",
      "D": "Criptografia automática"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 1053,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Descarta automaticamente",
      "B": "Falha e interrompe a pipeline",
      "C": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "D": "Converte tudo para string"
    },
    "correctAnswer": "C",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 1054,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas bancos de dados relacionais",
      "B": "Apenas S3",
      "C": "Apenas APIs externas",
      "D": "S3, ADLS, GCS, SFTP"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 1055,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "COPY INTO é mais rápido",
      "B": "Auto Loader não funciona com Delta",
      "C": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "D": "Não há diferença"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 1056,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Apenas transferir dados entre clusters",
      "B": "Gerenciar permissões de usuários",
      "C": "Fazer backup de notebooks",
      "D": "Conectar IDEs locais ao Databricks para desenvolvimento remoto"
    },
    "correctAnswer": "D",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 1057,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Apenas compressão de dados",
      "B": "Eliminação de duplicatas",
      "C": "Schema Evolution automática e detecção de novos arquivos",
      "D": "Criptografia automática"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 1058,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Descarta automaticamente",
      "B": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "C": "Converte tudo para string",
      "D": "Falha e interrompe a pipeline"
    },
    "correctAnswer": "B",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 1059,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas APIs externas",
      "B": "Apenas S3",
      "C": "Apenas bancos de dados relacionais",
      "D": "S3, ADLS, GCS, SFTP"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 1060,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "B": "Não há diferença",
      "C": "Auto Loader não funciona com Delta",
      "D": "COPY INTO é mais rápido"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 1061,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "B": "Apenas transferir dados entre clusters",
      "C": "Fazer backup de notebooks",
      "D": "Gerenciar permissões de usuários"
    },
    "correctAnswer": "A",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 1062,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Eliminação de duplicatas",
      "B": "Apenas compressão de dados",
      "C": "Criptografia automática",
      "D": "Schema Evolution automática e detecção de novos arquivos"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 1063,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "B": "Converte tudo para string",
      "C": "Falha e interrompe a pipeline",
      "D": "Descarta automaticamente"
    },
    "correctAnswer": "A",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 1064,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas APIs externas",
      "B": "S3, ADLS, GCS, SFTP",
      "C": "Apenas bancos de dados relacionais",
      "D": "Apenas S3"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 1065,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "COPY INTO é mais rápido",
      "B": "Auto Loader não funciona com Delta",
      "C": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "D": "Não há diferença"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 1066,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Fazer backup de notebooks",
      "B": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "C": "Apenas transferir dados entre clusters",
      "D": "Gerenciar permissões de usuários"
    },
    "correctAnswer": "B",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 1067,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Apenas compressão de dados",
      "B": "Schema Evolution automática e detecção de novos arquivos",
      "C": "Eliminação de duplicatas",
      "D": "Criptografia automática"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 1068,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Descarta automaticamente",
      "B": "Converte tudo para string",
      "C": "Falha e interrompe a pipeline",
      "D": "Usa 'rescuedDataColumn' para capturar dados problemáticos"
    },
    "correctAnswer": "D",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 1069,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas bancos de dados relacionais",
      "B": "Apenas S3",
      "C": "S3, ADLS, GCS, SFTP",
      "D": "Apenas APIs externas"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 1070,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Não há diferença",
      "B": "Auto Loader não funciona com Delta",
      "C": "COPY INTO é mais rápido",
      "D": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 1071,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "B": "Apenas transferir dados entre clusters",
      "C": "Gerenciar permissões de usuários",
      "D": "Fazer backup de notebooks"
    },
    "correctAnswer": "A",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 1072,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Criptografia automática",
      "B": "Eliminação de duplicatas",
      "C": "Schema Evolution automática e detecção de novos arquivos",
      "D": "Apenas compressão de dados"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 1073,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Converte tudo para string",
      "B": "Falha e interrompe a pipeline",
      "C": "Descarta automaticamente",
      "D": "Usa 'rescuedDataColumn' para capturar dados problemáticos"
    },
    "correctAnswer": "D",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 1074,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas bancos de dados relacionais",
      "B": "Apenas APIs externas",
      "C": "S3, ADLS, GCS, SFTP",
      "D": "Apenas S3"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 1075,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Não há diferença",
      "B": "Auto Loader não funciona com Delta",
      "C": "COPY INTO é mais rápido",
      "D": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 1076,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Gerenciar permissões de usuários",
      "B": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "C": "Fazer backup de notebooks",
      "D": "Apenas transferir dados entre clusters"
    },
    "correctAnswer": "B",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 1077,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Eliminação de duplicatas",
      "B": "Criptografia automática",
      "C": "Apenas compressão de dados",
      "D": "Schema Evolution automática e detecção de novos arquivos"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 1078,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Converte tudo para string",
      "B": "Descarta automaticamente",
      "C": "Falha e interrompe a pipeline",
      "D": "Usa 'rescuedDataColumn' para capturar dados problemáticos"
    },
    "correctAnswer": "D",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 1079,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas S3",
      "B": "Apenas bancos de dados relacionais",
      "C": "Apenas APIs externas",
      "D": "S3, ADLS, GCS, SFTP"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 1080,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "B": "Auto Loader não funciona com Delta",
      "C": "Não há diferença",
      "D": "COPY INTO é mais rápido"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 1081,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Gerenciar permissões de usuários",
      "B": "Apenas transferir dados entre clusters",
      "C": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "D": "Fazer backup de notebooks"
    },
    "correctAnswer": "C",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 1082,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Schema Evolution automática e detecção de novos arquivos",
      "B": "Eliminação de duplicatas",
      "C": "Apenas compressão de dados",
      "D": "Criptografia automática"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 1083,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Converte tudo para string",
      "B": "Descarta automaticamente",
      "C": "Falha e interrompe a pipeline",
      "D": "Usa 'rescuedDataColumn' para capturar dados problemáticos"
    },
    "correctAnswer": "D",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 1084,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas bancos de dados relacionais",
      "B": "Apenas S3",
      "C": "S3, ADLS, GCS, SFTP",
      "D": "Apenas APIs externas"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 1085,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "B": "COPY INTO é mais rápido",
      "C": "Não há diferença",
      "D": "Auto Loader não funciona com Delta"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 1086,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Fazer backup de notebooks",
      "B": "Apenas transferir dados entre clusters",
      "C": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "D": "Gerenciar permissões de usuários"
    },
    "correctAnswer": "C",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 1087,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Criptografia automática",
      "B": "Eliminação de duplicatas",
      "C": "Apenas compressão de dados",
      "D": "Schema Evolution automática e detecção de novos arquivos"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 1088,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Descarta automaticamente",
      "B": "Converte tudo para string",
      "C": "Falha e interrompe a pipeline",
      "D": "Usa 'rescuedDataColumn' para capturar dados problemáticos"
    },
    "correctAnswer": "D",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 1089,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "S3, ADLS, GCS, SFTP",
      "B": "Apenas S3",
      "C": "Apenas APIs externas",
      "D": "Apenas bancos de dados relacionais"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 1090,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Não há diferença",
      "B": "Auto Loader não funciona com Delta",
      "C": "COPY INTO é mais rápido",
      "D": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 1091,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "B": "Gerenciar permissões de usuários",
      "C": "Apenas transferir dados entre clusters",
      "D": "Fazer backup de notebooks"
    },
    "correctAnswer": "A",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 1092,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Apenas compressão de dados",
      "B": "Criptografia automática",
      "C": "Eliminação de duplicatas",
      "D": "Schema Evolution automática e detecção de novos arquivos"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 1093,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Falha e interrompe a pipeline",
      "B": "Converte tudo para string",
      "C": "Descarta automaticamente",
      "D": "Usa 'rescuedDataColumn' para capturar dados problemáticos"
    },
    "correctAnswer": "D",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 1094,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas bancos de dados relacionais",
      "B": "Apenas APIs externas",
      "C": "Apenas S3",
      "D": "S3, ADLS, GCS, SFTP"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 1095,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "B": "Auto Loader não funciona com Delta",
      "C": "COPY INTO é mais rápido",
      "D": "Não há diferença"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 1096,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Apenas transferir dados entre clusters",
      "B": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "C": "Fazer backup de notebooks",
      "D": "Gerenciar permissões de usuários"
    },
    "correctAnswer": "B",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 1097,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Schema Evolution automática e detecção de novos arquivos",
      "B": "Eliminação de duplicatas",
      "C": "Apenas compressão de dados",
      "D": "Criptografia automática"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 1098,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Descarta automaticamente",
      "B": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "C": "Converte tudo para string",
      "D": "Falha e interrompe a pipeline"
    },
    "correctAnswer": "B",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 1099,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas S3",
      "B": "Apenas bancos de dados relacionais",
      "C": "S3, ADLS, GCS, SFTP",
      "D": "Apenas APIs externas"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 1100,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "COPY INTO é mais rápido",
      "B": "Não há diferença",
      "C": "Auto Loader não funciona com Delta",
      "D": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 1101,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Apenas transferir dados entre clusters",
      "B": "Fazer backup de notebooks",
      "C": "Gerenciar permissões de usuários",
      "D": "Conectar IDEs locais ao Databricks para desenvolvimento remoto"
    },
    "correctAnswer": "D",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 1102,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Criptografia automática",
      "B": "Apenas compressão de dados",
      "C": "Eliminação de duplicatas",
      "D": "Schema Evolution automática e detecção de novos arquivos"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 1103,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Falha e interrompe a pipeline",
      "B": "Descarta automaticamente",
      "C": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "D": "Converte tudo para string"
    },
    "correctAnswer": "C",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 1104,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "S3, ADLS, GCS, SFTP",
      "B": "Apenas APIs externas",
      "C": "Apenas S3",
      "D": "Apenas bancos de dados relacionais"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 1105,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "COPY INTO é mais rápido",
      "B": "Não há diferença",
      "C": "Auto Loader não funciona com Delta",
      "D": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 1106,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "B": "Fazer backup de notebooks",
      "C": "Gerenciar permissões de usuários",
      "D": "Apenas transferir dados entre clusters"
    },
    "correctAnswer": "A",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 1107,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Eliminação de duplicatas",
      "B": "Apenas compressão de dados",
      "C": "Criptografia automática",
      "D": "Schema Evolution automática e detecção de novos arquivos"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 1108,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "B": "Converte tudo para string",
      "C": "Falha e interrompe a pipeline",
      "D": "Descarta automaticamente"
    },
    "correctAnswer": "A",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 1109,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas bancos de dados relacionais",
      "B": "Apenas APIs externas",
      "C": "S3, ADLS, GCS, SFTP",
      "D": "Apenas S3"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 1110,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader não funciona com Delta",
      "B": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "C": "Não há diferença",
      "D": "COPY INTO é mais rápido"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 1111,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Apenas transferir dados entre clusters",
      "B": "Gerenciar permissões de usuários",
      "C": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "D": "Fazer backup de notebooks"
    },
    "correctAnswer": "C",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 1112,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Eliminação de duplicatas",
      "B": "Apenas compressão de dados",
      "C": "Criptografia automática",
      "D": "Schema Evolution automática e detecção de novos arquivos"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 1113,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "B": "Descarta automaticamente",
      "C": "Falha e interrompe a pipeline",
      "D": "Converte tudo para string"
    },
    "correctAnswer": "A",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 1114,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas bancos de dados relacionais",
      "B": "S3, ADLS, GCS, SFTP",
      "C": "Apenas APIs externas",
      "D": "Apenas S3"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 1115,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Não há diferença",
      "B": "COPY INTO é mais rápido",
      "C": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "D": "Auto Loader não funciona com Delta"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 1116,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "B": "Fazer backup de notebooks",
      "C": "Gerenciar permissões de usuários",
      "D": "Apenas transferir dados entre clusters"
    },
    "correctAnswer": "A",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 1117,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Eliminação de duplicatas",
      "B": "Criptografia automática",
      "C": "Schema Evolution automática e detecção de novos arquivos",
      "D": "Apenas compressão de dados"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 1118,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Falha e interrompe a pipeline",
      "B": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "C": "Descarta automaticamente",
      "D": "Converte tudo para string"
    },
    "correctAnswer": "B",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 1119,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas APIs externas",
      "B": "Apenas S3",
      "C": "S3, ADLS, GCS, SFTP",
      "D": "Apenas bancos de dados relacionais"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 1120,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "COPY INTO é mais rápido",
      "B": "Não há diferença",
      "C": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "D": "Auto Loader não funciona com Delta"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 1121,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Apenas transferir dados entre clusters",
      "B": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "C": "Fazer backup de notebooks",
      "D": "Gerenciar permissões de usuários"
    },
    "correctAnswer": "B",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 1122,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Eliminação de duplicatas",
      "B": "Criptografia automática",
      "C": "Apenas compressão de dados",
      "D": "Schema Evolution automática e detecção de novos arquivos"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 1123,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Descarta automaticamente",
      "B": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "C": "Converte tudo para string",
      "D": "Falha e interrompe a pipeline"
    },
    "correctAnswer": "B",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 1124,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas APIs externas",
      "B": "Apenas bancos de dados relacionais",
      "C": "S3, ADLS, GCS, SFTP",
      "D": "Apenas S3"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 1125,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Não há diferença",
      "B": "COPY INTO é mais rápido",
      "C": "Auto Loader não funciona com Delta",
      "D": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 1126,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Fazer backup de notebooks",
      "B": "Apenas transferir dados entre clusters",
      "C": "Gerenciar permissões de usuários",
      "D": "Conectar IDEs locais ao Databricks para desenvolvimento remoto"
    },
    "correctAnswer": "D",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 1127,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Schema Evolution automática e detecção de novos arquivos",
      "B": "Eliminação de duplicatas",
      "C": "Criptografia automática",
      "D": "Apenas compressão de dados"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 1128,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Descarta automaticamente",
      "B": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "C": "Converte tudo para string",
      "D": "Falha e interrompe a pipeline"
    },
    "correctAnswer": "B",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 1129,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas bancos de dados relacionais",
      "B": "Apenas APIs externas",
      "C": "S3, ADLS, GCS, SFTP",
      "D": "Apenas S3"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 1130,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Não há diferença",
      "B": "COPY INTO é mais rápido",
      "C": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "D": "Auto Loader não funciona com Delta"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 1131,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Gerenciar permissões de usuários",
      "B": "Fazer backup de notebooks",
      "C": "Apenas transferir dados entre clusters",
      "D": "Conectar IDEs locais ao Databricks para desenvolvimento remoto"
    },
    "correctAnswer": "D",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 1132,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Schema Evolution automática e detecção de novos arquivos",
      "B": "Criptografia automática",
      "C": "Eliminação de duplicatas",
      "D": "Apenas compressão de dados"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 1133,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Descarta automaticamente",
      "B": "Falha e interrompe a pipeline",
      "C": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "D": "Converte tudo para string"
    },
    "correctAnswer": "C",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 1134,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas bancos de dados relacionais",
      "B": "Apenas APIs externas",
      "C": "S3, ADLS, GCS, SFTP",
      "D": "Apenas S3"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 1135,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "B": "COPY INTO é mais rápido",
      "C": "Não há diferença",
      "D": "Auto Loader não funciona com Delta"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 1136,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Fazer backup de notebooks",
      "B": "Apenas transferir dados entre clusters",
      "C": "Gerenciar permissões de usuários",
      "D": "Conectar IDEs locais ao Databricks para desenvolvimento remoto"
    },
    "correctAnswer": "D",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 1137,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Schema Evolution automática e detecção de novos arquivos",
      "B": "Apenas compressão de dados",
      "C": "Eliminação de duplicatas",
      "D": "Criptografia automática"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 1138,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Descarta automaticamente",
      "B": "Falha e interrompe a pipeline",
      "C": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "D": "Converte tudo para string"
    },
    "correctAnswer": "C",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 1139,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas APIs externas",
      "B": "S3, ADLS, GCS, SFTP",
      "C": "Apenas S3",
      "D": "Apenas bancos de dados relacionais"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 1140,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "B": "Não há diferença",
      "C": "Auto Loader não funciona com Delta",
      "D": "COPY INTO é mais rápido"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 1141,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Apenas transferir dados entre clusters",
      "B": "Gerenciar permissões de usuários",
      "C": "Fazer backup de notebooks",
      "D": "Conectar IDEs locais ao Databricks para desenvolvimento remoto"
    },
    "correctAnswer": "D",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 1142,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Schema Evolution automática e detecção de novos arquivos",
      "B": "Criptografia automática",
      "C": "Apenas compressão de dados",
      "D": "Eliminação de duplicatas"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 1143,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Falha e interrompe a pipeline",
      "B": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "C": "Converte tudo para string",
      "D": "Descarta automaticamente"
    },
    "correctAnswer": "B",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 1144,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas APIs externas",
      "B": "Apenas S3",
      "C": "Apenas bancos de dados relacionais",
      "D": "S3, ADLS, GCS, SFTP"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 1145,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Não há diferença",
      "B": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "C": "Auto Loader não funciona com Delta",
      "D": "COPY INTO é mais rápido"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 1146,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Fazer backup de notebooks",
      "B": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "C": "Gerenciar permissões de usuários",
      "D": "Apenas transferir dados entre clusters"
    },
    "correctAnswer": "B",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 1147,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Schema Evolution automática e detecção de novos arquivos",
      "B": "Criptografia automática",
      "C": "Eliminação de duplicatas",
      "D": "Apenas compressão de dados"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 1148,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "B": "Converte tudo para string",
      "C": "Descarta automaticamente",
      "D": "Falha e interrompe a pipeline"
    },
    "correctAnswer": "A",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 1149,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "S3, ADLS, GCS, SFTP",
      "B": "Apenas S3",
      "C": "Apenas APIs externas",
      "D": "Apenas bancos de dados relacionais"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 1150,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "COPY INTO é mais rápido",
      "B": "Não há diferença",
      "C": "Auto Loader não funciona com Delta",
      "D": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 1151,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "B": "Apenas transferir dados entre clusters",
      "C": "Gerenciar permissões de usuários",
      "D": "Fazer backup de notebooks"
    },
    "correctAnswer": "A",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 1152,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Criptografia automática",
      "B": "Apenas compressão de dados",
      "C": "Eliminação de duplicatas",
      "D": "Schema Evolution automática e detecção de novos arquivos"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 1153,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "B": "Descarta automaticamente",
      "C": "Falha e interrompe a pipeline",
      "D": "Converte tudo para string"
    },
    "correctAnswer": "A",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 1154,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "S3, ADLS, GCS, SFTP",
      "B": "Apenas bancos de dados relacionais",
      "C": "Apenas APIs externas",
      "D": "Apenas S3"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 1155,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "COPY INTO é mais rápido",
      "B": "Auto Loader não funciona com Delta",
      "C": "Não há diferença",
      "D": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 1156,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "B": "Fazer backup de notebooks",
      "C": "Apenas transferir dados entre clusters",
      "D": "Gerenciar permissões de usuários"
    },
    "correctAnswer": "A",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 1157,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Criptografia automática",
      "B": "Apenas compressão de dados",
      "C": "Schema Evolution automática e detecção de novos arquivos",
      "D": "Eliminação de duplicatas"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 1158,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "B": "Falha e interrompe a pipeline",
      "C": "Converte tudo para string",
      "D": "Descarta automaticamente"
    },
    "correctAnswer": "A",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 1159,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas APIs externas",
      "B": "S3, ADLS, GCS, SFTP",
      "C": "Apenas bancos de dados relacionais",
      "D": "Apenas S3"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 1160,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader não funciona com Delta",
      "B": "COPY INTO é mais rápido",
      "C": "Não há diferença",
      "D": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 1161,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Apenas transferir dados entre clusters",
      "B": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "C": "Gerenciar permissões de usuários",
      "D": "Fazer backup de notebooks"
    },
    "correctAnswer": "B",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 1162,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Apenas compressão de dados",
      "B": "Schema Evolution automática e detecção de novos arquivos",
      "C": "Eliminação de duplicatas",
      "D": "Criptografia automática"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 1163,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Converte tudo para string",
      "B": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "C": "Falha e interrompe a pipeline",
      "D": "Descarta automaticamente"
    },
    "correctAnswer": "B",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 1164,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas APIs externas",
      "B": "Apenas S3",
      "C": "S3, ADLS, GCS, SFTP",
      "D": "Apenas bancos de dados relacionais"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 1165,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "COPY INTO é mais rápido",
      "B": "Auto Loader não funciona com Delta",
      "C": "Não há diferença",
      "D": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 1166,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Apenas transferir dados entre clusters",
      "B": "Gerenciar permissões de usuários",
      "C": "Fazer backup de notebooks",
      "D": "Conectar IDEs locais ao Databricks para desenvolvimento remoto"
    },
    "correctAnswer": "D",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 1167,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Eliminação de duplicatas",
      "B": "Criptografia automática",
      "C": "Schema Evolution automática e detecção de novos arquivos",
      "D": "Apenas compressão de dados"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 1168,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Converte tudo para string",
      "B": "Descarta automaticamente",
      "C": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "D": "Falha e interrompe a pipeline"
    },
    "correctAnswer": "C",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 1169,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas APIs externas",
      "B": "Apenas S3",
      "C": "Apenas bancos de dados relacionais",
      "D": "S3, ADLS, GCS, SFTP"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 1170,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "B": "COPY INTO é mais rápido",
      "C": "Não há diferença",
      "D": "Auto Loader não funciona com Delta"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 1171,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Apenas transferir dados entre clusters",
      "B": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "C": "Fazer backup de notebooks",
      "D": "Gerenciar permissões de usuários"
    },
    "correctAnswer": "B",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 1172,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Schema Evolution automática e detecção de novos arquivos",
      "B": "Criptografia automática",
      "C": "Eliminação de duplicatas",
      "D": "Apenas compressão de dados"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 1173,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Falha e interrompe a pipeline",
      "B": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "C": "Descarta automaticamente",
      "D": "Converte tudo para string"
    },
    "correctAnswer": "B",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 1174,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas bancos de dados relacionais",
      "B": "Apenas S3",
      "C": "S3, ADLS, GCS, SFTP",
      "D": "Apenas APIs externas"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 1175,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "B": "COPY INTO é mais rápido",
      "C": "Não há diferença",
      "D": "Auto Loader não funciona com Delta"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 1176,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Fazer backup de notebooks",
      "B": "Gerenciar permissões de usuários",
      "C": "Apenas transferir dados entre clusters",
      "D": "Conectar IDEs locais ao Databricks para desenvolvimento remoto"
    },
    "correctAnswer": "D",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 1177,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Criptografia automática",
      "B": "Eliminação de duplicatas",
      "C": "Apenas compressão de dados",
      "D": "Schema Evolution automática e detecção de novos arquivos"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 1178,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Falha e interrompe a pipeline",
      "B": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "C": "Converte tudo para string",
      "D": "Descarta automaticamente"
    },
    "correctAnswer": "B",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 1179,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "S3, ADLS, GCS, SFTP",
      "B": "Apenas bancos de dados relacionais",
      "C": "Apenas APIs externas",
      "D": "Apenas S3"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 1180,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "B": "COPY INTO é mais rápido",
      "C": "Auto Loader não funciona com Delta",
      "D": "Não há diferença"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 1181,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Gerenciar permissões de usuários",
      "B": "Fazer backup de notebooks",
      "C": "Apenas transferir dados entre clusters",
      "D": "Conectar IDEs locais ao Databricks para desenvolvimento remoto"
    },
    "correctAnswer": "D",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 1182,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Eliminação de duplicatas",
      "B": "Apenas compressão de dados",
      "C": "Schema Evolution automática e detecção de novos arquivos",
      "D": "Criptografia automática"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 1183,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Falha e interrompe a pipeline",
      "B": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "C": "Converte tudo para string",
      "D": "Descarta automaticamente"
    },
    "correctAnswer": "B",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 1184,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas S3",
      "B": "Apenas APIs externas",
      "C": "Apenas bancos de dados relacionais",
      "D": "S3, ADLS, GCS, SFTP"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 1185,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "B": "Não há diferença",
      "C": "COPY INTO é mais rápido",
      "D": "Auto Loader não funciona com Delta"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 1186,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "B": "Fazer backup de notebooks",
      "C": "Apenas transferir dados entre clusters",
      "D": "Gerenciar permissões de usuários"
    },
    "correctAnswer": "A",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 1187,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Criptografia automática",
      "B": "Schema Evolution automática e detecção de novos arquivos",
      "C": "Apenas compressão de dados",
      "D": "Eliminação de duplicatas"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 1188,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Converte tudo para string",
      "B": "Falha e interrompe a pipeline",
      "C": "Descarta automaticamente",
      "D": "Usa 'rescuedDataColumn' para capturar dados problemáticos"
    },
    "correctAnswer": "D",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 1189,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas S3",
      "B": "Apenas APIs externas",
      "C": "Apenas bancos de dados relacionais",
      "D": "S3, ADLS, GCS, SFTP"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 1190,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader não funciona com Delta",
      "B": "Não há diferença",
      "C": "COPY INTO é mais rápido",
      "D": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual"
    },
    "correctAnswer": "D",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 1191,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Apenas transferir dados entre clusters",
      "B": "Gerenciar permissões de usuários",
      "C": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "D": "Fazer backup de notebooks"
    },
    "correctAnswer": "C",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 1192,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Apenas compressão de dados",
      "B": "Schema Evolution automática e detecção de novos arquivos",
      "C": "Eliminação de duplicatas",
      "D": "Criptografia automática"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 1193,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Falha e interrompe a pipeline",
      "B": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "C": "Descarta automaticamente",
      "D": "Converte tudo para string"
    },
    "correctAnswer": "B",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 1194,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "Apenas bancos de dados relacionais",
      "B": "S3, ADLS, GCS, SFTP",
      "C": "Apenas S3",
      "D": "Apenas APIs externas"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 1195,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "B": "Auto Loader não funciona com Delta",
      "C": "COPY INTO é mais rápido",
      "D": "Não há diferença"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 1196,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é o propósito principal do Databricks Connect?",
    "options": {
      "A": "Gerenciar permissões de usuários",
      "B": "Conectar IDEs locais ao Databricks para desenvolvimento remoto",
      "C": "Apenas transferir dados entre clusters",
      "D": "Fazer backup de notebooks"
    },
    "correctAnswer": "B",
    "rationale": "Databricks Connect permite que desenvolvedores usem IDEs locais (VS Code, PyCharm) para desenvolver código que executa remotamente no Databricks.",
    "tip": "Pense em desenvolvimento local com execução remota no Databricks."
  },
  {
    "id": 1197,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a principal vantagem do Auto Loader em relação a ingestão tradicional?",
    "options": {
      "A": "Eliminação de duplicatas",
      "B": "Apenas compressão de dados",
      "C": "Schema Evolution automática e detecção de novos arquivos",
      "D": "Criptografia automática"
    },
    "correctAnswer": "C",
    "rationale": "Auto Loader detecta automaticamente novos arquivos e evolui o schema sem intervenção manual, simplificando a ingestão de dados.",
    "tip": "Auto Loader = detecção automática + schema evolution."
  },
  {
    "id": 1198,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Como o Auto Loader trata dados com schema desconhecido?",
    "options": {
      "A": "Usa 'rescuedDataColumn' para capturar dados problemáticos",
      "B": "Falha e interrompe a pipeline",
      "C": "Descarta automaticamente",
      "D": "Converte tudo para string"
    },
    "correctAnswer": "A",
    "rationale": "O Auto Loader oferece a opção 'rescuedDataColumn' para capturar dados com schema desconhecido em uma coluna especial.",
    "tip": "rescuedDataColumn = coluna de quarentena para dados problemáticos."
  },
  {
    "id": 1199,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Quais são as fontes válidas para Auto Loader?",
    "options": {
      "A": "S3, ADLS, GCS, SFTP",
      "B": "Apenas S3",
      "C": "Apenas APIs externas",
      "D": "Apenas bancos de dados relacionais"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader suporta múltiplas fontes de armazenamento em nuvem: S3, Azure Data Lake Storage, Google Cloud Storage e SFTP.",
    "tip": "Auto Loader suporta os principais provedores de nuvem."
  },
  {
    "id": 1200,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Auto Loader e COPY INTO?",
    "options": {
      "A": "Auto Loader é contínuo e automático; COPY INTO é manual e pontual",
      "B": "Não há diferença",
      "C": "COPY INTO é mais rápido",
      "D": "Auto Loader não funciona com Delta"
    },
    "correctAnswer": "A",
    "rationale": "Auto Loader é projetado para ingestão contínua e automática, enquanto COPY INTO é para carregamento manual e pontual de dados.",
    "tip": "Auto Loader = contínuo; COPY INTO = pontual."
  },
  {
    "id": 1201,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos e validados",
      "B": "Armazenar dados brutos e não processados",
      "C": "Dados agregados para relatórios",
      "D": "Backup de dados"
    },
    "correctAnswer": "B",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1202,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos, validados e enriquecidos",
      "B": "Dados de backup",
      "C": "Dados brutos",
      "D": "Dados agregados"
    },
    "correctAnswer": "A",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1203,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados e transformados para relatórios de negócio",
      "B": "Dados de teste",
      "C": "Dados de backup",
      "D": "Dados brutos"
    },
    "correctAnswer": "A",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1204,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Em todas as camadas",
      "B": "Na camada Bronze",
      "C": "Na camada Silver após limpeza inicial",
      "D": "Na camada Gold"
    },
    "correctAnswer": "C",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1205,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Rejeitar toda a ingestão",
      "B": "Descartar imediatamente",
      "C": "Mover para Silver",
      "D": "Armazenar em tabela separada de quarentena"
    },
    "correctAnswer": "D",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1206,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos e validados",
      "B": "Armazenar dados brutos e não processados",
      "C": "Dados agregados para relatórios",
      "D": "Backup de dados"
    },
    "correctAnswer": "B",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1207,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos, validados e enriquecidos",
      "B": "Dados brutos",
      "C": "Dados de backup",
      "D": "Dados agregados"
    },
    "correctAnswer": "A",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1208,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados brutos",
      "C": "Dados agregados e transformados para relatórios de negócio",
      "D": "Dados de teste"
    },
    "correctAnswer": "C",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1209,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Gold",
      "B": "Na camada Silver após limpeza inicial",
      "C": "Em todas as camadas",
      "D": "Na camada Bronze"
    },
    "correctAnswer": "B",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1210,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Mover para Silver",
      "B": "Armazenar em tabela separada de quarentena",
      "C": "Rejeitar toda a ingestão",
      "D": "Descartar imediatamente"
    },
    "correctAnswer": "B",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1211,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos e validados",
      "B": "Armazenar dados brutos e não processados",
      "C": "Backup de dados",
      "D": "Dados agregados para relatórios"
    },
    "correctAnswer": "B",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1212,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados brutos",
      "C": "Dados limpos, validados e enriquecidos",
      "D": "Dados agregados"
    },
    "correctAnswer": "C",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1213,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados de backup",
      "C": "Dados de teste",
      "D": "Dados agregados e transformados para relatórios de negócio"
    },
    "correctAnswer": "D",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1214,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Bronze",
      "B": "Na camada Silver após limpeza inicial",
      "C": "Em todas as camadas",
      "D": "Na camada Gold"
    },
    "correctAnswer": "B",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1215,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Rejeitar toda a ingestão",
      "B": "Mover para Silver",
      "C": "Descartar imediatamente",
      "D": "Armazenar em tabela separada de quarentena"
    },
    "correctAnswer": "D",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1216,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados para relatórios",
      "B": "Dados limpos e validados",
      "C": "Armazenar dados brutos e não processados",
      "D": "Backup de dados"
    },
    "correctAnswer": "C",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1217,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos, validados e enriquecidos",
      "B": "Dados agregados",
      "C": "Dados brutos",
      "D": "Dados de backup"
    },
    "correctAnswer": "A",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1218,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados brutos",
      "C": "Dados agregados e transformados para relatórios de negócio",
      "D": "Dados de teste"
    },
    "correctAnswer": "C",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1219,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Gold",
      "B": "Na camada Bronze",
      "C": "Na camada Silver após limpeza inicial",
      "D": "Em todas as camadas"
    },
    "correctAnswer": "C",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1220,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Armazenar em tabela separada de quarentena",
      "B": "Mover para Silver",
      "C": "Rejeitar toda a ingestão",
      "D": "Descartar imediatamente"
    },
    "correctAnswer": "A",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1221,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Backup de dados",
      "B": "Dados limpos e validados",
      "C": "Armazenar dados brutos e não processados",
      "D": "Dados agregados para relatórios"
    },
    "correctAnswer": "C",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1222,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos, validados e enriquecidos",
      "B": "Dados agregados",
      "C": "Dados brutos",
      "D": "Dados de backup"
    },
    "correctAnswer": "A",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1223,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados de backup",
      "C": "Dados agregados e transformados para relatórios de negócio",
      "D": "Dados de teste"
    },
    "correctAnswer": "C",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1224,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Em todas as camadas",
      "B": "Na camada Bronze",
      "C": "Na camada Silver após limpeza inicial",
      "D": "Na camada Gold"
    },
    "correctAnswer": "C",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1225,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Mover para Silver",
      "B": "Armazenar em tabela separada de quarentena",
      "C": "Rejeitar toda a ingestão",
      "D": "Descartar imediatamente"
    },
    "correctAnswer": "B",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1226,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos e validados",
      "B": "Dados agregados para relatórios",
      "C": "Backup de dados",
      "D": "Armazenar dados brutos e não processados"
    },
    "correctAnswer": "D",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1227,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados agregados",
      "C": "Dados limpos, validados e enriquecidos",
      "D": "Dados de backup"
    },
    "correctAnswer": "C",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1228,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados e transformados para relatórios de negócio",
      "B": "Dados de backup",
      "C": "Dados brutos",
      "D": "Dados de teste"
    },
    "correctAnswer": "A",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1229,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Gold",
      "B": "Na camada Bronze",
      "C": "Na camada Silver após limpeza inicial",
      "D": "Em todas as camadas"
    },
    "correctAnswer": "C",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1230,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Rejeitar toda a ingestão",
      "B": "Descartar imediatamente",
      "C": "Mover para Silver",
      "D": "Armazenar em tabela separada de quarentena"
    },
    "correctAnswer": "D",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1231,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos e validados",
      "B": "Backup de dados",
      "C": "Armazenar dados brutos e não processados",
      "D": "Dados agregados para relatórios"
    },
    "correctAnswer": "C",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1232,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados limpos, validados e enriquecidos",
      "C": "Dados de backup",
      "D": "Dados agregados"
    },
    "correctAnswer": "B",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1233,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados e transformados para relatórios de negócio",
      "B": "Dados de teste",
      "C": "Dados de backup",
      "D": "Dados brutos"
    },
    "correctAnswer": "A",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1234,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Em todas as camadas",
      "B": "Na camada Bronze",
      "C": "Na camada Gold",
      "D": "Na camada Silver após limpeza inicial"
    },
    "correctAnswer": "D",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1235,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Mover para Silver",
      "B": "Armazenar em tabela separada de quarentena",
      "C": "Rejeitar toda a ingestão",
      "D": "Descartar imediatamente"
    },
    "correctAnswer": "B",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1236,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Armazenar dados brutos e não processados",
      "B": "Backup de dados",
      "C": "Dados agregados para relatórios",
      "D": "Dados limpos e validados"
    },
    "correctAnswer": "A",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1237,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados agregados",
      "C": "Dados de backup",
      "D": "Dados limpos, validados e enriquecidos"
    },
    "correctAnswer": "D",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1238,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados de teste",
      "B": "Dados de backup",
      "C": "Dados brutos",
      "D": "Dados agregados e transformados para relatórios de negócio"
    },
    "correctAnswer": "D",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1239,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Bronze",
      "B": "Em todas as camadas",
      "C": "Na camada Gold",
      "D": "Na camada Silver após limpeza inicial"
    },
    "correctAnswer": "D",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1240,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Armazenar em tabela separada de quarentena",
      "B": "Mover para Silver",
      "C": "Rejeitar toda a ingestão",
      "D": "Descartar imediatamente"
    },
    "correctAnswer": "A",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1241,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos e validados",
      "B": "Backup de dados",
      "C": "Armazenar dados brutos e não processados",
      "D": "Dados agregados para relatórios"
    },
    "correctAnswer": "C",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1242,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados de backup",
      "C": "Dados agregados",
      "D": "Dados limpos, validados e enriquecidos"
    },
    "correctAnswer": "D",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1243,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados e transformados para relatórios de negócio",
      "B": "Dados brutos",
      "C": "Dados de backup",
      "D": "Dados de teste"
    },
    "correctAnswer": "A",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1244,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Em todas as camadas",
      "B": "Na camada Silver após limpeza inicial",
      "C": "Na camada Gold",
      "D": "Na camada Bronze"
    },
    "correctAnswer": "B",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1245,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Rejeitar toda a ingestão",
      "B": "Descartar imediatamente",
      "C": "Armazenar em tabela separada de quarentena",
      "D": "Mover para Silver"
    },
    "correctAnswer": "C",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1246,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Armazenar dados brutos e não processados",
      "B": "Dados agregados para relatórios",
      "C": "Dados limpos e validados",
      "D": "Backup de dados"
    },
    "correctAnswer": "A",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1247,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos, validados e enriquecidos",
      "B": "Dados brutos",
      "C": "Dados agregados",
      "D": "Dados de backup"
    },
    "correctAnswer": "A",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1248,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados e transformados para relatórios de negócio",
      "B": "Dados brutos",
      "C": "Dados de backup",
      "D": "Dados de teste"
    },
    "correctAnswer": "A",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1249,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Silver após limpeza inicial",
      "B": "Na camada Gold",
      "C": "Em todas as camadas",
      "D": "Na camada Bronze"
    },
    "correctAnswer": "A",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1250,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Rejeitar toda a ingestão",
      "B": "Mover para Silver",
      "C": "Descartar imediatamente",
      "D": "Armazenar em tabela separada de quarentena"
    },
    "correctAnswer": "D",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1251,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados para relatórios",
      "B": "Armazenar dados brutos e não processados",
      "C": "Backup de dados",
      "D": "Dados limpos e validados"
    },
    "correctAnswer": "B",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1252,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos, validados e enriquecidos",
      "B": "Dados agregados",
      "C": "Dados de backup",
      "D": "Dados brutos"
    },
    "correctAnswer": "A",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1253,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados e transformados para relatórios de negócio",
      "B": "Dados de backup",
      "C": "Dados brutos",
      "D": "Dados de teste"
    },
    "correctAnswer": "A",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1254,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Bronze",
      "B": "Na camada Gold",
      "C": "Em todas as camadas",
      "D": "Na camada Silver após limpeza inicial"
    },
    "correctAnswer": "D",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1255,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Armazenar em tabela separada de quarentena",
      "B": "Rejeitar toda a ingestão",
      "C": "Mover para Silver",
      "D": "Descartar imediatamente"
    },
    "correctAnswer": "A",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1256,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados para relatórios",
      "B": "Armazenar dados brutos e não processados",
      "C": "Dados limpos e validados",
      "D": "Backup de dados"
    },
    "correctAnswer": "B",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1257,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados de backup",
      "C": "Dados agregados",
      "D": "Dados limpos, validados e enriquecidos"
    },
    "correctAnswer": "D",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1258,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados agregados e transformados para relatórios de negócio",
      "C": "Dados de teste",
      "D": "Dados de backup"
    },
    "correctAnswer": "B",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1259,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Bronze",
      "B": "Na camada Gold",
      "C": "Na camada Silver após limpeza inicial",
      "D": "Em todas as camadas"
    },
    "correctAnswer": "C",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1260,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Mover para Silver",
      "B": "Descartar imediatamente",
      "C": "Armazenar em tabela separada de quarentena",
      "D": "Rejeitar toda a ingestão"
    },
    "correctAnswer": "C",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1261,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados para relatórios",
      "B": "Dados limpos e validados",
      "C": "Backup de dados",
      "D": "Armazenar dados brutos e não processados"
    },
    "correctAnswer": "D",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1262,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos, validados e enriquecidos",
      "B": "Dados agregados",
      "C": "Dados brutos",
      "D": "Dados de backup"
    },
    "correctAnswer": "A",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1263,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados e transformados para relatórios de negócio",
      "B": "Dados brutos",
      "C": "Dados de backup",
      "D": "Dados de teste"
    },
    "correctAnswer": "A",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1264,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Silver após limpeza inicial",
      "B": "Na camada Gold",
      "C": "Em todas as camadas",
      "D": "Na camada Bronze"
    },
    "correctAnswer": "A",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1265,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Rejeitar toda a ingestão",
      "B": "Mover para Silver",
      "C": "Descartar imediatamente",
      "D": "Armazenar em tabela separada de quarentena"
    },
    "correctAnswer": "D",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1266,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados para relatórios",
      "B": "Backup de dados",
      "C": "Armazenar dados brutos e não processados",
      "D": "Dados limpos e validados"
    },
    "correctAnswer": "C",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1267,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados brutos",
      "C": "Dados limpos, validados e enriquecidos",
      "D": "Dados agregados"
    },
    "correctAnswer": "C",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1268,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados agregados e transformados para relatórios de negócio",
      "C": "Dados de teste",
      "D": "Dados de backup"
    },
    "correctAnswer": "B",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1269,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Em todas as camadas",
      "B": "Na camada Gold",
      "C": "Na camada Silver após limpeza inicial",
      "D": "Na camada Bronze"
    },
    "correctAnswer": "C",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1270,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Rejeitar toda a ingestão",
      "B": "Mover para Silver",
      "C": "Descartar imediatamente",
      "D": "Armazenar em tabela separada de quarentena"
    },
    "correctAnswer": "D",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1271,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Backup de dados",
      "B": "Dados agregados para relatórios",
      "C": "Armazenar dados brutos e não processados",
      "D": "Dados limpos e validados"
    },
    "correctAnswer": "C",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1272,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos, validados e enriquecidos",
      "B": "Dados agregados",
      "C": "Dados de backup",
      "D": "Dados brutos"
    },
    "correctAnswer": "A",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1273,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados de teste",
      "C": "Dados agregados e transformados para relatórios de negócio",
      "D": "Dados de backup"
    },
    "correctAnswer": "C",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1274,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Gold",
      "B": "Na camada Silver após limpeza inicial",
      "C": "Em todas as camadas",
      "D": "Na camada Bronze"
    },
    "correctAnswer": "B",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1275,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Armazenar em tabela separada de quarentena",
      "B": "Rejeitar toda a ingestão",
      "C": "Mover para Silver",
      "D": "Descartar imediatamente"
    },
    "correctAnswer": "A",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1276,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Armazenar dados brutos e não processados",
      "B": "Dados agregados para relatórios",
      "C": "Backup de dados",
      "D": "Dados limpos e validados"
    },
    "correctAnswer": "A",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1277,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados agregados",
      "C": "Dados limpos, validados e enriquecidos",
      "D": "Dados brutos"
    },
    "correctAnswer": "C",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1278,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados agregados e transformados para relatórios de negócio",
      "C": "Dados de backup",
      "D": "Dados de teste"
    },
    "correctAnswer": "B",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1279,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Em todas as camadas",
      "B": "Na camada Silver após limpeza inicial",
      "C": "Na camada Bronze",
      "D": "Na camada Gold"
    },
    "correctAnswer": "B",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1280,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Armazenar em tabela separada de quarentena",
      "B": "Mover para Silver",
      "C": "Rejeitar toda a ingestão",
      "D": "Descartar imediatamente"
    },
    "correctAnswer": "A",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1281,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados para relatórios",
      "B": "Dados limpos e validados",
      "C": "Armazenar dados brutos e não processados",
      "D": "Backup de dados"
    },
    "correctAnswer": "C",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1282,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados agregados",
      "C": "Dados brutos",
      "D": "Dados limpos, validados e enriquecidos"
    },
    "correctAnswer": "D",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1283,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados de backup",
      "C": "Dados agregados e transformados para relatórios de negócio",
      "D": "Dados de teste"
    },
    "correctAnswer": "C",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1284,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Bronze",
      "B": "Na camada Gold",
      "C": "Em todas as camadas",
      "D": "Na camada Silver após limpeza inicial"
    },
    "correctAnswer": "D",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1285,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Mover para Silver",
      "B": "Rejeitar toda a ingestão",
      "C": "Armazenar em tabela separada de quarentena",
      "D": "Descartar imediatamente"
    },
    "correctAnswer": "C",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1286,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Backup de dados",
      "B": "Armazenar dados brutos e não processados",
      "C": "Dados agregados para relatórios",
      "D": "Dados limpos e validados"
    },
    "correctAnswer": "B",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1287,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados limpos, validados e enriquecidos",
      "C": "Dados de backup",
      "D": "Dados agregados"
    },
    "correctAnswer": "B",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1288,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados e transformados para relatórios de negócio",
      "B": "Dados de backup",
      "C": "Dados brutos",
      "D": "Dados de teste"
    },
    "correctAnswer": "A",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1289,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Gold",
      "B": "Na camada Silver após limpeza inicial",
      "C": "Em todas as camadas",
      "D": "Na camada Bronze"
    },
    "correctAnswer": "B",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1290,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Armazenar em tabela separada de quarentena",
      "B": "Rejeitar toda a ingestão",
      "C": "Descartar imediatamente",
      "D": "Mover para Silver"
    },
    "correctAnswer": "A",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1291,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Backup de dados",
      "B": "Dados agregados para relatórios",
      "C": "Dados limpos e validados",
      "D": "Armazenar dados brutos e não processados"
    },
    "correctAnswer": "D",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1292,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados brutos",
      "C": "Dados limpos, validados e enriquecidos",
      "D": "Dados agregados"
    },
    "correctAnswer": "C",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1293,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados e transformados para relatórios de negócio",
      "B": "Dados de backup",
      "C": "Dados de teste",
      "D": "Dados brutos"
    },
    "correctAnswer": "A",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1294,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Em todas as camadas",
      "B": "Na camada Silver após limpeza inicial",
      "C": "Na camada Gold",
      "D": "Na camada Bronze"
    },
    "correctAnswer": "B",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1295,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Armazenar em tabela separada de quarentena",
      "B": "Mover para Silver",
      "C": "Descartar imediatamente",
      "D": "Rejeitar toda a ingestão"
    },
    "correctAnswer": "A",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1296,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos e validados",
      "B": "Backup de dados",
      "C": "Dados agregados para relatórios",
      "D": "Armazenar dados brutos e não processados"
    },
    "correctAnswer": "D",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1297,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados agregados",
      "C": "Dados limpos, validados e enriquecidos",
      "D": "Dados brutos"
    },
    "correctAnswer": "C",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1298,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados agregados e transformados para relatórios de negócio",
      "C": "Dados brutos",
      "D": "Dados de teste"
    },
    "correctAnswer": "B",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1299,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Em todas as camadas",
      "B": "Na camada Silver após limpeza inicial",
      "C": "Na camada Gold",
      "D": "Na camada Bronze"
    },
    "correctAnswer": "B",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1300,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Descartar imediatamente",
      "B": "Mover para Silver",
      "C": "Armazenar em tabela separada de quarentena",
      "D": "Rejeitar toda a ingestão"
    },
    "correctAnswer": "C",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1301,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Armazenar dados brutos e não processados",
      "B": "Backup de dados",
      "C": "Dados agregados para relatórios",
      "D": "Dados limpos e validados"
    },
    "correctAnswer": "A",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1302,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados de backup",
      "C": "Dados limpos, validados e enriquecidos",
      "D": "Dados agregados"
    },
    "correctAnswer": "C",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1303,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados agregados e transformados para relatórios de negócio",
      "C": "Dados de backup",
      "D": "Dados de teste"
    },
    "correctAnswer": "B",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1304,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Gold",
      "B": "Na camada Bronze",
      "C": "Na camada Silver após limpeza inicial",
      "D": "Em todas as camadas"
    },
    "correctAnswer": "C",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1305,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Mover para Silver",
      "B": "Rejeitar toda a ingestão",
      "C": "Descartar imediatamente",
      "D": "Armazenar em tabela separada de quarentena"
    },
    "correctAnswer": "D",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1306,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados para relatórios",
      "B": "Armazenar dados brutos e não processados",
      "C": "Dados limpos e validados",
      "D": "Backup de dados"
    },
    "correctAnswer": "B",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1307,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados limpos, validados e enriquecidos",
      "C": "Dados agregados",
      "D": "Dados de backup"
    },
    "correctAnswer": "B",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1308,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados e transformados para relatórios de negócio",
      "B": "Dados brutos",
      "C": "Dados de backup",
      "D": "Dados de teste"
    },
    "correctAnswer": "A",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1309,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Silver após limpeza inicial",
      "B": "Na camada Gold",
      "C": "Na camada Bronze",
      "D": "Em todas as camadas"
    },
    "correctAnswer": "A",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1310,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Rejeitar toda a ingestão",
      "B": "Mover para Silver",
      "C": "Armazenar em tabela separada de quarentena",
      "D": "Descartar imediatamente"
    },
    "correctAnswer": "C",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1311,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados para relatórios",
      "B": "Backup de dados",
      "C": "Dados limpos e validados",
      "D": "Armazenar dados brutos e não processados"
    },
    "correctAnswer": "D",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1312,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos, validados e enriquecidos",
      "B": "Dados de backup",
      "C": "Dados brutos",
      "D": "Dados agregados"
    },
    "correctAnswer": "A",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1313,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados agregados e transformados para relatórios de negócio",
      "C": "Dados brutos",
      "D": "Dados de teste"
    },
    "correctAnswer": "B",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1314,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Em todas as camadas",
      "B": "Na camada Gold",
      "C": "Na camada Silver após limpeza inicial",
      "D": "Na camada Bronze"
    },
    "correctAnswer": "C",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1315,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Rejeitar toda a ingestão",
      "B": "Descartar imediatamente",
      "C": "Mover para Silver",
      "D": "Armazenar em tabela separada de quarentena"
    },
    "correctAnswer": "D",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1316,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Backup de dados",
      "B": "Armazenar dados brutos e não processados",
      "C": "Dados agregados para relatórios",
      "D": "Dados limpos e validados"
    },
    "correctAnswer": "B",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1317,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos, validados e enriquecidos",
      "B": "Dados de backup",
      "C": "Dados agregados",
      "D": "Dados brutos"
    },
    "correctAnswer": "A",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1318,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados de backup",
      "C": "Dados de teste",
      "D": "Dados agregados e transformados para relatórios de negócio"
    },
    "correctAnswer": "D",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1319,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Gold",
      "B": "Em todas as camadas",
      "C": "Na camada Bronze",
      "D": "Na camada Silver após limpeza inicial"
    },
    "correctAnswer": "D",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1320,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Rejeitar toda a ingestão",
      "B": "Mover para Silver",
      "C": "Armazenar em tabela separada de quarentena",
      "D": "Descartar imediatamente"
    },
    "correctAnswer": "C",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1321,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos e validados",
      "B": "Backup de dados",
      "C": "Dados agregados para relatórios",
      "D": "Armazenar dados brutos e não processados"
    },
    "correctAnswer": "D",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1322,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados",
      "B": "Dados de backup",
      "C": "Dados limpos, validados e enriquecidos",
      "D": "Dados brutos"
    },
    "correctAnswer": "C",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1323,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados e transformados para relatórios de negócio",
      "B": "Dados de teste",
      "C": "Dados brutos",
      "D": "Dados de backup"
    },
    "correctAnswer": "A",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1324,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Gold",
      "B": "Na camada Bronze",
      "C": "Na camada Silver após limpeza inicial",
      "D": "Em todas as camadas"
    },
    "correctAnswer": "C",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1325,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Rejeitar toda a ingestão",
      "B": "Armazenar em tabela separada de quarentena",
      "C": "Descartar imediatamente",
      "D": "Mover para Silver"
    },
    "correctAnswer": "B",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1326,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados para relatórios",
      "B": "Backup de dados",
      "C": "Armazenar dados brutos e não processados",
      "D": "Dados limpos e validados"
    },
    "correctAnswer": "C",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1327,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados brutos",
      "C": "Dados agregados",
      "D": "Dados limpos, validados e enriquecidos"
    },
    "correctAnswer": "D",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1328,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados de teste",
      "B": "Dados brutos",
      "C": "Dados agregados e transformados para relatórios de negócio",
      "D": "Dados de backup"
    },
    "correctAnswer": "C",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1329,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Silver após limpeza inicial",
      "B": "Na camada Gold",
      "C": "Em todas as camadas",
      "D": "Na camada Bronze"
    },
    "correctAnswer": "A",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1330,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Rejeitar toda a ingestão",
      "B": "Descartar imediatamente",
      "C": "Armazenar em tabela separada de quarentena",
      "D": "Mover para Silver"
    },
    "correctAnswer": "C",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1331,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos e validados",
      "B": "Dados agregados para relatórios",
      "C": "Backup de dados",
      "D": "Armazenar dados brutos e não processados"
    },
    "correctAnswer": "D",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1332,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados agregados",
      "C": "Dados limpos, validados e enriquecidos",
      "D": "Dados brutos"
    },
    "correctAnswer": "C",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1333,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados agregados e transformados para relatórios de negócio",
      "C": "Dados de backup",
      "D": "Dados de teste"
    },
    "correctAnswer": "B",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1334,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Bronze",
      "B": "Na camada Silver após limpeza inicial",
      "C": "Em todas as camadas",
      "D": "Na camada Gold"
    },
    "correctAnswer": "B",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1335,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Rejeitar toda a ingestão",
      "B": "Descartar imediatamente",
      "C": "Armazenar em tabela separada de quarentena",
      "D": "Mover para Silver"
    },
    "correctAnswer": "C",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1336,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados para relatórios",
      "B": "Backup de dados",
      "C": "Armazenar dados brutos e não processados",
      "D": "Dados limpos e validados"
    },
    "correctAnswer": "C",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1337,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados de backup",
      "C": "Dados agregados",
      "D": "Dados limpos, validados e enriquecidos"
    },
    "correctAnswer": "D",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1338,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados de teste",
      "C": "Dados agregados e transformados para relatórios de negócio",
      "D": "Dados brutos"
    },
    "correctAnswer": "C",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1339,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Em todas as camadas",
      "B": "Na camada Silver após limpeza inicial",
      "C": "Na camada Gold",
      "D": "Na camada Bronze"
    },
    "correctAnswer": "B",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1340,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Armazenar em tabela separada de quarentena",
      "B": "Mover para Silver",
      "C": "Rejeitar toda a ingestão",
      "D": "Descartar imediatamente"
    },
    "correctAnswer": "A",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1341,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Backup de dados",
      "B": "Armazenar dados brutos e não processados",
      "C": "Dados limpos e validados",
      "D": "Dados agregados para relatórios"
    },
    "correctAnswer": "B",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1342,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados de backup",
      "C": "Dados agregados",
      "D": "Dados limpos, validados e enriquecidos"
    },
    "correctAnswer": "D",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1343,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados de backup",
      "C": "Dados agregados e transformados para relatórios de negócio",
      "D": "Dados de teste"
    },
    "correctAnswer": "C",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1344,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Em todas as camadas",
      "B": "Na camada Gold",
      "C": "Na camada Silver após limpeza inicial",
      "D": "Na camada Bronze"
    },
    "correctAnswer": "C",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1345,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Mover para Silver",
      "B": "Armazenar em tabela separada de quarentena",
      "C": "Descartar imediatamente",
      "D": "Rejeitar toda a ingestão"
    },
    "correctAnswer": "B",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1346,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Backup de dados",
      "B": "Armazenar dados brutos e não processados",
      "C": "Dados limpos e validados",
      "D": "Dados agregados para relatórios"
    },
    "correctAnswer": "B",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1347,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados agregados",
      "C": "Dados brutos",
      "D": "Dados limpos, validados e enriquecidos"
    },
    "correctAnswer": "D",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1348,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados e transformados para relatórios de negócio",
      "B": "Dados de backup",
      "C": "Dados brutos",
      "D": "Dados de teste"
    },
    "correctAnswer": "A",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1349,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Bronze",
      "B": "Em todas as camadas",
      "C": "Na camada Silver após limpeza inicial",
      "D": "Na camada Gold"
    },
    "correctAnswer": "C",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1350,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Rejeitar toda a ingestão",
      "B": "Descartar imediatamente",
      "C": "Armazenar em tabela separada de quarentena",
      "D": "Mover para Silver"
    },
    "correctAnswer": "C",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1351,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Backup de dados",
      "B": "Dados agregados para relatórios",
      "C": "Dados limpos e validados",
      "D": "Armazenar dados brutos e não processados"
    },
    "correctAnswer": "D",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1352,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados brutos",
      "C": "Dados limpos, validados e enriquecidos",
      "D": "Dados agregados"
    },
    "correctAnswer": "C",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1353,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados agregados e transformados para relatórios de negócio",
      "C": "Dados de backup",
      "D": "Dados de teste"
    },
    "correctAnswer": "B",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1354,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Silver após limpeza inicial",
      "B": "Na camada Gold",
      "C": "Na camada Bronze",
      "D": "Em todas as camadas"
    },
    "correctAnswer": "A",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1355,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Descartar imediatamente",
      "B": "Mover para Silver",
      "C": "Armazenar em tabela separada de quarentena",
      "D": "Rejeitar toda a ingestão"
    },
    "correctAnswer": "C",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1356,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados para relatórios",
      "B": "Armazenar dados brutos e não processados",
      "C": "Dados limpos e validados",
      "D": "Backup de dados"
    },
    "correctAnswer": "B",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1357,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos, validados e enriquecidos",
      "B": "Dados agregados",
      "C": "Dados de backup",
      "D": "Dados brutos"
    },
    "correctAnswer": "A",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1358,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados brutos",
      "C": "Dados agregados e transformados para relatórios de negócio",
      "D": "Dados de teste"
    },
    "correctAnswer": "C",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1359,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Bronze",
      "B": "Na camada Gold",
      "C": "Em todas as camadas",
      "D": "Na camada Silver após limpeza inicial"
    },
    "correctAnswer": "D",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1360,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Armazenar em tabela separada de quarentena",
      "B": "Rejeitar toda a ingestão",
      "C": "Mover para Silver",
      "D": "Descartar imediatamente"
    },
    "correctAnswer": "A",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1361,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados para relatórios",
      "B": "Dados limpos e validados",
      "C": "Armazenar dados brutos e não processados",
      "D": "Backup de dados"
    },
    "correctAnswer": "C",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1362,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados de backup",
      "C": "Dados limpos, validados e enriquecidos",
      "D": "Dados agregados"
    },
    "correctAnswer": "C",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1363,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados de teste",
      "C": "Dados agregados e transformados para relatórios de negócio",
      "D": "Dados brutos"
    },
    "correctAnswer": "C",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1364,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Gold",
      "B": "Na camada Silver após limpeza inicial",
      "C": "Na camada Bronze",
      "D": "Em todas as camadas"
    },
    "correctAnswer": "B",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1365,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Armazenar em tabela separada de quarentena",
      "B": "Rejeitar toda a ingestão",
      "C": "Mover para Silver",
      "D": "Descartar imediatamente"
    },
    "correctAnswer": "A",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1366,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos e validados",
      "B": "Armazenar dados brutos e não processados",
      "C": "Backup de dados",
      "D": "Dados agregados para relatórios"
    },
    "correctAnswer": "B",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1367,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados de backup",
      "C": "Dados agregados",
      "D": "Dados limpos, validados e enriquecidos"
    },
    "correctAnswer": "D",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1368,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados de teste",
      "B": "Dados brutos",
      "C": "Dados de backup",
      "D": "Dados agregados e transformados para relatórios de negócio"
    },
    "correctAnswer": "D",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1369,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Em todas as camadas",
      "B": "Na camada Bronze",
      "C": "Na camada Gold",
      "D": "Na camada Silver após limpeza inicial"
    },
    "correctAnswer": "D",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1370,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Descartar imediatamente",
      "B": "Armazenar em tabela separada de quarentena",
      "C": "Rejeitar toda a ingestão",
      "D": "Mover para Silver"
    },
    "correctAnswer": "B",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1371,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos e validados",
      "B": "Armazenar dados brutos e não processados",
      "C": "Dados agregados para relatórios",
      "D": "Backup de dados"
    },
    "correctAnswer": "B",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1372,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados agregados",
      "C": "Dados limpos, validados e enriquecidos",
      "D": "Dados de backup"
    },
    "correctAnswer": "C",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1373,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados de teste",
      "B": "Dados agregados e transformados para relatórios de negócio",
      "C": "Dados de backup",
      "D": "Dados brutos"
    },
    "correctAnswer": "B",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1374,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Gold",
      "B": "Na camada Silver após limpeza inicial",
      "C": "Na camada Bronze",
      "D": "Em todas as camadas"
    },
    "correctAnswer": "B",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1375,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Armazenar em tabela separada de quarentena",
      "B": "Mover para Silver",
      "C": "Descartar imediatamente",
      "D": "Rejeitar toda a ingestão"
    },
    "correctAnswer": "A",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1376,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Armazenar dados brutos e não processados",
      "B": "Dados agregados para relatórios",
      "C": "Dados limpos e validados",
      "D": "Backup de dados"
    },
    "correctAnswer": "A",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1377,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados agregados",
      "C": "Dados limpos, validados e enriquecidos",
      "D": "Dados brutos"
    },
    "correctAnswer": "C",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1378,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados e transformados para relatórios de negócio",
      "B": "Dados de teste",
      "C": "Dados de backup",
      "D": "Dados brutos"
    },
    "correctAnswer": "A",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1379,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Bronze",
      "B": "Na camada Silver após limpeza inicial",
      "C": "Em todas as camadas",
      "D": "Na camada Gold"
    },
    "correctAnswer": "B",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1380,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Rejeitar toda a ingestão",
      "B": "Mover para Silver",
      "C": "Descartar imediatamente",
      "D": "Armazenar em tabela separada de quarentena"
    },
    "correctAnswer": "D",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1381,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos e validados",
      "B": "Backup de dados",
      "C": "Armazenar dados brutos e não processados",
      "D": "Dados agregados para relatórios"
    },
    "correctAnswer": "C",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1382,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados agregados",
      "C": "Dados limpos, validados e enriquecidos",
      "D": "Dados brutos"
    },
    "correctAnswer": "C",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1383,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados de teste",
      "C": "Dados brutos",
      "D": "Dados agregados e transformados para relatórios de negócio"
    },
    "correctAnswer": "D",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1384,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Bronze",
      "B": "Na camada Gold",
      "C": "Na camada Silver após limpeza inicial",
      "D": "Em todas as camadas"
    },
    "correctAnswer": "C",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1385,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Descartar imediatamente",
      "B": "Armazenar em tabela separada de quarentena",
      "C": "Rejeitar toda a ingestão",
      "D": "Mover para Silver"
    },
    "correctAnswer": "B",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1386,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Armazenar dados brutos e não processados",
      "B": "Dados agregados para relatórios",
      "C": "Backup de dados",
      "D": "Dados limpos e validados"
    },
    "correctAnswer": "A",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1387,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados limpos, validados e enriquecidos",
      "C": "Dados brutos",
      "D": "Dados agregados"
    },
    "correctAnswer": "B",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1388,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados de teste",
      "C": "Dados agregados e transformados para relatórios de negócio",
      "D": "Dados de backup"
    },
    "correctAnswer": "C",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1389,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Silver após limpeza inicial",
      "B": "Na camada Gold",
      "C": "Na camada Bronze",
      "D": "Em todas as camadas"
    },
    "correctAnswer": "A",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1390,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Mover para Silver",
      "B": "Armazenar em tabela separada de quarentena",
      "C": "Descartar imediatamente",
      "D": "Rejeitar toda a ingestão"
    },
    "correctAnswer": "B",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1391,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados para relatórios",
      "B": "Armazenar dados brutos e não processados",
      "C": "Dados limpos e validados",
      "D": "Backup de dados"
    },
    "correctAnswer": "B",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1392,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos, validados e enriquecidos",
      "B": "Dados agregados",
      "C": "Dados de backup",
      "D": "Dados brutos"
    },
    "correctAnswer": "A",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1393,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados brutos",
      "C": "Dados agregados e transformados para relatórios de negócio",
      "D": "Dados de teste"
    },
    "correctAnswer": "C",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1394,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Gold",
      "B": "Na camada Silver após limpeza inicial",
      "C": "Em todas as camadas",
      "D": "Na camada Bronze"
    },
    "correctAnswer": "B",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1395,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Armazenar em tabela separada de quarentena",
      "B": "Descartar imediatamente",
      "C": "Rejeitar toda a ingestão",
      "D": "Mover para Silver"
    },
    "correctAnswer": "A",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1396,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Armazenar dados brutos e não processados",
      "B": "Dados agregados para relatórios",
      "C": "Dados limpos e validados",
      "D": "Backup de dados"
    },
    "correctAnswer": "A",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1397,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados brutos",
      "C": "Dados agregados",
      "D": "Dados limpos, validados e enriquecidos"
    },
    "correctAnswer": "D",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1398,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados de teste",
      "C": "Dados agregados e transformados para relatórios de negócio",
      "D": "Dados brutos"
    },
    "correctAnswer": "C",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1399,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Em todas as camadas",
      "B": "Na camada Gold",
      "C": "Na camada Silver após limpeza inicial",
      "D": "Na camada Bronze"
    },
    "correctAnswer": "C",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1400,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Descartar imediatamente",
      "B": "Mover para Silver",
      "C": "Armazenar em tabela separada de quarentena",
      "D": "Rejeitar toda a ingestão"
    },
    "correctAnswer": "C",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1401,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Armazenar dados brutos e não processados",
      "B": "Dados limpos e validados",
      "C": "Backup de dados",
      "D": "Dados agregados para relatórios"
    },
    "correctAnswer": "A",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1402,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados agregados",
      "C": "Dados brutos",
      "D": "Dados limpos, validados e enriquecidos"
    },
    "correctAnswer": "D",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1403,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados e transformados para relatórios de negócio",
      "B": "Dados de backup",
      "C": "Dados de teste",
      "D": "Dados brutos"
    },
    "correctAnswer": "A",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1404,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Gold",
      "B": "Em todas as camadas",
      "C": "Na camada Silver após limpeza inicial",
      "D": "Na camada Bronze"
    },
    "correctAnswer": "C",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1405,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Descartar imediatamente",
      "B": "Mover para Silver",
      "C": "Rejeitar toda a ingestão",
      "D": "Armazenar em tabela separada de quarentena"
    },
    "correctAnswer": "D",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1406,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos e validados",
      "B": "Backup de dados",
      "C": "Armazenar dados brutos e não processados",
      "D": "Dados agregados para relatórios"
    },
    "correctAnswer": "C",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1407,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos, validados e enriquecidos",
      "B": "Dados agregados",
      "C": "Dados de backup",
      "D": "Dados brutos"
    },
    "correctAnswer": "A",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1408,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados de backup",
      "C": "Dados de teste",
      "D": "Dados agregados e transformados para relatórios de negócio"
    },
    "correctAnswer": "D",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1409,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Silver após limpeza inicial",
      "B": "Na camada Bronze",
      "C": "Em todas as camadas",
      "D": "Na camada Gold"
    },
    "correctAnswer": "A",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1410,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Armazenar em tabela separada de quarentena",
      "B": "Mover para Silver",
      "C": "Descartar imediatamente",
      "D": "Rejeitar toda a ingestão"
    },
    "correctAnswer": "A",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1411,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Backup de dados",
      "B": "Dados agregados para relatórios",
      "C": "Dados limpos e validados",
      "D": "Armazenar dados brutos e não processados"
    },
    "correctAnswer": "D",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1412,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados de backup",
      "C": "Dados agregados",
      "D": "Dados limpos, validados e enriquecidos"
    },
    "correctAnswer": "D",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1413,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados brutos",
      "C": "Dados de teste",
      "D": "Dados agregados e transformados para relatórios de negócio"
    },
    "correctAnswer": "D",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1414,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Gold",
      "B": "Em todas as camadas",
      "C": "Na camada Silver após limpeza inicial",
      "D": "Na camada Bronze"
    },
    "correctAnswer": "C",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1415,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Mover para Silver",
      "B": "Armazenar em tabela separada de quarentena",
      "C": "Descartar imediatamente",
      "D": "Rejeitar toda a ingestão"
    },
    "correctAnswer": "B",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1416,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Armazenar dados brutos e não processados",
      "B": "Dados agregados para relatórios",
      "C": "Dados limpos e validados",
      "D": "Backup de dados"
    },
    "correctAnswer": "A",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1417,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados",
      "B": "Dados de backup",
      "C": "Dados brutos",
      "D": "Dados limpos, validados e enriquecidos"
    },
    "correctAnswer": "D",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1418,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados de teste",
      "B": "Dados de backup",
      "C": "Dados brutos",
      "D": "Dados agregados e transformados para relatórios de negócio"
    },
    "correctAnswer": "D",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1419,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Bronze",
      "B": "Em todas as camadas",
      "C": "Na camada Gold",
      "D": "Na camada Silver após limpeza inicial"
    },
    "correctAnswer": "D",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1420,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Descartar imediatamente",
      "B": "Mover para Silver",
      "C": "Armazenar em tabela separada de quarentena",
      "D": "Rejeitar toda a ingestão"
    },
    "correctAnswer": "C",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1421,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Backup de dados",
      "B": "Dados limpos e validados",
      "C": "Armazenar dados brutos e não processados",
      "D": "Dados agregados para relatórios"
    },
    "correctAnswer": "C",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1422,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados",
      "B": "Dados brutos",
      "C": "Dados limpos, validados e enriquecidos",
      "D": "Dados de backup"
    },
    "correctAnswer": "C",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1423,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados de teste",
      "C": "Dados agregados e transformados para relatórios de negócio",
      "D": "Dados de backup"
    },
    "correctAnswer": "C",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1424,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Bronze",
      "B": "Na camada Gold",
      "C": "Na camada Silver após limpeza inicial",
      "D": "Em todas as camadas"
    },
    "correctAnswer": "C",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1425,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Mover para Silver",
      "B": "Rejeitar toda a ingestão",
      "C": "Descartar imediatamente",
      "D": "Armazenar em tabela separada de quarentena"
    },
    "correctAnswer": "D",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1426,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos e validados",
      "B": "Armazenar dados brutos e não processados",
      "C": "Dados agregados para relatórios",
      "D": "Backup de dados"
    },
    "correctAnswer": "B",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1427,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados limpos, validados e enriquecidos",
      "C": "Dados de backup",
      "D": "Dados agregados"
    },
    "correctAnswer": "B",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1428,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados brutos",
      "C": "Dados agregados e transformados para relatórios de negócio",
      "D": "Dados de teste"
    },
    "correctAnswer": "C",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1429,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Gold",
      "B": "Em todas as camadas",
      "C": "Na camada Bronze",
      "D": "Na camada Silver após limpeza inicial"
    },
    "correctAnswer": "D",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1430,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Rejeitar toda a ingestão",
      "B": "Armazenar em tabela separada de quarentena",
      "C": "Descartar imediatamente",
      "D": "Mover para Silver"
    },
    "correctAnswer": "B",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1431,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos e validados",
      "B": "Dados agregados para relatórios",
      "C": "Backup de dados",
      "D": "Armazenar dados brutos e não processados"
    },
    "correctAnswer": "D",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1432,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos, validados e enriquecidos",
      "B": "Dados de backup",
      "C": "Dados agregados",
      "D": "Dados brutos"
    },
    "correctAnswer": "A",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1433,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados de teste",
      "B": "Dados de backup",
      "C": "Dados brutos",
      "D": "Dados agregados e transformados para relatórios de negócio"
    },
    "correctAnswer": "D",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1434,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Gold",
      "B": "Em todas as camadas",
      "C": "Na camada Silver após limpeza inicial",
      "D": "Na camada Bronze"
    },
    "correctAnswer": "C",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1435,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Armazenar em tabela separada de quarentena",
      "B": "Rejeitar toda a ingestão",
      "C": "Descartar imediatamente",
      "D": "Mover para Silver"
    },
    "correctAnswer": "A",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1436,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Armazenar dados brutos e não processados",
      "B": "Backup de dados",
      "C": "Dados limpos e validados",
      "D": "Dados agregados para relatórios"
    },
    "correctAnswer": "A",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1437,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados agregados",
      "C": "Dados limpos, validados e enriquecidos",
      "D": "Dados de backup"
    },
    "correctAnswer": "C",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1438,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados de teste",
      "B": "Dados brutos",
      "C": "Dados de backup",
      "D": "Dados agregados e transformados para relatórios de negócio"
    },
    "correctAnswer": "D",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1439,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Silver após limpeza inicial",
      "B": "Em todas as camadas",
      "C": "Na camada Bronze",
      "D": "Na camada Gold"
    },
    "correctAnswer": "A",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1440,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Rejeitar toda a ingestão",
      "B": "Armazenar em tabela separada de quarentena",
      "C": "Descartar imediatamente",
      "D": "Mover para Silver"
    },
    "correctAnswer": "B",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1441,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Armazenar dados brutos e não processados",
      "B": "Dados limpos e validados",
      "C": "Dados agregados para relatórios",
      "D": "Backup de dados"
    },
    "correctAnswer": "A",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1442,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados limpos, validados e enriquecidos",
      "C": "Dados de backup",
      "D": "Dados agregados"
    },
    "correctAnswer": "B",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1443,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados de backup",
      "C": "Dados agregados e transformados para relatórios de negócio",
      "D": "Dados de teste"
    },
    "correctAnswer": "C",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1444,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Em todas as camadas",
      "B": "Na camada Gold",
      "C": "Na camada Bronze",
      "D": "Na camada Silver após limpeza inicial"
    },
    "correctAnswer": "D",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1445,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Armazenar em tabela separada de quarentena",
      "B": "Rejeitar toda a ingestão",
      "C": "Mover para Silver",
      "D": "Descartar imediatamente"
    },
    "correctAnswer": "A",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1446,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Backup de dados",
      "B": "Dados agregados para relatórios",
      "C": "Armazenar dados brutos e não processados",
      "D": "Dados limpos e validados"
    },
    "correctAnswer": "C",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1447,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados",
      "B": "Dados brutos",
      "C": "Dados limpos, validados e enriquecidos",
      "D": "Dados de backup"
    },
    "correctAnswer": "C",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1448,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados de teste",
      "B": "Dados brutos",
      "C": "Dados de backup",
      "D": "Dados agregados e transformados para relatórios de negócio"
    },
    "correctAnswer": "D",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1449,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Bronze",
      "B": "Na camada Silver após limpeza inicial",
      "C": "Em todas as camadas",
      "D": "Na camada Gold"
    },
    "correctAnswer": "B",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1450,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Armazenar em tabela separada de quarentena",
      "B": "Descartar imediatamente",
      "C": "Mover para Silver",
      "D": "Rejeitar toda a ingestão"
    },
    "correctAnswer": "A",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1451,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados para relatórios",
      "B": "Armazenar dados brutos e não processados",
      "C": "Dados limpos e validados",
      "D": "Backup de dados"
    },
    "correctAnswer": "B",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1452,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados agregados",
      "C": "Dados limpos, validados e enriquecidos",
      "D": "Dados de backup"
    },
    "correctAnswer": "C",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1453,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados agregados e transformados para relatórios de negócio",
      "C": "Dados de teste",
      "D": "Dados brutos"
    },
    "correctAnswer": "B",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1454,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Silver após limpeza inicial",
      "B": "Em todas as camadas",
      "C": "Na camada Bronze",
      "D": "Na camada Gold"
    },
    "correctAnswer": "A",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1455,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Armazenar em tabela separada de quarentena",
      "B": "Mover para Silver",
      "C": "Descartar imediatamente",
      "D": "Rejeitar toda a ingestão"
    },
    "correctAnswer": "A",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1456,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos e validados",
      "B": "Armazenar dados brutos e não processados",
      "C": "Dados agregados para relatórios",
      "D": "Backup de dados"
    },
    "correctAnswer": "B",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1457,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos, validados e enriquecidos",
      "B": "Dados agregados",
      "C": "Dados de backup",
      "D": "Dados brutos"
    },
    "correctAnswer": "A",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1458,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados de backup",
      "C": "Dados agregados e transformados para relatórios de negócio",
      "D": "Dados de teste"
    },
    "correctAnswer": "C",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1459,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Silver após limpeza inicial",
      "B": "Em todas as camadas",
      "C": "Na camada Gold",
      "D": "Na camada Bronze"
    },
    "correctAnswer": "A",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1460,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Descartar imediatamente",
      "B": "Armazenar em tabela separada de quarentena",
      "C": "Rejeitar toda a ingestão",
      "D": "Mover para Silver"
    },
    "correctAnswer": "B",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1461,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados para relatórios",
      "B": "Armazenar dados brutos e não processados",
      "C": "Dados limpos e validados",
      "D": "Backup de dados"
    },
    "correctAnswer": "B",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1462,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados",
      "B": "Dados de backup",
      "C": "Dados limpos, validados e enriquecidos",
      "D": "Dados brutos"
    },
    "correctAnswer": "C",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1463,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados de teste",
      "B": "Dados brutos",
      "C": "Dados de backup",
      "D": "Dados agregados e transformados para relatórios de negócio"
    },
    "correctAnswer": "D",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1464,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Gold",
      "B": "Na camada Bronze",
      "C": "Em todas as camadas",
      "D": "Na camada Silver após limpeza inicial"
    },
    "correctAnswer": "D",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1465,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Rejeitar toda a ingestão",
      "B": "Armazenar em tabela separada de quarentena",
      "C": "Mover para Silver",
      "D": "Descartar imediatamente"
    },
    "correctAnswer": "B",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1466,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Backup de dados",
      "B": "Dados limpos e validados",
      "C": "Armazenar dados brutos e não processados",
      "D": "Dados agregados para relatórios"
    },
    "correctAnswer": "C",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1467,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados de backup",
      "C": "Dados limpos, validados e enriquecidos",
      "D": "Dados agregados"
    },
    "correctAnswer": "C",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1468,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados agregados e transformados para relatórios de negócio",
      "C": "Dados de backup",
      "D": "Dados de teste"
    },
    "correctAnswer": "B",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1469,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Bronze",
      "B": "Na camada Gold",
      "C": "Na camada Silver após limpeza inicial",
      "D": "Em todas as camadas"
    },
    "correctAnswer": "C",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1470,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Descartar imediatamente",
      "B": "Rejeitar toda a ingestão",
      "C": "Armazenar em tabela separada de quarentena",
      "D": "Mover para Silver"
    },
    "correctAnswer": "C",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1471,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados para relatórios",
      "B": "Armazenar dados brutos e não processados",
      "C": "Backup de dados",
      "D": "Dados limpos e validados"
    },
    "correctAnswer": "B",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1472,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos, validados e enriquecidos",
      "B": "Dados brutos",
      "C": "Dados agregados",
      "D": "Dados de backup"
    },
    "correctAnswer": "A",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1473,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados e transformados para relatórios de negócio",
      "B": "Dados de teste",
      "C": "Dados de backup",
      "D": "Dados brutos"
    },
    "correctAnswer": "A",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1474,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Em todas as camadas",
      "B": "Na camada Silver após limpeza inicial",
      "C": "Na camada Gold",
      "D": "Na camada Bronze"
    },
    "correctAnswer": "B",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1475,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Mover para Silver",
      "B": "Rejeitar toda a ingestão",
      "C": "Armazenar em tabela separada de quarentena",
      "D": "Descartar imediatamente"
    },
    "correctAnswer": "C",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1476,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados para relatórios",
      "B": "Armazenar dados brutos e não processados",
      "C": "Backup de dados",
      "D": "Dados limpos e validados"
    },
    "correctAnswer": "B",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1477,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados limpos, validados e enriquecidos",
      "C": "Dados agregados",
      "D": "Dados brutos"
    },
    "correctAnswer": "B",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1478,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados e transformados para relatórios de negócio",
      "B": "Dados brutos",
      "C": "Dados de teste",
      "D": "Dados de backup"
    },
    "correctAnswer": "A",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1479,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Gold",
      "B": "Na camada Silver após limpeza inicial",
      "C": "Na camada Bronze",
      "D": "Em todas as camadas"
    },
    "correctAnswer": "B",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1480,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Armazenar em tabela separada de quarentena",
      "B": "Mover para Silver",
      "C": "Rejeitar toda a ingestão",
      "D": "Descartar imediatamente"
    },
    "correctAnswer": "A",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1481,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Backup de dados",
      "B": "Armazenar dados brutos e não processados",
      "C": "Dados agregados para relatórios",
      "D": "Dados limpos e validados"
    },
    "correctAnswer": "B",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1482,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos, validados e enriquecidos",
      "B": "Dados agregados",
      "C": "Dados brutos",
      "D": "Dados de backup"
    },
    "correctAnswer": "A",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1483,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados de teste",
      "C": "Dados agregados e transformados para relatórios de negócio",
      "D": "Dados de backup"
    },
    "correctAnswer": "C",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1484,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Bronze",
      "B": "Na camada Gold",
      "C": "Na camada Silver após limpeza inicial",
      "D": "Em todas as camadas"
    },
    "correctAnswer": "C",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1485,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Descartar imediatamente",
      "B": "Mover para Silver",
      "C": "Rejeitar toda a ingestão",
      "D": "Armazenar em tabela separada de quarentena"
    },
    "correctAnswer": "D",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1486,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos e validados",
      "B": "Backup de dados",
      "C": "Armazenar dados brutos e não processados",
      "D": "Dados agregados para relatórios"
    },
    "correctAnswer": "C",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1487,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos, validados e enriquecidos",
      "B": "Dados de backup",
      "C": "Dados agregados",
      "D": "Dados brutos"
    },
    "correctAnswer": "A",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1488,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados de backup",
      "C": "Dados de teste",
      "D": "Dados agregados e transformados para relatórios de negócio"
    },
    "correctAnswer": "D",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1489,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Silver após limpeza inicial",
      "B": "Na camada Gold",
      "C": "Na camada Bronze",
      "D": "Em todas as camadas"
    },
    "correctAnswer": "A",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1490,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Descartar imediatamente",
      "B": "Mover para Silver",
      "C": "Armazenar em tabela separada de quarentena",
      "D": "Rejeitar toda a ingestão"
    },
    "correctAnswer": "C",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1491,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Armazenar dados brutos e não processados",
      "B": "Dados agregados para relatórios",
      "C": "Backup de dados",
      "D": "Dados limpos e validados"
    },
    "correctAnswer": "A",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1492,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados de backup",
      "C": "Dados agregados",
      "D": "Dados limpos, validados e enriquecidos"
    },
    "correctAnswer": "D",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1493,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados agregados e transformados para relatórios de negócio",
      "C": "Dados de backup",
      "D": "Dados de teste"
    },
    "correctAnswer": "B",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1494,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Bronze",
      "B": "Na camada Gold",
      "C": "Na camada Silver após limpeza inicial",
      "D": "Em todas as camadas"
    },
    "correctAnswer": "C",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1495,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Descartar imediatamente",
      "B": "Rejeitar toda a ingestão",
      "C": "Armazenar em tabela separada de quarentena",
      "D": "Mover para Silver"
    },
    "correctAnswer": "C",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1496,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Armazenar dados brutos e não processados",
      "B": "Dados agregados para relatórios",
      "C": "Backup de dados",
      "D": "Dados limpos e validados"
    },
    "correctAnswer": "A",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1497,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos, validados e enriquecidos",
      "B": "Dados de backup",
      "C": "Dados brutos",
      "D": "Dados agregados"
    },
    "correctAnswer": "A",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1498,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados de teste",
      "B": "Dados de backup",
      "C": "Dados agregados e transformados para relatórios de negócio",
      "D": "Dados brutos"
    },
    "correctAnswer": "C",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1499,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Silver após limpeza inicial",
      "B": "Na camada Gold",
      "C": "Em todas as camadas",
      "D": "Na camada Bronze"
    },
    "correctAnswer": "A",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1500,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Mover para Silver",
      "B": "Armazenar em tabela separada de quarentena",
      "C": "Rejeitar toda a ingestão",
      "D": "Descartar imediatamente"
    },
    "correctAnswer": "B",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1501,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados para relatórios",
      "B": "Backup de dados",
      "C": "Armazenar dados brutos e não processados",
      "D": "Dados limpos e validados"
    },
    "correctAnswer": "C",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1502,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados limpos, validados e enriquecidos",
      "C": "Dados de backup",
      "D": "Dados agregados"
    },
    "correctAnswer": "B",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1503,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados de teste",
      "C": "Dados agregados e transformados para relatórios de negócio",
      "D": "Dados de backup"
    },
    "correctAnswer": "C",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1504,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Bronze",
      "B": "Em todas as camadas",
      "C": "Na camada Gold",
      "D": "Na camada Silver após limpeza inicial"
    },
    "correctAnswer": "D",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1505,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Armazenar em tabela separada de quarentena",
      "B": "Mover para Silver",
      "C": "Rejeitar toda a ingestão",
      "D": "Descartar imediatamente"
    },
    "correctAnswer": "A",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1506,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Backup de dados",
      "B": "Dados agregados para relatórios",
      "C": "Dados limpos e validados",
      "D": "Armazenar dados brutos e não processados"
    },
    "correctAnswer": "D",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1507,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos, validados e enriquecidos",
      "B": "Dados de backup",
      "C": "Dados brutos",
      "D": "Dados agregados"
    },
    "correctAnswer": "A",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1508,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados e transformados para relatórios de negócio",
      "B": "Dados de teste",
      "C": "Dados de backup",
      "D": "Dados brutos"
    },
    "correctAnswer": "A",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1509,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Em todas as camadas",
      "B": "Na camada Gold",
      "C": "Na camada Silver após limpeza inicial",
      "D": "Na camada Bronze"
    },
    "correctAnswer": "C",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1510,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Armazenar em tabela separada de quarentena",
      "B": "Descartar imediatamente",
      "C": "Mover para Silver",
      "D": "Rejeitar toda a ingestão"
    },
    "correctAnswer": "A",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1511,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados para relatórios",
      "B": "Armazenar dados brutos e não processados",
      "C": "Dados limpos e validados",
      "D": "Backup de dados"
    },
    "correctAnswer": "B",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1512,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos, validados e enriquecidos",
      "B": "Dados brutos",
      "C": "Dados de backup",
      "D": "Dados agregados"
    },
    "correctAnswer": "A",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1513,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados de teste",
      "B": "Dados brutos",
      "C": "Dados agregados e transformados para relatórios de negócio",
      "D": "Dados de backup"
    },
    "correctAnswer": "C",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1514,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Silver após limpeza inicial",
      "B": "Na camada Gold",
      "C": "Em todas as camadas",
      "D": "Na camada Bronze"
    },
    "correctAnswer": "A",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1515,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Armazenar em tabela separada de quarentena",
      "B": "Mover para Silver",
      "C": "Descartar imediatamente",
      "D": "Rejeitar toda a ingestão"
    },
    "correctAnswer": "A",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1516,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Armazenar dados brutos e não processados",
      "B": "Dados agregados para relatórios",
      "C": "Dados limpos e validados",
      "D": "Backup de dados"
    },
    "correctAnswer": "A",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1517,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados agregados",
      "C": "Dados brutos",
      "D": "Dados limpos, validados e enriquecidos"
    },
    "correctAnswer": "D",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1518,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados de teste",
      "B": "Dados brutos",
      "C": "Dados de backup",
      "D": "Dados agregados e transformados para relatórios de negócio"
    },
    "correctAnswer": "D",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1519,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Gold",
      "B": "Na camada Silver após limpeza inicial",
      "C": "Na camada Bronze",
      "D": "Em todas as camadas"
    },
    "correctAnswer": "B",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1520,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Armazenar em tabela separada de quarentena",
      "B": "Descartar imediatamente",
      "C": "Rejeitar toda a ingestão",
      "D": "Mover para Silver"
    },
    "correctAnswer": "A",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1521,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Backup de dados",
      "B": "Dados limpos e validados",
      "C": "Dados agregados para relatórios",
      "D": "Armazenar dados brutos e não processados"
    },
    "correctAnswer": "D",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1522,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados limpos, validados e enriquecidos",
      "C": "Dados de backup",
      "D": "Dados agregados"
    },
    "correctAnswer": "B",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1523,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados brutos",
      "C": "Dados de teste",
      "D": "Dados agregados e transformados para relatórios de negócio"
    },
    "correctAnswer": "D",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1524,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Bronze",
      "B": "Na camada Silver após limpeza inicial",
      "C": "Em todas as camadas",
      "D": "Na camada Gold"
    },
    "correctAnswer": "B",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1525,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Armazenar em tabela separada de quarentena",
      "B": "Mover para Silver",
      "C": "Rejeitar toda a ingestão",
      "D": "Descartar imediatamente"
    },
    "correctAnswer": "A",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1526,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Backup de dados",
      "B": "Dados limpos e validados",
      "C": "Dados agregados para relatórios",
      "D": "Armazenar dados brutos e não processados"
    },
    "correctAnswer": "D",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1527,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados",
      "B": "Dados limpos, validados e enriquecidos",
      "C": "Dados brutos",
      "D": "Dados de backup"
    },
    "correctAnswer": "B",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1528,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados agregados e transformados para relatórios de negócio",
      "C": "Dados de teste",
      "D": "Dados de backup"
    },
    "correctAnswer": "B",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1529,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Gold",
      "B": "Na camada Bronze",
      "C": "Na camada Silver após limpeza inicial",
      "D": "Em todas as camadas"
    },
    "correctAnswer": "C",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1530,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Rejeitar toda a ingestão",
      "B": "Armazenar em tabela separada de quarentena",
      "C": "Mover para Silver",
      "D": "Descartar imediatamente"
    },
    "correctAnswer": "B",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1531,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos e validados",
      "B": "Dados agregados para relatórios",
      "C": "Armazenar dados brutos e não processados",
      "D": "Backup de dados"
    },
    "correctAnswer": "C",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1532,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados",
      "B": "Dados brutos",
      "C": "Dados limpos, validados e enriquecidos",
      "D": "Dados de backup"
    },
    "correctAnswer": "C",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1533,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados agregados e transformados para relatórios de negócio",
      "C": "Dados brutos",
      "D": "Dados de teste"
    },
    "correctAnswer": "B",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1534,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Gold",
      "B": "Na camada Silver após limpeza inicial",
      "C": "Em todas as camadas",
      "D": "Na camada Bronze"
    },
    "correctAnswer": "B",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1535,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Descartar imediatamente",
      "B": "Mover para Silver",
      "C": "Rejeitar toda a ingestão",
      "D": "Armazenar em tabela separada de quarentena"
    },
    "correctAnswer": "D",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1536,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados para relatórios",
      "B": "Armazenar dados brutos e não processados",
      "C": "Backup de dados",
      "D": "Dados limpos e validados"
    },
    "correctAnswer": "B",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1537,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados",
      "B": "Dados brutos",
      "C": "Dados de backup",
      "D": "Dados limpos, validados e enriquecidos"
    },
    "correctAnswer": "D",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1538,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados e transformados para relatórios de negócio",
      "B": "Dados de backup",
      "C": "Dados brutos",
      "D": "Dados de teste"
    },
    "correctAnswer": "A",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1539,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Em todas as camadas",
      "B": "Na camada Silver após limpeza inicial",
      "C": "Na camada Bronze",
      "D": "Na camada Gold"
    },
    "correctAnswer": "B",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1540,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Armazenar em tabela separada de quarentena",
      "B": "Mover para Silver",
      "C": "Rejeitar toda a ingestão",
      "D": "Descartar imediatamente"
    },
    "correctAnswer": "A",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1541,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos e validados",
      "B": "Dados agregados para relatórios",
      "C": "Armazenar dados brutos e não processados",
      "D": "Backup de dados"
    },
    "correctAnswer": "C",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1542,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados de backup",
      "C": "Dados agregados",
      "D": "Dados limpos, validados e enriquecidos"
    },
    "correctAnswer": "D",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1543,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados de teste",
      "C": "Dados brutos",
      "D": "Dados agregados e transformados para relatórios de negócio"
    },
    "correctAnswer": "D",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1544,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Em todas as camadas",
      "B": "Na camada Gold",
      "C": "Na camada Silver após limpeza inicial",
      "D": "Na camada Bronze"
    },
    "correctAnswer": "C",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1545,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Mover para Silver",
      "B": "Descartar imediatamente",
      "C": "Armazenar em tabela separada de quarentena",
      "D": "Rejeitar toda a ingestão"
    },
    "correctAnswer": "C",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1546,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados para relatórios",
      "B": "Backup de dados",
      "C": "Armazenar dados brutos e não processados",
      "D": "Dados limpos e validados"
    },
    "correctAnswer": "C",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1547,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados limpos, validados e enriquecidos",
      "C": "Dados de backup",
      "D": "Dados agregados"
    },
    "correctAnswer": "B",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1548,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados de backup",
      "C": "Dados de teste",
      "D": "Dados agregados e transformados para relatórios de negócio"
    },
    "correctAnswer": "D",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1549,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Silver após limpeza inicial",
      "B": "Em todas as camadas",
      "C": "Na camada Gold",
      "D": "Na camada Bronze"
    },
    "correctAnswer": "A",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1550,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Descartar imediatamente",
      "B": "Armazenar em tabela separada de quarentena",
      "C": "Mover para Silver",
      "D": "Rejeitar toda a ingestão"
    },
    "correctAnswer": "B",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1551,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos e validados",
      "B": "Armazenar dados brutos e não processados",
      "C": "Dados agregados para relatórios",
      "D": "Backup de dados"
    },
    "correctAnswer": "B",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1552,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados agregados",
      "C": "Dados brutos",
      "D": "Dados limpos, validados e enriquecidos"
    },
    "correctAnswer": "D",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1553,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados e transformados para relatórios de negócio",
      "B": "Dados de backup",
      "C": "Dados brutos",
      "D": "Dados de teste"
    },
    "correctAnswer": "A",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1554,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Bronze",
      "B": "Na camada Gold",
      "C": "Na camada Silver após limpeza inicial",
      "D": "Em todas as camadas"
    },
    "correctAnswer": "C",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1555,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Mover para Silver",
      "B": "Rejeitar toda a ingestão",
      "C": "Armazenar em tabela separada de quarentena",
      "D": "Descartar imediatamente"
    },
    "correctAnswer": "C",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1556,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Armazenar dados brutos e não processados",
      "B": "Backup de dados",
      "C": "Dados agregados para relatórios",
      "D": "Dados limpos e validados"
    },
    "correctAnswer": "A",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1557,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados de backup",
      "C": "Dados limpos, validados e enriquecidos",
      "D": "Dados agregados"
    },
    "correctAnswer": "C",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1558,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados e transformados para relatórios de negócio",
      "B": "Dados brutos",
      "C": "Dados de backup",
      "D": "Dados de teste"
    },
    "correctAnswer": "A",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1559,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Silver após limpeza inicial",
      "B": "Em todas as camadas",
      "C": "Na camada Gold",
      "D": "Na camada Bronze"
    },
    "correctAnswer": "A",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1560,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Rejeitar toda a ingestão",
      "B": "Armazenar em tabela separada de quarentena",
      "C": "Mover para Silver",
      "D": "Descartar imediatamente"
    },
    "correctAnswer": "B",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1561,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados para relatórios",
      "B": "Dados limpos e validados",
      "C": "Backup de dados",
      "D": "Armazenar dados brutos e não processados"
    },
    "correctAnswer": "D",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1562,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados",
      "B": "Dados de backup",
      "C": "Dados brutos",
      "D": "Dados limpos, validados e enriquecidos"
    },
    "correctAnswer": "D",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1563,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados e transformados para relatórios de negócio",
      "B": "Dados brutos",
      "C": "Dados de backup",
      "D": "Dados de teste"
    },
    "correctAnswer": "A",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1564,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Em todas as camadas",
      "B": "Na camada Gold",
      "C": "Na camada Bronze",
      "D": "Na camada Silver após limpeza inicial"
    },
    "correctAnswer": "D",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1565,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Armazenar em tabela separada de quarentena",
      "B": "Rejeitar toda a ingestão",
      "C": "Descartar imediatamente",
      "D": "Mover para Silver"
    },
    "correctAnswer": "A",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1566,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Backup de dados",
      "B": "Armazenar dados brutos e não processados",
      "C": "Dados limpos e validados",
      "D": "Dados agregados para relatórios"
    },
    "correctAnswer": "B",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1567,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados agregados",
      "C": "Dados brutos",
      "D": "Dados limpos, validados e enriquecidos"
    },
    "correctAnswer": "D",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1568,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados agregados e transformados para relatórios de negócio",
      "C": "Dados brutos",
      "D": "Dados de teste"
    },
    "correctAnswer": "B",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1569,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Em todas as camadas",
      "B": "Na camada Bronze",
      "C": "Na camada Gold",
      "D": "Na camada Silver após limpeza inicial"
    },
    "correctAnswer": "D",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1570,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Descartar imediatamente",
      "B": "Armazenar em tabela separada de quarentena",
      "C": "Rejeitar toda a ingestão",
      "D": "Mover para Silver"
    },
    "correctAnswer": "B",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1571,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Armazenar dados brutos e não processados",
      "B": "Backup de dados",
      "C": "Dados agregados para relatórios",
      "D": "Dados limpos e validados"
    },
    "correctAnswer": "A",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1572,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados agregados",
      "C": "Dados de backup",
      "D": "Dados limpos, validados e enriquecidos"
    },
    "correctAnswer": "D",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1573,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados e transformados para relatórios de negócio",
      "B": "Dados de backup",
      "C": "Dados brutos",
      "D": "Dados de teste"
    },
    "correctAnswer": "A",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1574,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Em todas as camadas",
      "B": "Na camada Gold",
      "C": "Na camada Silver após limpeza inicial",
      "D": "Na camada Bronze"
    },
    "correctAnswer": "C",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1575,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Descartar imediatamente",
      "B": "Armazenar em tabela separada de quarentena",
      "C": "Mover para Silver",
      "D": "Rejeitar toda a ingestão"
    },
    "correctAnswer": "B",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1576,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos e validados",
      "B": "Backup de dados",
      "C": "Dados agregados para relatórios",
      "D": "Armazenar dados brutos e não processados"
    },
    "correctAnswer": "D",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1577,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados de backup",
      "C": "Dados agregados",
      "D": "Dados limpos, validados e enriquecidos"
    },
    "correctAnswer": "D",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1578,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados brutos",
      "C": "Dados de teste",
      "D": "Dados agregados e transformados para relatórios de negócio"
    },
    "correctAnswer": "D",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1579,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Silver após limpeza inicial",
      "B": "Na camada Gold",
      "C": "Em todas as camadas",
      "D": "Na camada Bronze"
    },
    "correctAnswer": "A",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1580,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Mover para Silver",
      "B": "Armazenar em tabela separada de quarentena",
      "C": "Rejeitar toda a ingestão",
      "D": "Descartar imediatamente"
    },
    "correctAnswer": "B",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1581,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados para relatórios",
      "B": "Dados limpos e validados",
      "C": "Backup de dados",
      "D": "Armazenar dados brutos e não processados"
    },
    "correctAnswer": "D",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1582,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos, validados e enriquecidos",
      "B": "Dados de backup",
      "C": "Dados brutos",
      "D": "Dados agregados"
    },
    "correctAnswer": "A",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1583,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados de teste",
      "C": "Dados agregados e transformados para relatórios de negócio",
      "D": "Dados de backup"
    },
    "correctAnswer": "C",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1584,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Em todas as camadas",
      "B": "Na camada Bronze",
      "C": "Na camada Gold",
      "D": "Na camada Silver após limpeza inicial"
    },
    "correctAnswer": "D",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1585,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Rejeitar toda a ingestão",
      "B": "Descartar imediatamente",
      "C": "Mover para Silver",
      "D": "Armazenar em tabela separada de quarentena"
    },
    "correctAnswer": "D",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1586,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Armazenar dados brutos e não processados",
      "B": "Dados limpos e validados",
      "C": "Dados agregados para relatórios",
      "D": "Backup de dados"
    },
    "correctAnswer": "A",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1587,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados",
      "B": "Dados brutos",
      "C": "Dados de backup",
      "D": "Dados limpos, validados e enriquecidos"
    },
    "correctAnswer": "D",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1588,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados de teste",
      "C": "Dados agregados e transformados para relatórios de negócio",
      "D": "Dados brutos"
    },
    "correctAnswer": "C",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1589,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Silver após limpeza inicial",
      "B": "Em todas as camadas",
      "C": "Na camada Gold",
      "D": "Na camada Bronze"
    },
    "correctAnswer": "A",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1590,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Rejeitar toda a ingestão",
      "B": "Armazenar em tabela separada de quarentena",
      "C": "Mover para Silver",
      "D": "Descartar imediatamente"
    },
    "correctAnswer": "B",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1591,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Backup de dados",
      "B": "Dados limpos e validados",
      "C": "Armazenar dados brutos e não processados",
      "D": "Dados agregados para relatórios"
    },
    "correctAnswer": "C",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1592,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos, validados e enriquecidos",
      "B": "Dados de backup",
      "C": "Dados agregados",
      "D": "Dados brutos"
    },
    "correctAnswer": "A",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1593,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados de teste",
      "B": "Dados de backup",
      "C": "Dados brutos",
      "D": "Dados agregados e transformados para relatórios de negócio"
    },
    "correctAnswer": "D",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1594,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Gold",
      "B": "Na camada Bronze",
      "C": "Na camada Silver após limpeza inicial",
      "D": "Em todas as camadas"
    },
    "correctAnswer": "C",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1595,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Rejeitar toda a ingestão",
      "B": "Armazenar em tabela separada de quarentena",
      "C": "Descartar imediatamente",
      "D": "Mover para Silver"
    },
    "correctAnswer": "B",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1596,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Armazenar dados brutos e não processados",
      "B": "Dados agregados para relatórios",
      "C": "Dados limpos e validados",
      "D": "Backup de dados"
    },
    "correctAnswer": "A",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1597,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados agregados",
      "C": "Dados limpos, validados e enriquecidos",
      "D": "Dados brutos"
    },
    "correctAnswer": "C",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1598,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados brutos",
      "C": "Dados agregados e transformados para relatórios de negócio",
      "D": "Dados de teste"
    },
    "correctAnswer": "C",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1599,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Silver após limpeza inicial",
      "B": "Em todas as camadas",
      "C": "Na camada Gold",
      "D": "Na camada Bronze"
    },
    "correctAnswer": "A",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1600,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Armazenar em tabela separada de quarentena",
      "B": "Rejeitar toda a ingestão",
      "C": "Descartar imediatamente",
      "D": "Mover para Silver"
    },
    "correctAnswer": "A",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1601,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Armazenar dados brutos e não processados",
      "B": "Dados limpos e validados",
      "C": "Backup de dados",
      "D": "Dados agregados para relatórios"
    },
    "correctAnswer": "A",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1602,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados agregados",
      "C": "Dados de backup",
      "D": "Dados limpos, validados e enriquecidos"
    },
    "correctAnswer": "D",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1603,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados de teste",
      "C": "Dados agregados e transformados para relatórios de negócio",
      "D": "Dados brutos"
    },
    "correctAnswer": "C",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1604,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Gold",
      "B": "Em todas as camadas",
      "C": "Na camada Silver após limpeza inicial",
      "D": "Na camada Bronze"
    },
    "correctAnswer": "C",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1605,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Descartar imediatamente",
      "B": "Armazenar em tabela separada de quarentena",
      "C": "Rejeitar toda a ingestão",
      "D": "Mover para Silver"
    },
    "correctAnswer": "B",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1606,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos e validados",
      "B": "Armazenar dados brutos e não processados",
      "C": "Dados agregados para relatórios",
      "D": "Backup de dados"
    },
    "correctAnswer": "B",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1607,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados de backup",
      "C": "Dados agregados",
      "D": "Dados limpos, validados e enriquecidos"
    },
    "correctAnswer": "D",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1608,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados e transformados para relatórios de negócio",
      "B": "Dados de backup",
      "C": "Dados brutos",
      "D": "Dados de teste"
    },
    "correctAnswer": "A",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1609,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Em todas as camadas",
      "B": "Na camada Gold",
      "C": "Na camada Silver após limpeza inicial",
      "D": "Na camada Bronze"
    },
    "correctAnswer": "C",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1610,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Descartar imediatamente",
      "B": "Mover para Silver",
      "C": "Armazenar em tabela separada de quarentena",
      "D": "Rejeitar toda a ingestão"
    },
    "correctAnswer": "C",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1611,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados para relatórios",
      "B": "Dados limpos e validados",
      "C": "Backup de dados",
      "D": "Armazenar dados brutos e não processados"
    },
    "correctAnswer": "D",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1612,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados",
      "B": "Dados limpos, validados e enriquecidos",
      "C": "Dados brutos",
      "D": "Dados de backup"
    },
    "correctAnswer": "B",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1613,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados brutos",
      "C": "Dados agregados e transformados para relatórios de negócio",
      "D": "Dados de teste"
    },
    "correctAnswer": "C",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1614,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Silver após limpeza inicial",
      "B": "Na camada Gold",
      "C": "Em todas as camadas",
      "D": "Na camada Bronze"
    },
    "correctAnswer": "A",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1615,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Armazenar em tabela separada de quarentena",
      "B": "Descartar imediatamente",
      "C": "Mover para Silver",
      "D": "Rejeitar toda a ingestão"
    },
    "correctAnswer": "A",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1616,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados para relatórios",
      "B": "Armazenar dados brutos e não processados",
      "C": "Dados limpos e validados",
      "D": "Backup de dados"
    },
    "correctAnswer": "B",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1617,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados",
      "B": "Dados brutos",
      "C": "Dados de backup",
      "D": "Dados limpos, validados e enriquecidos"
    },
    "correctAnswer": "D",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1618,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados e transformados para relatórios de negócio",
      "B": "Dados brutos",
      "C": "Dados de backup",
      "D": "Dados de teste"
    },
    "correctAnswer": "A",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1619,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Gold",
      "B": "Na camada Silver após limpeza inicial",
      "C": "Na camada Bronze",
      "D": "Em todas as camadas"
    },
    "correctAnswer": "B",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1620,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Descartar imediatamente",
      "B": "Rejeitar toda a ingestão",
      "C": "Armazenar em tabela separada de quarentena",
      "D": "Mover para Silver"
    },
    "correctAnswer": "C",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1621,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados para relatórios",
      "B": "Armazenar dados brutos e não processados",
      "C": "Dados limpos e validados",
      "D": "Backup de dados"
    },
    "correctAnswer": "B",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1622,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados brutos",
      "C": "Dados limpos, validados e enriquecidos",
      "D": "Dados agregados"
    },
    "correctAnswer": "C",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1623,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados brutos",
      "C": "Dados de teste",
      "D": "Dados agregados e transformados para relatórios de negócio"
    },
    "correctAnswer": "D",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1624,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Silver após limpeza inicial",
      "B": "Na camada Gold",
      "C": "Na camada Bronze",
      "D": "Em todas as camadas"
    },
    "correctAnswer": "A",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1625,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Armazenar em tabela separada de quarentena",
      "B": "Rejeitar toda a ingestão",
      "C": "Mover para Silver",
      "D": "Descartar imediatamente"
    },
    "correctAnswer": "A",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1626,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos e validados",
      "B": "Backup de dados",
      "C": "Dados agregados para relatórios",
      "D": "Armazenar dados brutos e não processados"
    },
    "correctAnswer": "D",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1627,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados",
      "B": "Dados limpos, validados e enriquecidos",
      "C": "Dados brutos",
      "D": "Dados de backup"
    },
    "correctAnswer": "B",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1628,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados de teste",
      "C": "Dados agregados e transformados para relatórios de negócio",
      "D": "Dados de backup"
    },
    "correctAnswer": "C",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1629,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Silver após limpeza inicial",
      "B": "Em todas as camadas",
      "C": "Na camada Bronze",
      "D": "Na camada Gold"
    },
    "correctAnswer": "A",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1630,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Mover para Silver",
      "B": "Armazenar em tabela separada de quarentena",
      "C": "Descartar imediatamente",
      "D": "Rejeitar toda a ingestão"
    },
    "correctAnswer": "B",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1631,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos e validados",
      "B": "Dados agregados para relatórios",
      "C": "Armazenar dados brutos e não processados",
      "D": "Backup de dados"
    },
    "correctAnswer": "C",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1632,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados",
      "B": "Dados limpos, validados e enriquecidos",
      "C": "Dados de backup",
      "D": "Dados brutos"
    },
    "correctAnswer": "B",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1633,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados e transformados para relatórios de negócio",
      "B": "Dados de backup",
      "C": "Dados brutos",
      "D": "Dados de teste"
    },
    "correctAnswer": "A",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1634,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Bronze",
      "B": "Em todas as camadas",
      "C": "Na camada Gold",
      "D": "Na camada Silver após limpeza inicial"
    },
    "correctAnswer": "D",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1635,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Descartar imediatamente",
      "B": "Rejeitar toda a ingestão",
      "C": "Mover para Silver",
      "D": "Armazenar em tabela separada de quarentena"
    },
    "correctAnswer": "D",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1636,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Backup de dados",
      "B": "Dados limpos e validados",
      "C": "Dados agregados para relatórios",
      "D": "Armazenar dados brutos e não processados"
    },
    "correctAnswer": "D",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1637,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos, validados e enriquecidos",
      "B": "Dados brutos",
      "C": "Dados de backup",
      "D": "Dados agregados"
    },
    "correctAnswer": "A",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1638,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados de teste",
      "C": "Dados brutos",
      "D": "Dados agregados e transformados para relatórios de negócio"
    },
    "correctAnswer": "D",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1639,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Bronze",
      "B": "Na camada Silver após limpeza inicial",
      "C": "Em todas as camadas",
      "D": "Na camada Gold"
    },
    "correctAnswer": "B",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1640,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Descartar imediatamente",
      "B": "Rejeitar toda a ingestão",
      "C": "Armazenar em tabela separada de quarentena",
      "D": "Mover para Silver"
    },
    "correctAnswer": "C",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1641,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Backup de dados",
      "B": "Dados agregados para relatórios",
      "C": "Armazenar dados brutos e não processados",
      "D": "Dados limpos e validados"
    },
    "correctAnswer": "C",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1642,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos, validados e enriquecidos",
      "B": "Dados agregados",
      "C": "Dados brutos",
      "D": "Dados de backup"
    },
    "correctAnswer": "A",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1643,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados e transformados para relatórios de negócio",
      "B": "Dados de teste",
      "C": "Dados de backup",
      "D": "Dados brutos"
    },
    "correctAnswer": "A",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1644,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Bronze",
      "B": "Na camada Gold",
      "C": "Em todas as camadas",
      "D": "Na camada Silver após limpeza inicial"
    },
    "correctAnswer": "D",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1645,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Rejeitar toda a ingestão",
      "B": "Armazenar em tabela separada de quarentena",
      "C": "Mover para Silver",
      "D": "Descartar imediatamente"
    },
    "correctAnswer": "B",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1646,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados para relatórios",
      "B": "Dados limpos e validados",
      "C": "Armazenar dados brutos e não processados",
      "D": "Backup de dados"
    },
    "correctAnswer": "C",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1647,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos, validados e enriquecidos",
      "B": "Dados agregados",
      "C": "Dados de backup",
      "D": "Dados brutos"
    },
    "correctAnswer": "A",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1648,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados de backup",
      "C": "Dados de teste",
      "D": "Dados agregados e transformados para relatórios de negócio"
    },
    "correctAnswer": "D",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1649,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Em todas as camadas",
      "B": "Na camada Bronze",
      "C": "Na camada Gold",
      "D": "Na camada Silver após limpeza inicial"
    },
    "correctAnswer": "D",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1650,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Armazenar em tabela separada de quarentena",
      "B": "Descartar imediatamente",
      "C": "Rejeitar toda a ingestão",
      "D": "Mover para Silver"
    },
    "correctAnswer": "A",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1651,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Armazenar dados brutos e não processados",
      "B": "Dados agregados para relatórios",
      "C": "Backup de dados",
      "D": "Dados limpos e validados"
    },
    "correctAnswer": "A",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1652,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados de backup",
      "C": "Dados agregados",
      "D": "Dados limpos, validados e enriquecidos"
    },
    "correctAnswer": "D",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1653,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados de backup",
      "C": "Dados de teste",
      "D": "Dados agregados e transformados para relatórios de negócio"
    },
    "correctAnswer": "D",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1654,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Bronze",
      "B": "Em todas as camadas",
      "C": "Na camada Silver após limpeza inicial",
      "D": "Na camada Gold"
    },
    "correctAnswer": "C",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1655,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Descartar imediatamente",
      "B": "Rejeitar toda a ingestão",
      "C": "Mover para Silver",
      "D": "Armazenar em tabela separada de quarentena"
    },
    "correctAnswer": "D",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1656,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos e validados",
      "B": "Armazenar dados brutos e não processados",
      "C": "Dados agregados para relatórios",
      "D": "Backup de dados"
    },
    "correctAnswer": "B",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1657,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos, validados e enriquecidos",
      "B": "Dados agregados",
      "C": "Dados brutos",
      "D": "Dados de backup"
    },
    "correctAnswer": "A",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1658,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados e transformados para relatórios de negócio",
      "B": "Dados de backup",
      "C": "Dados de teste",
      "D": "Dados brutos"
    },
    "correctAnswer": "A",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1659,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Silver após limpeza inicial",
      "B": "Na camada Gold",
      "C": "Em todas as camadas",
      "D": "Na camada Bronze"
    },
    "correctAnswer": "A",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1660,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Descartar imediatamente",
      "B": "Rejeitar toda a ingestão",
      "C": "Armazenar em tabela separada de quarentena",
      "D": "Mover para Silver"
    },
    "correctAnswer": "C",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1661,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados para relatórios",
      "B": "Backup de dados",
      "C": "Dados limpos e validados",
      "D": "Armazenar dados brutos e não processados"
    },
    "correctAnswer": "D",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1662,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados",
      "B": "Dados de backup",
      "C": "Dados limpos, validados e enriquecidos",
      "D": "Dados brutos"
    },
    "correctAnswer": "C",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1663,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados e transformados para relatórios de negócio",
      "B": "Dados brutos",
      "C": "Dados de backup",
      "D": "Dados de teste"
    },
    "correctAnswer": "A",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1664,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Gold",
      "B": "Na camada Bronze",
      "C": "Em todas as camadas",
      "D": "Na camada Silver após limpeza inicial"
    },
    "correctAnswer": "D",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1665,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Rejeitar toda a ingestão",
      "B": "Mover para Silver",
      "C": "Descartar imediatamente",
      "D": "Armazenar em tabela separada de quarentena"
    },
    "correctAnswer": "D",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1666,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados para relatórios",
      "B": "Dados limpos e validados",
      "C": "Backup de dados",
      "D": "Armazenar dados brutos e não processados"
    },
    "correctAnswer": "D",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1667,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos, validados e enriquecidos",
      "B": "Dados brutos",
      "C": "Dados de backup",
      "D": "Dados agregados"
    },
    "correctAnswer": "A",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1668,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados brutos",
      "C": "Dados agregados e transformados para relatórios de negócio",
      "D": "Dados de teste"
    },
    "correctAnswer": "C",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1669,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Em todas as camadas",
      "B": "Na camada Bronze",
      "C": "Na camada Gold",
      "D": "Na camada Silver após limpeza inicial"
    },
    "correctAnswer": "D",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1670,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Armazenar em tabela separada de quarentena",
      "B": "Rejeitar toda a ingestão",
      "C": "Mover para Silver",
      "D": "Descartar imediatamente"
    },
    "correctAnswer": "A",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1671,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados para relatórios",
      "B": "Backup de dados",
      "C": "Armazenar dados brutos e não processados",
      "D": "Dados limpos e validados"
    },
    "correctAnswer": "C",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1672,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados limpos, validados e enriquecidos",
      "C": "Dados brutos",
      "D": "Dados agregados"
    },
    "correctAnswer": "B",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1673,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados de teste",
      "B": "Dados agregados e transformados para relatórios de negócio",
      "C": "Dados brutos",
      "D": "Dados de backup"
    },
    "correctAnswer": "B",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1674,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Gold",
      "B": "Em todas as camadas",
      "C": "Na camada Silver após limpeza inicial",
      "D": "Na camada Bronze"
    },
    "correctAnswer": "C",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1675,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Armazenar em tabela separada de quarentena",
      "B": "Rejeitar toda a ingestão",
      "C": "Mover para Silver",
      "D": "Descartar imediatamente"
    },
    "correctAnswer": "A",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1676,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados para relatórios",
      "B": "Armazenar dados brutos e não processados",
      "C": "Backup de dados",
      "D": "Dados limpos e validados"
    },
    "correctAnswer": "B",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1677,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados agregados",
      "C": "Dados de backup",
      "D": "Dados limpos, validados e enriquecidos"
    },
    "correctAnswer": "D",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1678,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados de teste",
      "B": "Dados brutos",
      "C": "Dados agregados e transformados para relatórios de negócio",
      "D": "Dados de backup"
    },
    "correctAnswer": "C",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1679,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Silver após limpeza inicial",
      "B": "Em todas as camadas",
      "C": "Na camada Gold",
      "D": "Na camada Bronze"
    },
    "correctAnswer": "A",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1680,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Armazenar em tabela separada de quarentena",
      "B": "Descartar imediatamente",
      "C": "Rejeitar toda a ingestão",
      "D": "Mover para Silver"
    },
    "correctAnswer": "A",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1681,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos e validados",
      "B": "Dados agregados para relatórios",
      "C": "Backup de dados",
      "D": "Armazenar dados brutos e não processados"
    },
    "correctAnswer": "D",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1682,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados agregados",
      "C": "Dados brutos",
      "D": "Dados limpos, validados e enriquecidos"
    },
    "correctAnswer": "D",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1683,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados de teste",
      "C": "Dados brutos",
      "D": "Dados agregados e transformados para relatórios de negócio"
    },
    "correctAnswer": "D",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1684,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Silver após limpeza inicial",
      "B": "Na camada Bronze",
      "C": "Em todas as camadas",
      "D": "Na camada Gold"
    },
    "correctAnswer": "A",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1685,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Armazenar em tabela separada de quarentena",
      "B": "Rejeitar toda a ingestão",
      "C": "Mover para Silver",
      "D": "Descartar imediatamente"
    },
    "correctAnswer": "A",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1686,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados para relatórios",
      "B": "Dados limpos e validados",
      "C": "Armazenar dados brutos e não processados",
      "D": "Backup de dados"
    },
    "correctAnswer": "C",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1687,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos, validados e enriquecidos",
      "B": "Dados agregados",
      "C": "Dados brutos",
      "D": "Dados de backup"
    },
    "correctAnswer": "A",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1688,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados agregados e transformados para relatórios de negócio",
      "C": "Dados de teste",
      "D": "Dados brutos"
    },
    "correctAnswer": "B",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1689,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Silver após limpeza inicial",
      "B": "Em todas as camadas",
      "C": "Na camada Gold",
      "D": "Na camada Bronze"
    },
    "correctAnswer": "A",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1690,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Rejeitar toda a ingestão",
      "B": "Descartar imediatamente",
      "C": "Mover para Silver",
      "D": "Armazenar em tabela separada de quarentena"
    },
    "correctAnswer": "D",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1691,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos e validados",
      "B": "Armazenar dados brutos e não processados",
      "C": "Backup de dados",
      "D": "Dados agregados para relatórios"
    },
    "correctAnswer": "B",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1692,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados limpos, validados e enriquecidos",
      "C": "Dados agregados",
      "D": "Dados de backup"
    },
    "correctAnswer": "B",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1693,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados de backup",
      "C": "Dados agregados e transformados para relatórios de negócio",
      "D": "Dados de teste"
    },
    "correctAnswer": "C",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1694,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Bronze",
      "B": "Na camada Gold",
      "C": "Na camada Silver após limpeza inicial",
      "D": "Em todas as camadas"
    },
    "correctAnswer": "C",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1695,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Armazenar em tabela separada de quarentena",
      "B": "Mover para Silver",
      "C": "Rejeitar toda a ingestão",
      "D": "Descartar imediatamente"
    },
    "correctAnswer": "A",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1696,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Armazenar dados brutos e não processados",
      "B": "Backup de dados",
      "C": "Dados limpos e validados",
      "D": "Dados agregados para relatórios"
    },
    "correctAnswer": "A",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1697,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados agregados",
      "C": "Dados de backup",
      "D": "Dados limpos, validados e enriquecidos"
    },
    "correctAnswer": "D",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1698,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados de teste",
      "B": "Dados agregados e transformados para relatórios de negócio",
      "C": "Dados brutos",
      "D": "Dados de backup"
    },
    "correctAnswer": "B",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1699,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Em todas as camadas",
      "B": "Na camada Bronze",
      "C": "Na camada Gold",
      "D": "Na camada Silver após limpeza inicial"
    },
    "correctAnswer": "D",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1700,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Armazenar em tabela separada de quarentena",
      "B": "Descartar imediatamente",
      "C": "Mover para Silver",
      "D": "Rejeitar toda a ingestão"
    },
    "correctAnswer": "A",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1701,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados para relatórios",
      "B": "Dados limpos e validados",
      "C": "Backup de dados",
      "D": "Armazenar dados brutos e não processados"
    },
    "correctAnswer": "D",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1702,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados de backup",
      "C": "Dados limpos, validados e enriquecidos",
      "D": "Dados agregados"
    },
    "correctAnswer": "C",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1703,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados de teste",
      "C": "Dados de backup",
      "D": "Dados agregados e transformados para relatórios de negócio"
    },
    "correctAnswer": "D",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1704,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Em todas as camadas",
      "B": "Na camada Silver após limpeza inicial",
      "C": "Na camada Bronze",
      "D": "Na camada Gold"
    },
    "correctAnswer": "B",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1705,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Descartar imediatamente",
      "B": "Rejeitar toda a ingestão",
      "C": "Mover para Silver",
      "D": "Armazenar em tabela separada de quarentena"
    },
    "correctAnswer": "D",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1706,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Armazenar dados brutos e não processados",
      "B": "Backup de dados",
      "C": "Dados agregados para relatórios",
      "D": "Dados limpos e validados"
    },
    "correctAnswer": "A",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1707,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados",
      "B": "Dados brutos",
      "C": "Dados de backup",
      "D": "Dados limpos, validados e enriquecidos"
    },
    "correctAnswer": "D",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1708,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados de teste",
      "C": "Dados agregados e transformados para relatórios de negócio",
      "D": "Dados brutos"
    },
    "correctAnswer": "C",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1709,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Bronze",
      "B": "Na camada Silver após limpeza inicial",
      "C": "Na camada Gold",
      "D": "Em todas as camadas"
    },
    "correctAnswer": "B",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1710,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Descartar imediatamente",
      "B": "Rejeitar toda a ingestão",
      "C": "Mover para Silver",
      "D": "Armazenar em tabela separada de quarentena"
    },
    "correctAnswer": "D",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1711,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Backup de dados",
      "B": "Armazenar dados brutos e não processados",
      "C": "Dados limpos e validados",
      "D": "Dados agregados para relatórios"
    },
    "correctAnswer": "B",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1712,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados",
      "B": "Dados limpos, validados e enriquecidos",
      "C": "Dados de backup",
      "D": "Dados brutos"
    },
    "correctAnswer": "B",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1713,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados de teste",
      "B": "Dados agregados e transformados para relatórios de negócio",
      "C": "Dados brutos",
      "D": "Dados de backup"
    },
    "correctAnswer": "B",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1714,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Silver após limpeza inicial",
      "B": "Em todas as camadas",
      "C": "Na camada Bronze",
      "D": "Na camada Gold"
    },
    "correctAnswer": "A",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1715,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Mover para Silver",
      "B": "Descartar imediatamente",
      "C": "Rejeitar toda a ingestão",
      "D": "Armazenar em tabela separada de quarentena"
    },
    "correctAnswer": "D",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1716,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Armazenar dados brutos e não processados",
      "B": "Dados limpos e validados",
      "C": "Dados agregados para relatórios",
      "D": "Backup de dados"
    },
    "correctAnswer": "A",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1717,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados de backup",
      "C": "Dados agregados",
      "D": "Dados limpos, validados e enriquecidos"
    },
    "correctAnswer": "D",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1718,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados e transformados para relatórios de negócio",
      "B": "Dados de backup",
      "C": "Dados de teste",
      "D": "Dados brutos"
    },
    "correctAnswer": "A",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1719,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Gold",
      "B": "Na camada Silver após limpeza inicial",
      "C": "Na camada Bronze",
      "D": "Em todas as camadas"
    },
    "correctAnswer": "B",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1720,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Rejeitar toda a ingestão",
      "B": "Armazenar em tabela separada de quarentena",
      "C": "Descartar imediatamente",
      "D": "Mover para Silver"
    },
    "correctAnswer": "B",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1721,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Backup de dados",
      "B": "Dados agregados para relatórios",
      "C": "Dados limpos e validados",
      "D": "Armazenar dados brutos e não processados"
    },
    "correctAnswer": "D",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1722,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados limpos, validados e enriquecidos",
      "C": "Dados agregados",
      "D": "Dados brutos"
    },
    "correctAnswer": "B",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1723,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados agregados e transformados para relatórios de negócio",
      "C": "Dados brutos",
      "D": "Dados de teste"
    },
    "correctAnswer": "B",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1724,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Em todas as camadas",
      "B": "Na camada Gold",
      "C": "Na camada Bronze",
      "D": "Na camada Silver após limpeza inicial"
    },
    "correctAnswer": "D",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1725,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Rejeitar toda a ingestão",
      "B": "Mover para Silver",
      "C": "Descartar imediatamente",
      "D": "Armazenar em tabela separada de quarentena"
    },
    "correctAnswer": "D",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1726,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados para relatórios",
      "B": "Dados limpos e validados",
      "C": "Backup de dados",
      "D": "Armazenar dados brutos e não processados"
    },
    "correctAnswer": "D",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1727,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos, validados e enriquecidos",
      "B": "Dados brutos",
      "C": "Dados de backup",
      "D": "Dados agregados"
    },
    "correctAnswer": "A",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1728,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados e transformados para relatórios de negócio",
      "B": "Dados brutos",
      "C": "Dados de teste",
      "D": "Dados de backup"
    },
    "correctAnswer": "A",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1729,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Em todas as camadas",
      "B": "Na camada Silver após limpeza inicial",
      "C": "Na camada Bronze",
      "D": "Na camada Gold"
    },
    "correctAnswer": "B",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1730,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Armazenar em tabela separada de quarentena",
      "B": "Mover para Silver",
      "C": "Descartar imediatamente",
      "D": "Rejeitar toda a ingestão"
    },
    "correctAnswer": "A",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1731,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos e validados",
      "B": "Armazenar dados brutos e não processados",
      "C": "Dados agregados para relatórios",
      "D": "Backup de dados"
    },
    "correctAnswer": "B",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1732,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados limpos, validados e enriquecidos",
      "C": "Dados brutos",
      "D": "Dados agregados"
    },
    "correctAnswer": "B",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1733,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados agregados e transformados para relatórios de negócio",
      "C": "Dados de backup",
      "D": "Dados de teste"
    },
    "correctAnswer": "B",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1734,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Silver após limpeza inicial",
      "B": "Na camada Gold",
      "C": "Na camada Bronze",
      "D": "Em todas as camadas"
    },
    "correctAnswer": "A",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1735,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Armazenar em tabela separada de quarentena",
      "B": "Mover para Silver",
      "C": "Descartar imediatamente",
      "D": "Rejeitar toda a ingestão"
    },
    "correctAnswer": "A",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1736,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos e validados",
      "B": "Backup de dados",
      "C": "Armazenar dados brutos e não processados",
      "D": "Dados agregados para relatórios"
    },
    "correctAnswer": "C",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1737,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados agregados",
      "C": "Dados limpos, validados e enriquecidos",
      "D": "Dados de backup"
    },
    "correctAnswer": "C",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1738,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados e transformados para relatórios de negócio",
      "B": "Dados brutos",
      "C": "Dados de backup",
      "D": "Dados de teste"
    },
    "correctAnswer": "A",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1739,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Em todas as camadas",
      "B": "Na camada Gold",
      "C": "Na camada Silver após limpeza inicial",
      "D": "Na camada Bronze"
    },
    "correctAnswer": "C",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1740,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Mover para Silver",
      "B": "Armazenar em tabela separada de quarentena",
      "C": "Descartar imediatamente",
      "D": "Rejeitar toda a ingestão"
    },
    "correctAnswer": "B",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1741,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Armazenar dados brutos e não processados",
      "B": "Dados limpos e validados",
      "C": "Backup de dados",
      "D": "Dados agregados para relatórios"
    },
    "correctAnswer": "A",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1742,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados brutos",
      "C": "Dados agregados",
      "D": "Dados limpos, validados e enriquecidos"
    },
    "correctAnswer": "D",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1743,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados de teste",
      "B": "Dados de backup",
      "C": "Dados agregados e transformados para relatórios de negócio",
      "D": "Dados brutos"
    },
    "correctAnswer": "C",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1744,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Bronze",
      "B": "Em todas as camadas",
      "C": "Na camada Gold",
      "D": "Na camada Silver após limpeza inicial"
    },
    "correctAnswer": "D",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1745,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Descartar imediatamente",
      "B": "Mover para Silver",
      "C": "Armazenar em tabela separada de quarentena",
      "D": "Rejeitar toda a ingestão"
    },
    "correctAnswer": "C",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1746,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos e validados",
      "B": "Backup de dados",
      "C": "Armazenar dados brutos e não processados",
      "D": "Dados agregados para relatórios"
    },
    "correctAnswer": "C",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1747,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados limpos, validados e enriquecidos",
      "C": "Dados brutos",
      "D": "Dados agregados"
    },
    "correctAnswer": "B",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1748,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados brutos",
      "C": "Dados agregados e transformados para relatórios de negócio",
      "D": "Dados de teste"
    },
    "correctAnswer": "C",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1749,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Gold",
      "B": "Na camada Bronze",
      "C": "Na camada Silver após limpeza inicial",
      "D": "Em todas as camadas"
    },
    "correctAnswer": "C",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1750,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Mover para Silver",
      "B": "Descartar imediatamente",
      "C": "Armazenar em tabela separada de quarentena",
      "D": "Rejeitar toda a ingestão"
    },
    "correctAnswer": "C",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1751,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Armazenar dados brutos e não processados",
      "B": "Dados limpos e validados",
      "C": "Backup de dados",
      "D": "Dados agregados para relatórios"
    },
    "correctAnswer": "A",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1752,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados",
      "B": "Dados limpos, validados e enriquecidos",
      "C": "Dados brutos",
      "D": "Dados de backup"
    },
    "correctAnswer": "B",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1753,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados brutos",
      "C": "Dados de teste",
      "D": "Dados agregados e transformados para relatórios de negócio"
    },
    "correctAnswer": "D",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1754,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Silver após limpeza inicial",
      "B": "Na camada Bronze",
      "C": "Na camada Gold",
      "D": "Em todas as camadas"
    },
    "correctAnswer": "A",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1755,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Mover para Silver",
      "B": "Armazenar em tabela separada de quarentena",
      "C": "Descartar imediatamente",
      "D": "Rejeitar toda a ingestão"
    },
    "correctAnswer": "B",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1756,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Backup de dados",
      "B": "Dados limpos e validados",
      "C": "Armazenar dados brutos e não processados",
      "D": "Dados agregados para relatórios"
    },
    "correctAnswer": "C",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1757,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados de backup",
      "C": "Dados limpos, validados e enriquecidos",
      "D": "Dados agregados"
    },
    "correctAnswer": "C",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1758,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados e transformados para relatórios de negócio",
      "B": "Dados de backup",
      "C": "Dados brutos",
      "D": "Dados de teste"
    },
    "correctAnswer": "A",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1759,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Em todas as camadas",
      "B": "Na camada Gold",
      "C": "Na camada Silver após limpeza inicial",
      "D": "Na camada Bronze"
    },
    "correctAnswer": "C",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1760,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Armazenar em tabela separada de quarentena",
      "B": "Descartar imediatamente",
      "C": "Mover para Silver",
      "D": "Rejeitar toda a ingestão"
    },
    "correctAnswer": "A",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1761,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Backup de dados",
      "B": "Dados limpos e validados",
      "C": "Dados agregados para relatórios",
      "D": "Armazenar dados brutos e não processados"
    },
    "correctAnswer": "D",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1762,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados limpos, validados e enriquecidos",
      "C": "Dados de backup",
      "D": "Dados agregados"
    },
    "correctAnswer": "B",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1763,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados de teste",
      "B": "Dados agregados e transformados para relatórios de negócio",
      "C": "Dados brutos",
      "D": "Dados de backup"
    },
    "correctAnswer": "B",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1764,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Bronze",
      "B": "Em todas as camadas",
      "C": "Na camada Silver após limpeza inicial",
      "D": "Na camada Gold"
    },
    "correctAnswer": "C",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1765,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Mover para Silver",
      "B": "Descartar imediatamente",
      "C": "Rejeitar toda a ingestão",
      "D": "Armazenar em tabela separada de quarentena"
    },
    "correctAnswer": "D",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1766,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados para relatórios",
      "B": "Armazenar dados brutos e não processados",
      "C": "Backup de dados",
      "D": "Dados limpos e validados"
    },
    "correctAnswer": "B",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1767,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos, validados e enriquecidos",
      "B": "Dados agregados",
      "C": "Dados de backup",
      "D": "Dados brutos"
    },
    "correctAnswer": "A",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1768,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados agregados e transformados para relatórios de negócio",
      "C": "Dados brutos",
      "D": "Dados de teste"
    },
    "correctAnswer": "B",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1769,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Silver após limpeza inicial",
      "B": "Na camada Bronze",
      "C": "Na camada Gold",
      "D": "Em todas as camadas"
    },
    "correctAnswer": "A",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1770,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Armazenar em tabela separada de quarentena",
      "B": "Descartar imediatamente",
      "C": "Mover para Silver",
      "D": "Rejeitar toda a ingestão"
    },
    "correctAnswer": "A",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1771,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Armazenar dados brutos e não processados",
      "B": "Backup de dados",
      "C": "Dados agregados para relatórios",
      "D": "Dados limpos e validados"
    },
    "correctAnswer": "A",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1772,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados limpos, validados e enriquecidos",
      "C": "Dados agregados",
      "D": "Dados brutos"
    },
    "correctAnswer": "B",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1773,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados e transformados para relatórios de negócio",
      "B": "Dados de teste",
      "C": "Dados brutos",
      "D": "Dados de backup"
    },
    "correctAnswer": "A",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1774,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Bronze",
      "B": "Na camada Silver após limpeza inicial",
      "C": "Na camada Gold",
      "D": "Em todas as camadas"
    },
    "correctAnswer": "B",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1775,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Descartar imediatamente",
      "B": "Mover para Silver",
      "C": "Armazenar em tabela separada de quarentena",
      "D": "Rejeitar toda a ingestão"
    },
    "correctAnswer": "C",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1776,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados para relatórios",
      "B": "Dados limpos e validados",
      "C": "Armazenar dados brutos e não processados",
      "D": "Backup de dados"
    },
    "correctAnswer": "C",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1777,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados agregados",
      "C": "Dados limpos, validados e enriquecidos",
      "D": "Dados brutos"
    },
    "correctAnswer": "C",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1778,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados de teste",
      "B": "Dados brutos",
      "C": "Dados agregados e transformados para relatórios de negócio",
      "D": "Dados de backup"
    },
    "correctAnswer": "C",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1779,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Bronze",
      "B": "Na camada Gold",
      "C": "Na camada Silver após limpeza inicial",
      "D": "Em todas as camadas"
    },
    "correctAnswer": "C",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1780,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Descartar imediatamente",
      "B": "Mover para Silver",
      "C": "Rejeitar toda a ingestão",
      "D": "Armazenar em tabela separada de quarentena"
    },
    "correctAnswer": "D",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1781,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos e validados",
      "B": "Backup de dados",
      "C": "Armazenar dados brutos e não processados",
      "D": "Dados agregados para relatórios"
    },
    "correctAnswer": "C",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1782,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados limpos, validados e enriquecidos",
      "C": "Dados brutos",
      "D": "Dados agregados"
    },
    "correctAnswer": "B",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1783,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados de backup",
      "C": "Dados de teste",
      "D": "Dados agregados e transformados para relatórios de negócio"
    },
    "correctAnswer": "D",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1784,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Silver após limpeza inicial",
      "B": "Na camada Gold",
      "C": "Na camada Bronze",
      "D": "Em todas as camadas"
    },
    "correctAnswer": "A",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1785,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Rejeitar toda a ingestão",
      "B": "Mover para Silver",
      "C": "Armazenar em tabela separada de quarentena",
      "D": "Descartar imediatamente"
    },
    "correctAnswer": "C",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1786,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Armazenar dados brutos e não processados",
      "B": "Backup de dados",
      "C": "Dados agregados para relatórios",
      "D": "Dados limpos e validados"
    },
    "correctAnswer": "A",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1787,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos, validados e enriquecidos",
      "B": "Dados brutos",
      "C": "Dados agregados",
      "D": "Dados de backup"
    },
    "correctAnswer": "A",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1788,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados de teste",
      "B": "Dados agregados e transformados para relatórios de negócio",
      "C": "Dados de backup",
      "D": "Dados brutos"
    },
    "correctAnswer": "B",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1789,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Em todas as camadas",
      "B": "Na camada Silver após limpeza inicial",
      "C": "Na camada Bronze",
      "D": "Na camada Gold"
    },
    "correctAnswer": "B",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1790,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Armazenar em tabela separada de quarentena",
      "B": "Mover para Silver",
      "C": "Descartar imediatamente",
      "D": "Rejeitar toda a ingestão"
    },
    "correctAnswer": "A",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1791,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados para relatórios",
      "B": "Dados limpos e validados",
      "C": "Backup de dados",
      "D": "Armazenar dados brutos e não processados"
    },
    "correctAnswer": "D",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1792,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados de backup",
      "C": "Dados agregados",
      "D": "Dados limpos, validados e enriquecidos"
    },
    "correctAnswer": "D",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1793,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados de teste",
      "C": "Dados agregados e transformados para relatórios de negócio",
      "D": "Dados de backup"
    },
    "correctAnswer": "C",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1794,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Bronze",
      "B": "Na camada Silver após limpeza inicial",
      "C": "Em todas as camadas",
      "D": "Na camada Gold"
    },
    "correctAnswer": "B",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1795,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Mover para Silver",
      "B": "Rejeitar toda a ingestão",
      "C": "Descartar imediatamente",
      "D": "Armazenar em tabela separada de quarentena"
    },
    "correctAnswer": "D",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1796,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos e validados",
      "B": "Dados agregados para relatórios",
      "C": "Armazenar dados brutos e não processados",
      "D": "Backup de dados"
    },
    "correctAnswer": "C",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1797,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados",
      "B": "Dados limpos, validados e enriquecidos",
      "C": "Dados de backup",
      "D": "Dados brutos"
    },
    "correctAnswer": "B",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1798,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados de teste",
      "B": "Dados de backup",
      "C": "Dados agregados e transformados para relatórios de negócio",
      "D": "Dados brutos"
    },
    "correctAnswer": "C",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1799,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Silver após limpeza inicial",
      "B": "Em todas as camadas",
      "C": "Na camada Bronze",
      "D": "Na camada Gold"
    },
    "correctAnswer": "A",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1800,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Rejeitar toda a ingestão",
      "B": "Armazenar em tabela separada de quarentena",
      "C": "Descartar imediatamente",
      "D": "Mover para Silver"
    },
    "correctAnswer": "B",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1801,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados para relatórios",
      "B": "Dados limpos e validados",
      "C": "Backup de dados",
      "D": "Armazenar dados brutos e não processados"
    },
    "correctAnswer": "D",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1802,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos, validados e enriquecidos",
      "B": "Dados agregados",
      "C": "Dados de backup",
      "D": "Dados brutos"
    },
    "correctAnswer": "A",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1803,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados e transformados para relatórios de negócio",
      "B": "Dados brutos",
      "C": "Dados de teste",
      "D": "Dados de backup"
    },
    "correctAnswer": "A",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1804,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Gold",
      "B": "Em todas as camadas",
      "C": "Na camada Silver após limpeza inicial",
      "D": "Na camada Bronze"
    },
    "correctAnswer": "C",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1805,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Rejeitar toda a ingestão",
      "B": "Armazenar em tabela separada de quarentena",
      "C": "Descartar imediatamente",
      "D": "Mover para Silver"
    },
    "correctAnswer": "B",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1806,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Armazenar dados brutos e não processados",
      "B": "Dados limpos e validados",
      "C": "Backup de dados",
      "D": "Dados agregados para relatórios"
    },
    "correctAnswer": "A",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1807,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos, validados e enriquecidos",
      "B": "Dados brutos",
      "C": "Dados de backup",
      "D": "Dados agregados"
    },
    "correctAnswer": "A",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1808,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados agregados e transformados para relatórios de negócio",
      "C": "Dados brutos",
      "D": "Dados de teste"
    },
    "correctAnswer": "B",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1809,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Bronze",
      "B": "Na camada Gold",
      "C": "Na camada Silver após limpeza inicial",
      "D": "Em todas as camadas"
    },
    "correctAnswer": "C",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1810,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Mover para Silver",
      "B": "Rejeitar toda a ingestão",
      "C": "Descartar imediatamente",
      "D": "Armazenar em tabela separada de quarentena"
    },
    "correctAnswer": "D",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1811,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos e validados",
      "B": "Dados agregados para relatórios",
      "C": "Backup de dados",
      "D": "Armazenar dados brutos e não processados"
    },
    "correctAnswer": "D",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1812,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados de backup",
      "C": "Dados agregados",
      "D": "Dados limpos, validados e enriquecidos"
    },
    "correctAnswer": "D",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1813,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados agregados e transformados para relatórios de negócio",
      "C": "Dados de backup",
      "D": "Dados de teste"
    },
    "correctAnswer": "B",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1814,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Em todas as camadas",
      "B": "Na camada Gold",
      "C": "Na camada Silver após limpeza inicial",
      "D": "Na camada Bronze"
    },
    "correctAnswer": "C",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1815,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Armazenar em tabela separada de quarentena",
      "B": "Descartar imediatamente",
      "C": "Mover para Silver",
      "D": "Rejeitar toda a ingestão"
    },
    "correctAnswer": "A",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1816,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos e validados",
      "B": "Backup de dados",
      "C": "Armazenar dados brutos e não processados",
      "D": "Dados agregados para relatórios"
    },
    "correctAnswer": "C",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1817,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados brutos",
      "C": "Dados agregados",
      "D": "Dados limpos, validados e enriquecidos"
    },
    "correctAnswer": "D",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1818,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados de backup",
      "C": "Dados de teste",
      "D": "Dados agregados e transformados para relatórios de negócio"
    },
    "correctAnswer": "D",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1819,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Em todas as camadas",
      "B": "Na camada Gold",
      "C": "Na camada Silver após limpeza inicial",
      "D": "Na camada Bronze"
    },
    "correctAnswer": "C",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1820,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Mover para Silver",
      "B": "Rejeitar toda a ingestão",
      "C": "Descartar imediatamente",
      "D": "Armazenar em tabela separada de quarentena"
    },
    "correctAnswer": "D",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1821,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados para relatórios",
      "B": "Armazenar dados brutos e não processados",
      "C": "Backup de dados",
      "D": "Dados limpos e validados"
    },
    "correctAnswer": "B",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1822,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados limpos, validados e enriquecidos",
      "C": "Dados agregados",
      "D": "Dados de backup"
    },
    "correctAnswer": "B",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1823,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados e transformados para relatórios de negócio",
      "B": "Dados de backup",
      "C": "Dados brutos",
      "D": "Dados de teste"
    },
    "correctAnswer": "A",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1824,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Silver após limpeza inicial",
      "B": "Na camada Bronze",
      "C": "Na camada Gold",
      "D": "Em todas as camadas"
    },
    "correctAnswer": "A",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1825,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Descartar imediatamente",
      "B": "Armazenar em tabela separada de quarentena",
      "C": "Rejeitar toda a ingestão",
      "D": "Mover para Silver"
    },
    "correctAnswer": "B",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1826,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados para relatórios",
      "B": "Armazenar dados brutos e não processados",
      "C": "Dados limpos e validados",
      "D": "Backup de dados"
    },
    "correctAnswer": "B",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1827,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados",
      "B": "Dados limpos, validados e enriquecidos",
      "C": "Dados de backup",
      "D": "Dados brutos"
    },
    "correctAnswer": "B",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1828,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados de teste",
      "B": "Dados agregados e transformados para relatórios de negócio",
      "C": "Dados de backup",
      "D": "Dados brutos"
    },
    "correctAnswer": "B",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1829,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Silver após limpeza inicial",
      "B": "Em todas as camadas",
      "C": "Na camada Bronze",
      "D": "Na camada Gold"
    },
    "correctAnswer": "A",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1830,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Rejeitar toda a ingestão",
      "B": "Armazenar em tabela separada de quarentena",
      "C": "Mover para Silver",
      "D": "Descartar imediatamente"
    },
    "correctAnswer": "B",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1831,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos e validados",
      "B": "Dados agregados para relatórios",
      "C": "Armazenar dados brutos e não processados",
      "D": "Backup de dados"
    },
    "correctAnswer": "C",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1832,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados brutos",
      "C": "Dados limpos, validados e enriquecidos",
      "D": "Dados agregados"
    },
    "correctAnswer": "C",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1833,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados agregados e transformados para relatórios de negócio",
      "C": "Dados de backup",
      "D": "Dados de teste"
    },
    "correctAnswer": "B",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1834,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Em todas as camadas",
      "B": "Na camada Silver após limpeza inicial",
      "C": "Na camada Bronze",
      "D": "Na camada Gold"
    },
    "correctAnswer": "B",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1835,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Rejeitar toda a ingestão",
      "B": "Armazenar em tabela separada de quarentena",
      "C": "Mover para Silver",
      "D": "Descartar imediatamente"
    },
    "correctAnswer": "B",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1836,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados para relatórios",
      "B": "Dados limpos e validados",
      "C": "Armazenar dados brutos e não processados",
      "D": "Backup de dados"
    },
    "correctAnswer": "C",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1837,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados limpos, validados e enriquecidos",
      "B": "Dados agregados",
      "C": "Dados de backup",
      "D": "Dados brutos"
    },
    "correctAnswer": "A",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1838,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados agregados e transformados para relatórios de negócio",
      "C": "Dados de teste",
      "D": "Dados de backup"
    },
    "correctAnswer": "B",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1839,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Gold",
      "B": "Em todas as camadas",
      "C": "Na camada Silver após limpeza inicial",
      "D": "Na camada Bronze"
    },
    "correctAnswer": "C",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1840,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Mover para Silver",
      "B": "Descartar imediatamente",
      "C": "Rejeitar toda a ingestão",
      "D": "Armazenar em tabela separada de quarentena"
    },
    "correctAnswer": "D",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1841,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Armazenar dados brutos e não processados",
      "B": "Dados limpos e validados",
      "C": "Backup de dados",
      "D": "Dados agregados para relatórios"
    },
    "correctAnswer": "A",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1842,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados de backup",
      "B": "Dados limpos, validados e enriquecidos",
      "C": "Dados brutos",
      "D": "Dados agregados"
    },
    "correctAnswer": "B",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1843,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados e transformados para relatórios de negócio",
      "B": "Dados brutos",
      "C": "Dados de backup",
      "D": "Dados de teste"
    },
    "correctAnswer": "A",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1844,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Bronze",
      "B": "Em todas as camadas",
      "C": "Na camada Gold",
      "D": "Na camada Silver após limpeza inicial"
    },
    "correctAnswer": "D",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1845,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Armazenar em tabela separada de quarentena",
      "B": "Descartar imediatamente",
      "C": "Rejeitar toda a ingestão",
      "D": "Mover para Silver"
    },
    "correctAnswer": "A",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1846,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Backup de dados",
      "B": "Dados limpos e validados",
      "C": "Dados agregados para relatórios",
      "D": "Armazenar dados brutos e não processados"
    },
    "correctAnswer": "D",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1847,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados agregados",
      "C": "Dados limpos, validados e enriquecidos",
      "D": "Dados de backup"
    },
    "correctAnswer": "C",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1848,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados e transformados para relatórios de negócio",
      "B": "Dados de backup",
      "C": "Dados brutos",
      "D": "Dados de teste"
    },
    "correctAnswer": "A",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1849,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Silver após limpeza inicial",
      "B": "Na camada Gold",
      "C": "Em todas as camadas",
      "D": "Na camada Bronze"
    },
    "correctAnswer": "A",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1850,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Rejeitar toda a ingestão",
      "B": "Mover para Silver",
      "C": "Descartar imediatamente",
      "D": "Armazenar em tabela separada de quarentena"
    },
    "correctAnswer": "D",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1851,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Backup de dados",
      "B": "Armazenar dados brutos e não processados",
      "C": "Dados agregados para relatórios",
      "D": "Dados limpos e validados"
    },
    "correctAnswer": "B",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1852,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados",
      "B": "Dados brutos",
      "C": "Dados de backup",
      "D": "Dados limpos, validados e enriquecidos"
    },
    "correctAnswer": "D",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1853,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados de teste",
      "B": "Dados brutos",
      "C": "Dados de backup",
      "D": "Dados agregados e transformados para relatórios de negócio"
    },
    "correctAnswer": "D",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1854,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Silver após limpeza inicial",
      "B": "Na camada Gold",
      "C": "Na camada Bronze",
      "D": "Em todas as camadas"
    },
    "correctAnswer": "A",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1855,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Mover para Silver",
      "B": "Rejeitar toda a ingestão",
      "C": "Armazenar em tabela separada de quarentena",
      "D": "Descartar imediatamente"
    },
    "correctAnswer": "C",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1856,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Bronze na Arquitetura Medallion?",
    "options": {
      "A": "Backup de dados",
      "B": "Dados agregados para relatórios",
      "C": "Dados limpos e validados",
      "D": "Armazenar dados brutos e não processados"
    },
    "correctAnswer": "D",
    "rationale": "A camada Bronze armazena dados brutos, exatamente como chegam da fonte, sem processamento ou transformação.",
    "tip": "Bronze = dados brutos e não processados."
  },
  {
    "id": 1857,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é o propósito da camada Silver na Arquitetura Medallion?",
    "options": {
      "A": "Dados brutos",
      "B": "Dados limpos, validados e enriquecidos",
      "C": "Dados de backup",
      "D": "Dados agregados"
    },
    "correctAnswer": "B",
    "rationale": "A camada Silver contém dados que foram limpos, validados, deduplicated e enriquecidos, prontos para análise.",
    "tip": "Silver = dados prontos para análise."
  },
  {
    "id": 1858,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da camada Gold na Arquitetura Medallion?",
    "options": {
      "A": "Dados agregados e transformados para relatórios de negócio",
      "B": "Dados brutos",
      "C": "Dados de teste",
      "D": "Dados de backup"
    },
    "correctAnswer": "A",
    "rationale": "A camada Gold contém dados agregados, transformados e otimizados para consumo por aplicações de negócio e dashboards.",
    "tip": "Gold = dados prontos para negócio."
  },
  {
    "id": 1859,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em qual camada da Medallion você deve fazer deduplicação de dados?",
    "options": {
      "A": "Na camada Gold",
      "B": "Em todas as camadas",
      "C": "Na camada Silver após limpeza inicial",
      "D": "Na camada Bronze"
    },
    "correctAnswer": "C",
    "rationale": "Deduplicação deve ser feita na camada Silver, após a limpeza inicial, para garantir dados únicos antes de transformações.",
    "tip": "Deduplicação é parte da limpeza de dados na Silver."
  },
  {
    "id": 1860,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Qual é a melhor prática para tratamento de dados inválidos na Bronze?",
    "options": {
      "A": "Armazenar em tabela separada de quarentena",
      "B": "Descartar imediatamente",
      "C": "Mover para Silver",
      "D": "Rejeitar toda a ingestão"
    },
    "correctAnswer": "A",
    "rationale": "Dados inválidos devem ser armazenados em tabela separada para análise e correção posterior, não descartados.",
    "tip": "Quarentena = preservar dados para análise."
  },
  {
    "id": 1861,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "Não há diferença",
      "B": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "C": "Deployment tradicional é mais rápido",
      "D": "DAB é apenas para notebooks"
    },
    "correctAnswer": "B",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 1862,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Arquivo databricks.yml com definições de recursos",
      "B": "Apenas clusters",
      "C": "Apenas notebooks",
      "D": "Apenas permissões"
    },
    "correctAnswer": "A",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 1863,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Reiniciando o cluster",
      "B": "Usando o botão 'Repair and rerun' na UI do Job",
      "C": "Reexecutando manualmente o notebook",
      "D": "Deletando e recriando o Job"
    },
    "correctAnswer": "B",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 1864,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas segurança",
      "B": "Apenas performance",
      "C": "Versionamento, reprodutibilidade e automação de deployment",
      "D": "Apenas documentação"
    },
    "correctAnswer": "C",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 1865,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Usando 'depends_on' na definição de cada tarefa",
      "B": "Não é possível",
      "C": "Através de variáveis de ambiente",
      "D": "Manualmente na UI"
    },
    "correctAnswer": "A",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 1866,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "Deployment tradicional é mais rápido",
      "B": "Não há diferença",
      "C": "DAB é apenas para notebooks",
      "D": "DAB permite Infrastructure as Code; deployment tradicional é manual"
    },
    "correctAnswer": "D",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 1867,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Arquivo databricks.yml com definições de recursos",
      "B": "Apenas permissões",
      "C": "Apenas clusters",
      "D": "Apenas notebooks"
    },
    "correctAnswer": "A",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 1868,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Usando o botão 'Repair and rerun' na UI do Job",
      "B": "Deletando e recriando o Job",
      "C": "Reexecutando manualmente o notebook",
      "D": "Reiniciando o cluster"
    },
    "correctAnswer": "A",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 1869,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas segurança",
      "B": "Apenas documentação",
      "C": "Apenas performance",
      "D": "Versionamento, reprodutibilidade e automação de deployment"
    },
    "correctAnswer": "D",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 1870,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Não é possível",
      "B": "Usando 'depends_on' na definição de cada tarefa",
      "C": "Através de variáveis de ambiente",
      "D": "Manualmente na UI"
    },
    "correctAnswer": "B",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 1871,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "Não há diferença",
      "B": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "C": "Deployment tradicional é mais rápido",
      "D": "DAB é apenas para notebooks"
    },
    "correctAnswer": "B",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 1872,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas clusters",
      "B": "Apenas permissões",
      "C": "Arquivo databricks.yml com definições de recursos",
      "D": "Apenas notebooks"
    },
    "correctAnswer": "C",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 1873,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Reiniciando o cluster",
      "B": "Reexecutando manualmente o notebook",
      "C": "Usando o botão 'Repair and rerun' na UI do Job",
      "D": "Deletando e recriando o Job"
    },
    "correctAnswer": "C",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 1874,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas segurança",
      "B": "Apenas documentação",
      "C": "Versionamento, reprodutibilidade e automação de deployment",
      "D": "Apenas performance"
    },
    "correctAnswer": "C",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 1875,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Não é possível",
      "B": "Através de variáveis de ambiente",
      "C": "Usando 'depends_on' na definição de cada tarefa",
      "D": "Manualmente na UI"
    },
    "correctAnswer": "C",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 1876,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "Deployment tradicional é mais rápido",
      "B": "Não há diferença",
      "C": "DAB é apenas para notebooks",
      "D": "DAB permite Infrastructure as Code; deployment tradicional é manual"
    },
    "correctAnswer": "D",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 1877,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas permissões",
      "B": "Apenas notebooks",
      "C": "Apenas clusters",
      "D": "Arquivo databricks.yml com definições de recursos"
    },
    "correctAnswer": "D",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 1878,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Reiniciando o cluster",
      "B": "Usando o botão 'Repair and rerun' na UI do Job",
      "C": "Reexecutando manualmente o notebook",
      "D": "Deletando e recriando o Job"
    },
    "correctAnswer": "B",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 1879,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas documentação",
      "B": "Versionamento, reprodutibilidade e automação de deployment",
      "C": "Apenas performance",
      "D": "Apenas segurança"
    },
    "correctAnswer": "B",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 1880,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Usando 'depends_on' na definição de cada tarefa",
      "B": "Não é possível",
      "C": "Manualmente na UI",
      "D": "Através de variáveis de ambiente"
    },
    "correctAnswer": "A",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 1881,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "B": "DAB é apenas para notebooks",
      "C": "Não há diferença",
      "D": "Deployment tradicional é mais rápido"
    },
    "correctAnswer": "A",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 1882,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Arquivo databricks.yml com definições de recursos",
      "B": "Apenas permissões",
      "C": "Apenas notebooks",
      "D": "Apenas clusters"
    },
    "correctAnswer": "A",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 1883,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Deletando e recriando o Job",
      "B": "Reiniciando o cluster",
      "C": "Reexecutando manualmente o notebook",
      "D": "Usando o botão 'Repair and rerun' na UI do Job"
    },
    "correctAnswer": "D",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 1884,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Versionamento, reprodutibilidade e automação de deployment",
      "B": "Apenas documentação",
      "C": "Apenas performance",
      "D": "Apenas segurança"
    },
    "correctAnswer": "A",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 1885,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Usando 'depends_on' na definição de cada tarefa",
      "B": "Não é possível",
      "C": "Manualmente na UI",
      "D": "Através de variáveis de ambiente"
    },
    "correctAnswer": "A",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 1886,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "Deployment tradicional é mais rápido",
      "B": "DAB é apenas para notebooks",
      "C": "Não há diferença",
      "D": "DAB permite Infrastructure as Code; deployment tradicional é manual"
    },
    "correctAnswer": "D",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 1887,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas permissões",
      "B": "Apenas notebooks",
      "C": "Apenas clusters",
      "D": "Arquivo databricks.yml com definições de recursos"
    },
    "correctAnswer": "D",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 1888,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Deletando e recriando o Job",
      "B": "Reexecutando manualmente o notebook",
      "C": "Usando o botão 'Repair and rerun' na UI do Job",
      "D": "Reiniciando o cluster"
    },
    "correctAnswer": "C",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 1889,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas documentação",
      "B": "Apenas segurança",
      "C": "Versionamento, reprodutibilidade e automação de deployment",
      "D": "Apenas performance"
    },
    "correctAnswer": "C",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 1890,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Não é possível",
      "B": "Manualmente na UI",
      "C": "Usando 'depends_on' na definição de cada tarefa",
      "D": "Através de variáveis de ambiente"
    },
    "correctAnswer": "C",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 1891,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "Não há diferença",
      "B": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "C": "DAB é apenas para notebooks",
      "D": "Deployment tradicional é mais rápido"
    },
    "correctAnswer": "B",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 1892,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas notebooks",
      "B": "Arquivo databricks.yml com definições de recursos",
      "C": "Apenas clusters",
      "D": "Apenas permissões"
    },
    "correctAnswer": "B",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 1893,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Usando o botão 'Repair and rerun' na UI do Job",
      "B": "Reiniciando o cluster",
      "C": "Deletando e recriando o Job",
      "D": "Reexecutando manualmente o notebook"
    },
    "correctAnswer": "A",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 1894,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas documentação",
      "B": "Versionamento, reprodutibilidade e automação de deployment",
      "C": "Apenas segurança",
      "D": "Apenas performance"
    },
    "correctAnswer": "B",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 1895,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Não é possível",
      "B": "Usando 'depends_on' na definição de cada tarefa",
      "C": "Manualmente na UI",
      "D": "Através de variáveis de ambiente"
    },
    "correctAnswer": "B",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 1896,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "Deployment tradicional é mais rápido",
      "B": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "C": "DAB é apenas para notebooks",
      "D": "Não há diferença"
    },
    "correctAnswer": "B",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 1897,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas clusters",
      "B": "Apenas notebooks",
      "C": "Arquivo databricks.yml com definições de recursos",
      "D": "Apenas permissões"
    },
    "correctAnswer": "C",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 1898,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Reiniciando o cluster",
      "B": "Reexecutando manualmente o notebook",
      "C": "Usando o botão 'Repair and rerun' na UI do Job",
      "D": "Deletando e recriando o Job"
    },
    "correctAnswer": "C",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 1899,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Versionamento, reprodutibilidade e automação de deployment",
      "B": "Apenas segurança",
      "C": "Apenas documentação",
      "D": "Apenas performance"
    },
    "correctAnswer": "A",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 1900,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Usando 'depends_on' na definição de cada tarefa",
      "B": "Manualmente na UI",
      "C": "Através de variáveis de ambiente",
      "D": "Não é possível"
    },
    "correctAnswer": "A",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 1901,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "Deployment tradicional é mais rápido",
      "B": "Não há diferença",
      "C": "DAB é apenas para notebooks",
      "D": "DAB permite Infrastructure as Code; deployment tradicional é manual"
    },
    "correctAnswer": "D",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 1902,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Arquivo databricks.yml com definições de recursos",
      "B": "Apenas permissões",
      "C": "Apenas notebooks",
      "D": "Apenas clusters"
    },
    "correctAnswer": "A",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 1903,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Deletando e recriando o Job",
      "B": "Usando o botão 'Repair and rerun' na UI do Job",
      "C": "Reiniciando o cluster",
      "D": "Reexecutando manualmente o notebook"
    },
    "correctAnswer": "B",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 1904,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas performance",
      "B": "Versionamento, reprodutibilidade e automação de deployment",
      "C": "Apenas segurança",
      "D": "Apenas documentação"
    },
    "correctAnswer": "B",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 1905,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Não é possível",
      "B": "Usando 'depends_on' na definição de cada tarefa",
      "C": "Através de variáveis de ambiente",
      "D": "Manualmente na UI"
    },
    "correctAnswer": "B",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 1906,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "Deployment tradicional é mais rápido",
      "B": "Não há diferença",
      "C": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "D": "DAB é apenas para notebooks"
    },
    "correctAnswer": "C",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 1907,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Arquivo databricks.yml com definições de recursos",
      "B": "Apenas notebooks",
      "C": "Apenas permissões",
      "D": "Apenas clusters"
    },
    "correctAnswer": "A",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 1908,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Reiniciando o cluster",
      "B": "Deletando e recriando o Job",
      "C": "Usando o botão 'Repair and rerun' na UI do Job",
      "D": "Reexecutando manualmente o notebook"
    },
    "correctAnswer": "C",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 1909,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas performance",
      "B": "Apenas segurança",
      "C": "Versionamento, reprodutibilidade e automação de deployment",
      "D": "Apenas documentação"
    },
    "correctAnswer": "C",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 1910,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Manualmente na UI",
      "B": "Não é possível",
      "C": "Através de variáveis de ambiente",
      "D": "Usando 'depends_on' na definição de cada tarefa"
    },
    "correctAnswer": "D",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 1911,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "DAB é apenas para notebooks",
      "B": "Não há diferença",
      "C": "Deployment tradicional é mais rápido",
      "D": "DAB permite Infrastructure as Code; deployment tradicional é manual"
    },
    "correctAnswer": "D",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 1912,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas clusters",
      "B": "Arquivo databricks.yml com definições de recursos",
      "C": "Apenas permissões",
      "D": "Apenas notebooks"
    },
    "correctAnswer": "B",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 1913,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Usando o botão 'Repair and rerun' na UI do Job",
      "B": "Deletando e recriando o Job",
      "C": "Reexecutando manualmente o notebook",
      "D": "Reiniciando o cluster"
    },
    "correctAnswer": "A",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 1914,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas performance",
      "B": "Apenas segurança",
      "C": "Apenas documentação",
      "D": "Versionamento, reprodutibilidade e automação de deployment"
    },
    "correctAnswer": "D",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 1915,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Manualmente na UI",
      "B": "Não é possível",
      "C": "Usando 'depends_on' na definição de cada tarefa",
      "D": "Através de variáveis de ambiente"
    },
    "correctAnswer": "C",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 1916,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "B": "Deployment tradicional é mais rápido",
      "C": "DAB é apenas para notebooks",
      "D": "Não há diferença"
    },
    "correctAnswer": "A",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 1917,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas permissões",
      "B": "Arquivo databricks.yml com definições de recursos",
      "C": "Apenas clusters",
      "D": "Apenas notebooks"
    },
    "correctAnswer": "B",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 1918,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Reiniciando o cluster",
      "B": "Reexecutando manualmente o notebook",
      "C": "Usando o botão 'Repair and rerun' na UI do Job",
      "D": "Deletando e recriando o Job"
    },
    "correctAnswer": "C",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 1919,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas segurança",
      "B": "Apenas documentação",
      "C": "Versionamento, reprodutibilidade e automação de deployment",
      "D": "Apenas performance"
    },
    "correctAnswer": "C",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 1920,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Usando 'depends_on' na definição de cada tarefa",
      "B": "Não é possível",
      "C": "Manualmente na UI",
      "D": "Através de variáveis de ambiente"
    },
    "correctAnswer": "A",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 1921,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "Deployment tradicional é mais rápido",
      "B": "Não há diferença",
      "C": "DAB é apenas para notebooks",
      "D": "DAB permite Infrastructure as Code; deployment tradicional é manual"
    },
    "correctAnswer": "D",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 1922,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas notebooks",
      "B": "Apenas permissões",
      "C": "Apenas clusters",
      "D": "Arquivo databricks.yml com definições de recursos"
    },
    "correctAnswer": "D",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 1923,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Deletando e recriando o Job",
      "B": "Reexecutando manualmente o notebook",
      "C": "Usando o botão 'Repair and rerun' na UI do Job",
      "D": "Reiniciando o cluster"
    },
    "correctAnswer": "C",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 1924,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas documentação",
      "B": "Versionamento, reprodutibilidade e automação de deployment",
      "C": "Apenas segurança",
      "D": "Apenas performance"
    },
    "correctAnswer": "B",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 1925,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Não é possível",
      "B": "Manualmente na UI",
      "C": "Usando 'depends_on' na definição de cada tarefa",
      "D": "Através de variáveis de ambiente"
    },
    "correctAnswer": "C",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 1926,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "DAB é apenas para notebooks",
      "B": "Deployment tradicional é mais rápido",
      "C": "Não há diferença",
      "D": "DAB permite Infrastructure as Code; deployment tradicional é manual"
    },
    "correctAnswer": "D",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 1927,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Arquivo databricks.yml com definições de recursos",
      "B": "Apenas clusters",
      "C": "Apenas notebooks",
      "D": "Apenas permissões"
    },
    "correctAnswer": "A",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 1928,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Reexecutando manualmente o notebook",
      "B": "Deletando e recriando o Job",
      "C": "Reiniciando o cluster",
      "D": "Usando o botão 'Repair and rerun' na UI do Job"
    },
    "correctAnswer": "D",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 1929,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas performance",
      "B": "Versionamento, reprodutibilidade e automação de deployment",
      "C": "Apenas documentação",
      "D": "Apenas segurança"
    },
    "correctAnswer": "B",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 1930,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Não é possível",
      "B": "Usando 'depends_on' na definição de cada tarefa",
      "C": "Através de variáveis de ambiente",
      "D": "Manualmente na UI"
    },
    "correctAnswer": "B",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 1931,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "Deployment tradicional é mais rápido",
      "B": "Não há diferença",
      "C": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "D": "DAB é apenas para notebooks"
    },
    "correctAnswer": "C",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 1932,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas permissões",
      "B": "Apenas clusters",
      "C": "Apenas notebooks",
      "D": "Arquivo databricks.yml com definições de recursos"
    },
    "correctAnswer": "D",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 1933,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Reexecutando manualmente o notebook",
      "B": "Usando o botão 'Repair and rerun' na UI do Job",
      "C": "Deletando e recriando o Job",
      "D": "Reiniciando o cluster"
    },
    "correctAnswer": "B",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 1934,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas performance",
      "B": "Versionamento, reprodutibilidade e automação de deployment",
      "C": "Apenas segurança",
      "D": "Apenas documentação"
    },
    "correctAnswer": "B",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 1935,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Manualmente na UI",
      "B": "Usando 'depends_on' na definição de cada tarefa",
      "C": "Não é possível",
      "D": "Através de variáveis de ambiente"
    },
    "correctAnswer": "B",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 1936,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "B": "Deployment tradicional é mais rápido",
      "C": "DAB é apenas para notebooks",
      "D": "Não há diferença"
    },
    "correctAnswer": "A",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 1937,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Arquivo databricks.yml com definições de recursos",
      "B": "Apenas permissões",
      "C": "Apenas clusters",
      "D": "Apenas notebooks"
    },
    "correctAnswer": "A",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 1938,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Reiniciando o cluster",
      "B": "Deletando e recriando o Job",
      "C": "Reexecutando manualmente o notebook",
      "D": "Usando o botão 'Repair and rerun' na UI do Job"
    },
    "correctAnswer": "D",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 1939,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas performance",
      "B": "Versionamento, reprodutibilidade e automação de deployment",
      "C": "Apenas documentação",
      "D": "Apenas segurança"
    },
    "correctAnswer": "B",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 1940,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Através de variáveis de ambiente",
      "B": "Manualmente na UI",
      "C": "Usando 'depends_on' na definição de cada tarefa",
      "D": "Não é possível"
    },
    "correctAnswer": "C",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 1941,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "DAB é apenas para notebooks",
      "B": "Deployment tradicional é mais rápido",
      "C": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "D": "Não há diferença"
    },
    "correctAnswer": "C",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 1942,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas notebooks",
      "B": "Apenas clusters",
      "C": "Arquivo databricks.yml com definições de recursos",
      "D": "Apenas permissões"
    },
    "correctAnswer": "C",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 1943,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Deletando e recriando o Job",
      "B": "Reexecutando manualmente o notebook",
      "C": "Reiniciando o cluster",
      "D": "Usando o botão 'Repair and rerun' na UI do Job"
    },
    "correctAnswer": "D",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 1944,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas documentação",
      "B": "Versionamento, reprodutibilidade e automação de deployment",
      "C": "Apenas segurança",
      "D": "Apenas performance"
    },
    "correctAnswer": "B",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 1945,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Não é possível",
      "B": "Através de variáveis de ambiente",
      "C": "Usando 'depends_on' na definição de cada tarefa",
      "D": "Manualmente na UI"
    },
    "correctAnswer": "C",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 1946,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "Deployment tradicional é mais rápido",
      "B": "DAB é apenas para notebooks",
      "C": "Não há diferença",
      "D": "DAB permite Infrastructure as Code; deployment tradicional é manual"
    },
    "correctAnswer": "D",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 1947,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Arquivo databricks.yml com definições de recursos",
      "B": "Apenas clusters",
      "C": "Apenas permissões",
      "D": "Apenas notebooks"
    },
    "correctAnswer": "A",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 1948,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Reiniciando o cluster",
      "B": "Deletando e recriando o Job",
      "C": "Reexecutando manualmente o notebook",
      "D": "Usando o botão 'Repair and rerun' na UI do Job"
    },
    "correctAnswer": "D",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 1949,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas segurança",
      "B": "Apenas performance",
      "C": "Versionamento, reprodutibilidade e automação de deployment",
      "D": "Apenas documentação"
    },
    "correctAnswer": "C",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 1950,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Através de variáveis de ambiente",
      "B": "Usando 'depends_on' na definição de cada tarefa",
      "C": "Não é possível",
      "D": "Manualmente na UI"
    },
    "correctAnswer": "B",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 1951,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "Deployment tradicional é mais rápido",
      "B": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "C": "DAB é apenas para notebooks",
      "D": "Não há diferença"
    },
    "correctAnswer": "B",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 1952,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas notebooks",
      "B": "Apenas clusters",
      "C": "Arquivo databricks.yml com definições de recursos",
      "D": "Apenas permissões"
    },
    "correctAnswer": "C",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 1953,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Usando o botão 'Repair and rerun' na UI do Job",
      "B": "Deletando e recriando o Job",
      "C": "Reexecutando manualmente o notebook",
      "D": "Reiniciando o cluster"
    },
    "correctAnswer": "A",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 1954,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Versionamento, reprodutibilidade e automação de deployment",
      "B": "Apenas documentação",
      "C": "Apenas performance",
      "D": "Apenas segurança"
    },
    "correctAnswer": "A",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 1955,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Manualmente na UI",
      "B": "Não é possível",
      "C": "Através de variáveis de ambiente",
      "D": "Usando 'depends_on' na definição de cada tarefa"
    },
    "correctAnswer": "D",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 1956,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "B": "Não há diferença",
      "C": "DAB é apenas para notebooks",
      "D": "Deployment tradicional é mais rápido"
    },
    "correctAnswer": "A",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 1957,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Arquivo databricks.yml com definições de recursos",
      "B": "Apenas permissões",
      "C": "Apenas clusters",
      "D": "Apenas notebooks"
    },
    "correctAnswer": "A",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 1958,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Reiniciando o cluster",
      "B": "Deletando e recriando o Job",
      "C": "Reexecutando manualmente o notebook",
      "D": "Usando o botão 'Repair and rerun' na UI do Job"
    },
    "correctAnswer": "D",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 1959,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas performance",
      "B": "Apenas documentação",
      "C": "Versionamento, reprodutibilidade e automação de deployment",
      "D": "Apenas segurança"
    },
    "correctAnswer": "C",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 1960,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Manualmente na UI",
      "B": "Usando 'depends_on' na definição de cada tarefa",
      "C": "Através de variáveis de ambiente",
      "D": "Não é possível"
    },
    "correctAnswer": "B",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 1961,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "Não há diferença",
      "B": "Deployment tradicional é mais rápido",
      "C": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "D": "DAB é apenas para notebooks"
    },
    "correctAnswer": "C",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 1962,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Arquivo databricks.yml com definições de recursos",
      "B": "Apenas notebooks",
      "C": "Apenas clusters",
      "D": "Apenas permissões"
    },
    "correctAnswer": "A",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 1963,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Deletando e recriando o Job",
      "B": "Usando o botão 'Repair and rerun' na UI do Job",
      "C": "Reexecutando manualmente o notebook",
      "D": "Reiniciando o cluster"
    },
    "correctAnswer": "B",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 1964,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas documentação",
      "B": "Apenas performance",
      "C": "Versionamento, reprodutibilidade e automação de deployment",
      "D": "Apenas segurança"
    },
    "correctAnswer": "C",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 1965,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Através de variáveis de ambiente",
      "B": "Usando 'depends_on' na definição de cada tarefa",
      "C": "Manualmente na UI",
      "D": "Não é possível"
    },
    "correctAnswer": "B",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 1966,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "Não há diferença",
      "B": "DAB é apenas para notebooks",
      "C": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "D": "Deployment tradicional é mais rápido"
    },
    "correctAnswer": "C",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 1967,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Arquivo databricks.yml com definições de recursos",
      "B": "Apenas notebooks",
      "C": "Apenas permissões",
      "D": "Apenas clusters"
    },
    "correctAnswer": "A",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 1968,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Reiniciando o cluster",
      "B": "Reexecutando manualmente o notebook",
      "C": "Usando o botão 'Repair and rerun' na UI do Job",
      "D": "Deletando e recriando o Job"
    },
    "correctAnswer": "C",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 1969,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Versionamento, reprodutibilidade e automação de deployment",
      "B": "Apenas documentação",
      "C": "Apenas segurança",
      "D": "Apenas performance"
    },
    "correctAnswer": "A",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 1970,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Não é possível",
      "B": "Através de variáveis de ambiente",
      "C": "Usando 'depends_on' na definição de cada tarefa",
      "D": "Manualmente na UI"
    },
    "correctAnswer": "C",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 1971,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "Não há diferença",
      "B": "Deployment tradicional é mais rápido",
      "C": "DAB é apenas para notebooks",
      "D": "DAB permite Infrastructure as Code; deployment tradicional é manual"
    },
    "correctAnswer": "D",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 1972,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas clusters",
      "B": "Apenas permissões",
      "C": "Apenas notebooks",
      "D": "Arquivo databricks.yml com definições de recursos"
    },
    "correctAnswer": "D",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 1973,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Reiniciando o cluster",
      "B": "Reexecutando manualmente o notebook",
      "C": "Deletando e recriando o Job",
      "D": "Usando o botão 'Repair and rerun' na UI do Job"
    },
    "correctAnswer": "D",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 1974,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas segurança",
      "B": "Apenas documentação",
      "C": "Versionamento, reprodutibilidade e automação de deployment",
      "D": "Apenas performance"
    },
    "correctAnswer": "C",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 1975,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Manualmente na UI",
      "B": "Não é possível",
      "C": "Usando 'depends_on' na definição de cada tarefa",
      "D": "Através de variáveis de ambiente"
    },
    "correctAnswer": "C",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 1976,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "Não há diferença",
      "B": "Deployment tradicional é mais rápido",
      "C": "DAB é apenas para notebooks",
      "D": "DAB permite Infrastructure as Code; deployment tradicional é manual"
    },
    "correctAnswer": "D",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 1977,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas notebooks",
      "B": "Apenas clusters",
      "C": "Arquivo databricks.yml com definições de recursos",
      "D": "Apenas permissões"
    },
    "correctAnswer": "C",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 1978,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Reexecutando manualmente o notebook",
      "B": "Reiniciando o cluster",
      "C": "Deletando e recriando o Job",
      "D": "Usando o botão 'Repair and rerun' na UI do Job"
    },
    "correctAnswer": "D",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 1979,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Versionamento, reprodutibilidade e automação de deployment",
      "B": "Apenas documentação",
      "C": "Apenas segurança",
      "D": "Apenas performance"
    },
    "correctAnswer": "A",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 1980,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Manualmente na UI",
      "B": "Através de variáveis de ambiente",
      "C": "Usando 'depends_on' na definição de cada tarefa",
      "D": "Não é possível"
    },
    "correctAnswer": "C",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 1981,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "Não há diferença",
      "B": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "C": "Deployment tradicional é mais rápido",
      "D": "DAB é apenas para notebooks"
    },
    "correctAnswer": "B",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 1982,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas permissões",
      "B": "Apenas notebooks",
      "C": "Apenas clusters",
      "D": "Arquivo databricks.yml com definições de recursos"
    },
    "correctAnswer": "D",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 1983,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Usando o botão 'Repair and rerun' na UI do Job",
      "B": "Reiniciando o cluster",
      "C": "Deletando e recriando o Job",
      "D": "Reexecutando manualmente o notebook"
    },
    "correctAnswer": "A",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 1984,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas segurança",
      "B": "Apenas documentação",
      "C": "Versionamento, reprodutibilidade e automação de deployment",
      "D": "Apenas performance"
    },
    "correctAnswer": "C",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 1985,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Através de variáveis de ambiente",
      "B": "Usando 'depends_on' na definição de cada tarefa",
      "C": "Não é possível",
      "D": "Manualmente na UI"
    },
    "correctAnswer": "B",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 1986,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "Não há diferença",
      "B": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "C": "Deployment tradicional é mais rápido",
      "D": "DAB é apenas para notebooks"
    },
    "correctAnswer": "B",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 1987,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas notebooks",
      "B": "Arquivo databricks.yml com definições de recursos",
      "C": "Apenas clusters",
      "D": "Apenas permissões"
    },
    "correctAnswer": "B",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 1988,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Reexecutando manualmente o notebook",
      "B": "Usando o botão 'Repair and rerun' na UI do Job",
      "C": "Deletando e recriando o Job",
      "D": "Reiniciando o cluster"
    },
    "correctAnswer": "B",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 1989,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas performance",
      "B": "Apenas segurança",
      "C": "Apenas documentação",
      "D": "Versionamento, reprodutibilidade e automação de deployment"
    },
    "correctAnswer": "D",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 1990,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Através de variáveis de ambiente",
      "B": "Não é possível",
      "C": "Usando 'depends_on' na definição de cada tarefa",
      "D": "Manualmente na UI"
    },
    "correctAnswer": "C",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 1991,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "Não há diferença",
      "B": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "C": "DAB é apenas para notebooks",
      "D": "Deployment tradicional é mais rápido"
    },
    "correctAnswer": "B",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 1992,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas notebooks",
      "B": "Apenas permissões",
      "C": "Apenas clusters",
      "D": "Arquivo databricks.yml com definições de recursos"
    },
    "correctAnswer": "D",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 1993,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Reexecutando manualmente o notebook",
      "B": "Usando o botão 'Repair and rerun' na UI do Job",
      "C": "Deletando e recriando o Job",
      "D": "Reiniciando o cluster"
    },
    "correctAnswer": "B",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 1994,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Versionamento, reprodutibilidade e automação de deployment",
      "B": "Apenas performance",
      "C": "Apenas documentação",
      "D": "Apenas segurança"
    },
    "correctAnswer": "A",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 1995,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Através de variáveis de ambiente",
      "B": "Manualmente na UI",
      "C": "Usando 'depends_on' na definição de cada tarefa",
      "D": "Não é possível"
    },
    "correctAnswer": "C",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 1996,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "Não há diferença",
      "B": "DAB é apenas para notebooks",
      "C": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "D": "Deployment tradicional é mais rápido"
    },
    "correctAnswer": "C",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 1997,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas clusters",
      "B": "Apenas permissões",
      "C": "Apenas notebooks",
      "D": "Arquivo databricks.yml com definições de recursos"
    },
    "correctAnswer": "D",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 1998,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Reiniciando o cluster",
      "B": "Deletando e recriando o Job",
      "C": "Usando o botão 'Repair and rerun' na UI do Job",
      "D": "Reexecutando manualmente o notebook"
    },
    "correctAnswer": "C",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 1999,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas documentação",
      "B": "Apenas performance",
      "C": "Apenas segurança",
      "D": "Versionamento, reprodutibilidade e automação de deployment"
    },
    "correctAnswer": "D",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2000,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Através de variáveis de ambiente",
      "B": "Usando 'depends_on' na definição de cada tarefa",
      "C": "Manualmente na UI",
      "D": "Não é possível"
    },
    "correctAnswer": "B",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2001,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "DAB é apenas para notebooks",
      "B": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "C": "Deployment tradicional é mais rápido",
      "D": "Não há diferença"
    },
    "correctAnswer": "B",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2002,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas clusters",
      "B": "Apenas notebooks",
      "C": "Arquivo databricks.yml com definições de recursos",
      "D": "Apenas permissões"
    },
    "correctAnswer": "C",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2003,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Deletando e recriando o Job",
      "B": "Usando o botão 'Repair and rerun' na UI do Job",
      "C": "Reiniciando o cluster",
      "D": "Reexecutando manualmente o notebook"
    },
    "correctAnswer": "B",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2004,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas performance",
      "B": "Apenas documentação",
      "C": "Apenas segurança",
      "D": "Versionamento, reprodutibilidade e automação de deployment"
    },
    "correctAnswer": "D",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2005,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Usando 'depends_on' na definição de cada tarefa",
      "B": "Não é possível",
      "C": "Manualmente na UI",
      "D": "Através de variáveis de ambiente"
    },
    "correctAnswer": "A",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2006,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "DAB é apenas para notebooks",
      "B": "Deployment tradicional é mais rápido",
      "C": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "D": "Não há diferença"
    },
    "correctAnswer": "C",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2007,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas permissões",
      "B": "Apenas clusters",
      "C": "Apenas notebooks",
      "D": "Arquivo databricks.yml com definições de recursos"
    },
    "correctAnswer": "D",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2008,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Reexecutando manualmente o notebook",
      "B": "Usando o botão 'Repair and rerun' na UI do Job",
      "C": "Reiniciando o cluster",
      "D": "Deletando e recriando o Job"
    },
    "correctAnswer": "B",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2009,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Versionamento, reprodutibilidade e automação de deployment",
      "B": "Apenas segurança",
      "C": "Apenas documentação",
      "D": "Apenas performance"
    },
    "correctAnswer": "A",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2010,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Usando 'depends_on' na definição de cada tarefa",
      "B": "Através de variáveis de ambiente",
      "C": "Manualmente na UI",
      "D": "Não é possível"
    },
    "correctAnswer": "A",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2011,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "DAB é apenas para notebooks",
      "B": "Deployment tradicional é mais rápido",
      "C": "Não há diferença",
      "D": "DAB permite Infrastructure as Code; deployment tradicional é manual"
    },
    "correctAnswer": "D",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2012,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas notebooks",
      "B": "Apenas clusters",
      "C": "Arquivo databricks.yml com definições de recursos",
      "D": "Apenas permissões"
    },
    "correctAnswer": "C",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2013,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Usando o botão 'Repair and rerun' na UI do Job",
      "B": "Reiniciando o cluster",
      "C": "Reexecutando manualmente o notebook",
      "D": "Deletando e recriando o Job"
    },
    "correctAnswer": "A",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2014,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas performance",
      "B": "Versionamento, reprodutibilidade e automação de deployment",
      "C": "Apenas segurança",
      "D": "Apenas documentação"
    },
    "correctAnswer": "B",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2015,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Não é possível",
      "B": "Através de variáveis de ambiente",
      "C": "Manualmente na UI",
      "D": "Usando 'depends_on' na definição de cada tarefa"
    },
    "correctAnswer": "D",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2016,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "Não há diferença",
      "B": "DAB é apenas para notebooks",
      "C": "Deployment tradicional é mais rápido",
      "D": "DAB permite Infrastructure as Code; deployment tradicional é manual"
    },
    "correctAnswer": "D",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2017,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas notebooks",
      "B": "Apenas permissões",
      "C": "Arquivo databricks.yml com definições de recursos",
      "D": "Apenas clusters"
    },
    "correctAnswer": "C",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2018,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Usando o botão 'Repair and rerun' na UI do Job",
      "B": "Reiniciando o cluster",
      "C": "Deletando e recriando o Job",
      "D": "Reexecutando manualmente o notebook"
    },
    "correctAnswer": "A",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2019,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas segurança",
      "B": "Versionamento, reprodutibilidade e automação de deployment",
      "C": "Apenas documentação",
      "D": "Apenas performance"
    },
    "correctAnswer": "B",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2020,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Através de variáveis de ambiente",
      "B": "Usando 'depends_on' na definição de cada tarefa",
      "C": "Não é possível",
      "D": "Manualmente na UI"
    },
    "correctAnswer": "B",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2021,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "Não há diferença",
      "B": "Deployment tradicional é mais rápido",
      "C": "DAB é apenas para notebooks",
      "D": "DAB permite Infrastructure as Code; deployment tradicional é manual"
    },
    "correctAnswer": "D",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2022,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas clusters",
      "B": "Apenas notebooks",
      "C": "Arquivo databricks.yml com definições de recursos",
      "D": "Apenas permissões"
    },
    "correctAnswer": "C",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2023,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Reiniciando o cluster",
      "B": "Deletando e recriando o Job",
      "C": "Reexecutando manualmente o notebook",
      "D": "Usando o botão 'Repair and rerun' na UI do Job"
    },
    "correctAnswer": "D",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2024,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas performance",
      "B": "Apenas documentação",
      "C": "Apenas segurança",
      "D": "Versionamento, reprodutibilidade e automação de deployment"
    },
    "correctAnswer": "D",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2025,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Manualmente na UI",
      "B": "Através de variáveis de ambiente",
      "C": "Usando 'depends_on' na definição de cada tarefa",
      "D": "Não é possível"
    },
    "correctAnswer": "C",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2026,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "B": "DAB é apenas para notebooks",
      "C": "Deployment tradicional é mais rápido",
      "D": "Não há diferença"
    },
    "correctAnswer": "A",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2027,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas notebooks",
      "B": "Arquivo databricks.yml com definições de recursos",
      "C": "Apenas permissões",
      "D": "Apenas clusters"
    },
    "correctAnswer": "B",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2028,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Usando o botão 'Repair and rerun' na UI do Job",
      "B": "Reiniciando o cluster",
      "C": "Reexecutando manualmente o notebook",
      "D": "Deletando e recriando o Job"
    },
    "correctAnswer": "A",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2029,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas documentação",
      "B": "Versionamento, reprodutibilidade e automação de deployment",
      "C": "Apenas performance",
      "D": "Apenas segurança"
    },
    "correctAnswer": "B",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2030,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Usando 'depends_on' na definição de cada tarefa",
      "B": "Através de variáveis de ambiente",
      "C": "Manualmente na UI",
      "D": "Não é possível"
    },
    "correctAnswer": "A",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2031,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "Não há diferença",
      "B": "DAB é apenas para notebooks",
      "C": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "D": "Deployment tradicional é mais rápido"
    },
    "correctAnswer": "C",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2032,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas clusters",
      "B": "Apenas permissões",
      "C": "Apenas notebooks",
      "D": "Arquivo databricks.yml com definições de recursos"
    },
    "correctAnswer": "D",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2033,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Usando o botão 'Repair and rerun' na UI do Job",
      "B": "Deletando e recriando o Job",
      "C": "Reexecutando manualmente o notebook",
      "D": "Reiniciando o cluster"
    },
    "correctAnswer": "A",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2034,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Versionamento, reprodutibilidade e automação de deployment",
      "B": "Apenas performance",
      "C": "Apenas segurança",
      "D": "Apenas documentação"
    },
    "correctAnswer": "A",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2035,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Manualmente na UI",
      "B": "Através de variáveis de ambiente",
      "C": "Não é possível",
      "D": "Usando 'depends_on' na definição de cada tarefa"
    },
    "correctAnswer": "D",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2036,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "Não há diferença",
      "B": "DAB é apenas para notebooks",
      "C": "Deployment tradicional é mais rápido",
      "D": "DAB permite Infrastructure as Code; deployment tradicional é manual"
    },
    "correctAnswer": "D",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2037,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas notebooks",
      "B": "Arquivo databricks.yml com definições de recursos",
      "C": "Apenas clusters",
      "D": "Apenas permissões"
    },
    "correctAnswer": "B",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2038,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Deletando e recriando o Job",
      "B": "Reexecutando manualmente o notebook",
      "C": "Reiniciando o cluster",
      "D": "Usando o botão 'Repair and rerun' na UI do Job"
    },
    "correctAnswer": "D",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2039,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas documentação",
      "B": "Versionamento, reprodutibilidade e automação de deployment",
      "C": "Apenas performance",
      "D": "Apenas segurança"
    },
    "correctAnswer": "B",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2040,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Usando 'depends_on' na definição de cada tarefa",
      "B": "Não é possível",
      "C": "Manualmente na UI",
      "D": "Através de variáveis de ambiente"
    },
    "correctAnswer": "A",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2041,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "Deployment tradicional é mais rápido",
      "B": "DAB é apenas para notebooks",
      "C": "Não há diferença",
      "D": "DAB permite Infrastructure as Code; deployment tradicional é manual"
    },
    "correctAnswer": "D",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2042,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas permissões",
      "B": "Apenas clusters",
      "C": "Apenas notebooks",
      "D": "Arquivo databricks.yml com definições de recursos"
    },
    "correctAnswer": "D",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2043,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Deletando e recriando o Job",
      "B": "Usando o botão 'Repair and rerun' na UI do Job",
      "C": "Reiniciando o cluster",
      "D": "Reexecutando manualmente o notebook"
    },
    "correctAnswer": "B",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2044,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas documentação",
      "B": "Versionamento, reprodutibilidade e automação de deployment",
      "C": "Apenas performance",
      "D": "Apenas segurança"
    },
    "correctAnswer": "B",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2045,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Usando 'depends_on' na definição de cada tarefa",
      "B": "Através de variáveis de ambiente",
      "C": "Manualmente na UI",
      "D": "Não é possível"
    },
    "correctAnswer": "A",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2046,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "Não há diferença",
      "B": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "C": "Deployment tradicional é mais rápido",
      "D": "DAB é apenas para notebooks"
    },
    "correctAnswer": "B",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2047,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas clusters",
      "B": "Apenas notebooks",
      "C": "Apenas permissões",
      "D": "Arquivo databricks.yml com definições de recursos"
    },
    "correctAnswer": "D",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2048,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Usando o botão 'Repair and rerun' na UI do Job",
      "B": "Deletando e recriando o Job",
      "C": "Reexecutando manualmente o notebook",
      "D": "Reiniciando o cluster"
    },
    "correctAnswer": "A",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2049,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas performance",
      "B": "Apenas segurança",
      "C": "Apenas documentação",
      "D": "Versionamento, reprodutibilidade e automação de deployment"
    },
    "correctAnswer": "D",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2050,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Através de variáveis de ambiente",
      "B": "Manualmente na UI",
      "C": "Usando 'depends_on' na definição de cada tarefa",
      "D": "Não é possível"
    },
    "correctAnswer": "C",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2051,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "B": "DAB é apenas para notebooks",
      "C": "Deployment tradicional é mais rápido",
      "D": "Não há diferença"
    },
    "correctAnswer": "A",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2052,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas notebooks",
      "B": "Apenas clusters",
      "C": "Arquivo databricks.yml com definições de recursos",
      "D": "Apenas permissões"
    },
    "correctAnswer": "C",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2053,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Deletando e recriando o Job",
      "B": "Usando o botão 'Repair and rerun' na UI do Job",
      "C": "Reexecutando manualmente o notebook",
      "D": "Reiniciando o cluster"
    },
    "correctAnswer": "B",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2054,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Versionamento, reprodutibilidade e automação de deployment",
      "B": "Apenas segurança",
      "C": "Apenas documentação",
      "D": "Apenas performance"
    },
    "correctAnswer": "A",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2055,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Usando 'depends_on' na definição de cada tarefa",
      "B": "Não é possível",
      "C": "Manualmente na UI",
      "D": "Através de variáveis de ambiente"
    },
    "correctAnswer": "A",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2056,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "DAB é apenas para notebooks",
      "B": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "C": "Deployment tradicional é mais rápido",
      "D": "Não há diferença"
    },
    "correctAnswer": "B",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2057,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas permissões",
      "B": "Apenas clusters",
      "C": "Arquivo databricks.yml com definições de recursos",
      "D": "Apenas notebooks"
    },
    "correctAnswer": "C",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2058,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Reiniciando o cluster",
      "B": "Reexecutando manualmente o notebook",
      "C": "Deletando e recriando o Job",
      "D": "Usando o botão 'Repair and rerun' na UI do Job"
    },
    "correctAnswer": "D",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2059,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Versionamento, reprodutibilidade e automação de deployment",
      "B": "Apenas performance",
      "C": "Apenas segurança",
      "D": "Apenas documentação"
    },
    "correctAnswer": "A",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2060,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Não é possível",
      "B": "Através de variáveis de ambiente",
      "C": "Manualmente na UI",
      "D": "Usando 'depends_on' na definição de cada tarefa"
    },
    "correctAnswer": "D",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2061,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "DAB é apenas para notebooks",
      "B": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "C": "Deployment tradicional é mais rápido",
      "D": "Não há diferença"
    },
    "correctAnswer": "B",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2062,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Arquivo databricks.yml com definições de recursos",
      "B": "Apenas notebooks",
      "C": "Apenas permissões",
      "D": "Apenas clusters"
    },
    "correctAnswer": "A",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2063,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Reexecutando manualmente o notebook",
      "B": "Deletando e recriando o Job",
      "C": "Reiniciando o cluster",
      "D": "Usando o botão 'Repair and rerun' na UI do Job"
    },
    "correctAnswer": "D",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2064,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas performance",
      "B": "Apenas segurança",
      "C": "Apenas documentação",
      "D": "Versionamento, reprodutibilidade e automação de deployment"
    },
    "correctAnswer": "D",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2065,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Usando 'depends_on' na definição de cada tarefa",
      "B": "Manualmente na UI",
      "C": "Através de variáveis de ambiente",
      "D": "Não é possível"
    },
    "correctAnswer": "A",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2066,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "DAB é apenas para notebooks",
      "B": "Deployment tradicional é mais rápido",
      "C": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "D": "Não há diferença"
    },
    "correctAnswer": "C",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2067,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas clusters",
      "B": "Apenas notebooks",
      "C": "Arquivo databricks.yml com definições de recursos",
      "D": "Apenas permissões"
    },
    "correctAnswer": "C",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2068,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Reexecutando manualmente o notebook",
      "B": "Reiniciando o cluster",
      "C": "Usando o botão 'Repair and rerun' na UI do Job",
      "D": "Deletando e recriando o Job"
    },
    "correctAnswer": "C",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2069,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas performance",
      "B": "Apenas documentação",
      "C": "Versionamento, reprodutibilidade e automação de deployment",
      "D": "Apenas segurança"
    },
    "correctAnswer": "C",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2070,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Não é possível",
      "B": "Manualmente na UI",
      "C": "Usando 'depends_on' na definição de cada tarefa",
      "D": "Através de variáveis de ambiente"
    },
    "correctAnswer": "C",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2071,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "Deployment tradicional é mais rápido",
      "B": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "C": "DAB é apenas para notebooks",
      "D": "Não há diferença"
    },
    "correctAnswer": "B",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2072,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Arquivo databricks.yml com definições de recursos",
      "B": "Apenas permissões",
      "C": "Apenas clusters",
      "D": "Apenas notebooks"
    },
    "correctAnswer": "A",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2073,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Reexecutando manualmente o notebook",
      "B": "Deletando e recriando o Job",
      "C": "Usando o botão 'Repair and rerun' na UI do Job",
      "D": "Reiniciando o cluster"
    },
    "correctAnswer": "C",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2074,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Versionamento, reprodutibilidade e automação de deployment",
      "B": "Apenas documentação",
      "C": "Apenas segurança",
      "D": "Apenas performance"
    },
    "correctAnswer": "A",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2075,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Manualmente na UI",
      "B": "Não é possível",
      "C": "Através de variáveis de ambiente",
      "D": "Usando 'depends_on' na definição de cada tarefa"
    },
    "correctAnswer": "D",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2076,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "DAB é apenas para notebooks",
      "B": "Não há diferença",
      "C": "Deployment tradicional é mais rápido",
      "D": "DAB permite Infrastructure as Code; deployment tradicional é manual"
    },
    "correctAnswer": "D",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2077,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Arquivo databricks.yml com definições de recursos",
      "B": "Apenas clusters",
      "C": "Apenas notebooks",
      "D": "Apenas permissões"
    },
    "correctAnswer": "A",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2078,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Usando o botão 'Repair and rerun' na UI do Job",
      "B": "Reexecutando manualmente o notebook",
      "C": "Deletando e recriando o Job",
      "D": "Reiniciando o cluster"
    },
    "correctAnswer": "A",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2079,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Versionamento, reprodutibilidade e automação de deployment",
      "B": "Apenas performance",
      "C": "Apenas documentação",
      "D": "Apenas segurança"
    },
    "correctAnswer": "A",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2080,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Manualmente na UI",
      "B": "Através de variáveis de ambiente",
      "C": "Usando 'depends_on' na definição de cada tarefa",
      "D": "Não é possível"
    },
    "correctAnswer": "C",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2081,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "Não há diferença",
      "B": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "C": "DAB é apenas para notebooks",
      "D": "Deployment tradicional é mais rápido"
    },
    "correctAnswer": "B",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2082,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas notebooks",
      "B": "Apenas permissões",
      "C": "Arquivo databricks.yml com definições de recursos",
      "D": "Apenas clusters"
    },
    "correctAnswer": "C",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2083,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Reiniciando o cluster",
      "B": "Deletando e recriando o Job",
      "C": "Reexecutando manualmente o notebook",
      "D": "Usando o botão 'Repair and rerun' na UI do Job"
    },
    "correctAnswer": "D",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2084,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas segurança",
      "B": "Versionamento, reprodutibilidade e automação de deployment",
      "C": "Apenas performance",
      "D": "Apenas documentação"
    },
    "correctAnswer": "B",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2085,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Não é possível",
      "B": "Através de variáveis de ambiente",
      "C": "Usando 'depends_on' na definição de cada tarefa",
      "D": "Manualmente na UI"
    },
    "correctAnswer": "C",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2086,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "B": "DAB é apenas para notebooks",
      "C": "Não há diferença",
      "D": "Deployment tradicional é mais rápido"
    },
    "correctAnswer": "A",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2087,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Arquivo databricks.yml com definições de recursos",
      "B": "Apenas permissões",
      "C": "Apenas clusters",
      "D": "Apenas notebooks"
    },
    "correctAnswer": "A",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2088,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Reiniciando o cluster",
      "B": "Deletando e recriando o Job",
      "C": "Usando o botão 'Repair and rerun' na UI do Job",
      "D": "Reexecutando manualmente o notebook"
    },
    "correctAnswer": "C",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2089,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas performance",
      "B": "Versionamento, reprodutibilidade e automação de deployment",
      "C": "Apenas documentação",
      "D": "Apenas segurança"
    },
    "correctAnswer": "B",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2090,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Não é possível",
      "B": "Usando 'depends_on' na definição de cada tarefa",
      "C": "Manualmente na UI",
      "D": "Através de variáveis de ambiente"
    },
    "correctAnswer": "B",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2091,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "B": "Não há diferença",
      "C": "DAB é apenas para notebooks",
      "D": "Deployment tradicional é mais rápido"
    },
    "correctAnswer": "A",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2092,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas permissões",
      "B": "Arquivo databricks.yml com definições de recursos",
      "C": "Apenas clusters",
      "D": "Apenas notebooks"
    },
    "correctAnswer": "B",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2093,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Deletando e recriando o Job",
      "B": "Usando o botão 'Repair and rerun' na UI do Job",
      "C": "Reiniciando o cluster",
      "D": "Reexecutando manualmente o notebook"
    },
    "correctAnswer": "B",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2094,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas documentação",
      "B": "Apenas segurança",
      "C": "Apenas performance",
      "D": "Versionamento, reprodutibilidade e automação de deployment"
    },
    "correctAnswer": "D",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2095,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Não é possível",
      "B": "Através de variáveis de ambiente",
      "C": "Usando 'depends_on' na definição de cada tarefa",
      "D": "Manualmente na UI"
    },
    "correctAnswer": "C",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2096,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "B": "DAB é apenas para notebooks",
      "C": "Deployment tradicional é mais rápido",
      "D": "Não há diferença"
    },
    "correctAnswer": "A",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2097,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas notebooks",
      "B": "Apenas permissões",
      "C": "Arquivo databricks.yml com definições de recursos",
      "D": "Apenas clusters"
    },
    "correctAnswer": "C",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2098,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Usando o botão 'Repair and rerun' na UI do Job",
      "B": "Reiniciando o cluster",
      "C": "Deletando e recriando o Job",
      "D": "Reexecutando manualmente o notebook"
    },
    "correctAnswer": "A",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2099,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas performance",
      "B": "Apenas segurança",
      "C": "Apenas documentação",
      "D": "Versionamento, reprodutibilidade e automação de deployment"
    },
    "correctAnswer": "D",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2100,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Não é possível",
      "B": "Através de variáveis de ambiente",
      "C": "Manualmente na UI",
      "D": "Usando 'depends_on' na definição de cada tarefa"
    },
    "correctAnswer": "D",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2101,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "Deployment tradicional é mais rápido",
      "B": "Não há diferença",
      "C": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "D": "DAB é apenas para notebooks"
    },
    "correctAnswer": "C",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2102,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas notebooks",
      "B": "Apenas permissões",
      "C": "Arquivo databricks.yml com definições de recursos",
      "D": "Apenas clusters"
    },
    "correctAnswer": "C",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2103,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Deletando e recriando o Job",
      "B": "Usando o botão 'Repair and rerun' na UI do Job",
      "C": "Reiniciando o cluster",
      "D": "Reexecutando manualmente o notebook"
    },
    "correctAnswer": "B",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2104,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas segurança",
      "B": "Versionamento, reprodutibilidade e automação de deployment",
      "C": "Apenas documentação",
      "D": "Apenas performance"
    },
    "correctAnswer": "B",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2105,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Não é possível",
      "B": "Usando 'depends_on' na definição de cada tarefa",
      "C": "Manualmente na UI",
      "D": "Através de variáveis de ambiente"
    },
    "correctAnswer": "B",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2106,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "B": "DAB é apenas para notebooks",
      "C": "Não há diferença",
      "D": "Deployment tradicional é mais rápido"
    },
    "correctAnswer": "A",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2107,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas permissões",
      "B": "Arquivo databricks.yml com definições de recursos",
      "C": "Apenas clusters",
      "D": "Apenas notebooks"
    },
    "correctAnswer": "B",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2108,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Usando o botão 'Repair and rerun' na UI do Job",
      "B": "Deletando e recriando o Job",
      "C": "Reiniciando o cluster",
      "D": "Reexecutando manualmente o notebook"
    },
    "correctAnswer": "A",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2109,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas documentação",
      "B": "Apenas segurança",
      "C": "Apenas performance",
      "D": "Versionamento, reprodutibilidade e automação de deployment"
    },
    "correctAnswer": "D",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2110,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Usando 'depends_on' na definição de cada tarefa",
      "B": "Através de variáveis de ambiente",
      "C": "Não é possível",
      "D": "Manualmente na UI"
    },
    "correctAnswer": "A",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2111,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "Não há diferença",
      "B": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "C": "Deployment tradicional é mais rápido",
      "D": "DAB é apenas para notebooks"
    },
    "correctAnswer": "B",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2112,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Arquivo databricks.yml com definições de recursos",
      "B": "Apenas permissões",
      "C": "Apenas clusters",
      "D": "Apenas notebooks"
    },
    "correctAnswer": "A",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2113,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Reexecutando manualmente o notebook",
      "B": "Reiniciando o cluster",
      "C": "Deletando e recriando o Job",
      "D": "Usando o botão 'Repair and rerun' na UI do Job"
    },
    "correctAnswer": "D",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2114,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas documentação",
      "B": "Apenas performance",
      "C": "Apenas segurança",
      "D": "Versionamento, reprodutibilidade e automação de deployment"
    },
    "correctAnswer": "D",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2115,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Manualmente na UI",
      "B": "Não é possível",
      "C": "Usando 'depends_on' na definição de cada tarefa",
      "D": "Através de variáveis de ambiente"
    },
    "correctAnswer": "C",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2116,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "Deployment tradicional é mais rápido",
      "B": "DAB é apenas para notebooks",
      "C": "Não há diferença",
      "D": "DAB permite Infrastructure as Code; deployment tradicional é manual"
    },
    "correctAnswer": "D",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2117,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas notebooks",
      "B": "Arquivo databricks.yml com definições de recursos",
      "C": "Apenas permissões",
      "D": "Apenas clusters"
    },
    "correctAnswer": "B",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2118,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Usando o botão 'Repair and rerun' na UI do Job",
      "B": "Deletando e recriando o Job",
      "C": "Reexecutando manualmente o notebook",
      "D": "Reiniciando o cluster"
    },
    "correctAnswer": "A",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2119,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas performance",
      "B": "Apenas documentação",
      "C": "Apenas segurança",
      "D": "Versionamento, reprodutibilidade e automação de deployment"
    },
    "correctAnswer": "D",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2120,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Usando 'depends_on' na definição de cada tarefa",
      "B": "Não é possível",
      "C": "Manualmente na UI",
      "D": "Através de variáveis de ambiente"
    },
    "correctAnswer": "A",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2121,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "B": "Não há diferença",
      "C": "Deployment tradicional é mais rápido",
      "D": "DAB é apenas para notebooks"
    },
    "correctAnswer": "A",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2122,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas notebooks",
      "B": "Apenas permissões",
      "C": "Arquivo databricks.yml com definições de recursos",
      "D": "Apenas clusters"
    },
    "correctAnswer": "C",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2123,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Deletando e recriando o Job",
      "B": "Reexecutando manualmente o notebook",
      "C": "Reiniciando o cluster",
      "D": "Usando o botão 'Repair and rerun' na UI do Job"
    },
    "correctAnswer": "D",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2124,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Versionamento, reprodutibilidade e automação de deployment",
      "B": "Apenas segurança",
      "C": "Apenas documentação",
      "D": "Apenas performance"
    },
    "correctAnswer": "A",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2125,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Usando 'depends_on' na definição de cada tarefa",
      "B": "Manualmente na UI",
      "C": "Não é possível",
      "D": "Através de variáveis de ambiente"
    },
    "correctAnswer": "A",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2126,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "DAB é apenas para notebooks",
      "B": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "C": "Deployment tradicional é mais rápido",
      "D": "Não há diferença"
    },
    "correctAnswer": "B",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2127,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Arquivo databricks.yml com definições de recursos",
      "B": "Apenas notebooks",
      "C": "Apenas permissões",
      "D": "Apenas clusters"
    },
    "correctAnswer": "A",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2128,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Usando o botão 'Repair and rerun' na UI do Job",
      "B": "Reexecutando manualmente o notebook",
      "C": "Deletando e recriando o Job",
      "D": "Reiniciando o cluster"
    },
    "correctAnswer": "A",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2129,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas segurança",
      "B": "Apenas performance",
      "C": "Versionamento, reprodutibilidade e automação de deployment",
      "D": "Apenas documentação"
    },
    "correctAnswer": "C",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2130,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Através de variáveis de ambiente",
      "B": "Manualmente na UI",
      "C": "Usando 'depends_on' na definição de cada tarefa",
      "D": "Não é possível"
    },
    "correctAnswer": "C",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2131,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "DAB é apenas para notebooks",
      "B": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "C": "Deployment tradicional é mais rápido",
      "D": "Não há diferença"
    },
    "correctAnswer": "B",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2132,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas clusters",
      "B": "Apenas permissões",
      "C": "Apenas notebooks",
      "D": "Arquivo databricks.yml com definições de recursos"
    },
    "correctAnswer": "D",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2133,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Deletando e recriando o Job",
      "B": "Reexecutando manualmente o notebook",
      "C": "Reiniciando o cluster",
      "D": "Usando o botão 'Repair and rerun' na UI do Job"
    },
    "correctAnswer": "D",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2134,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas segurança",
      "B": "Apenas documentação",
      "C": "Versionamento, reprodutibilidade e automação de deployment",
      "D": "Apenas performance"
    },
    "correctAnswer": "C",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2135,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Manualmente na UI",
      "B": "Não é possível",
      "C": "Usando 'depends_on' na definição de cada tarefa",
      "D": "Através de variáveis de ambiente"
    },
    "correctAnswer": "C",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2136,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "B": "Não há diferença",
      "C": "Deployment tradicional é mais rápido",
      "D": "DAB é apenas para notebooks"
    },
    "correctAnswer": "A",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2137,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas permissões",
      "B": "Arquivo databricks.yml com definições de recursos",
      "C": "Apenas notebooks",
      "D": "Apenas clusters"
    },
    "correctAnswer": "B",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2138,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Usando o botão 'Repair and rerun' na UI do Job",
      "B": "Deletando e recriando o Job",
      "C": "Reexecutando manualmente o notebook",
      "D": "Reiniciando o cluster"
    },
    "correctAnswer": "A",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2139,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas performance",
      "B": "Apenas segurança",
      "C": "Apenas documentação",
      "D": "Versionamento, reprodutibilidade e automação de deployment"
    },
    "correctAnswer": "D",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2140,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Manualmente na UI",
      "B": "Através de variáveis de ambiente",
      "C": "Usando 'depends_on' na definição de cada tarefa",
      "D": "Não é possível"
    },
    "correctAnswer": "C",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2141,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "B": "Deployment tradicional é mais rápido",
      "C": "Não há diferença",
      "D": "DAB é apenas para notebooks"
    },
    "correctAnswer": "A",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2142,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas clusters",
      "B": "Apenas notebooks",
      "C": "Arquivo databricks.yml com definições de recursos",
      "D": "Apenas permissões"
    },
    "correctAnswer": "C",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2143,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Reiniciando o cluster",
      "B": "Reexecutando manualmente o notebook",
      "C": "Deletando e recriando o Job",
      "D": "Usando o botão 'Repair and rerun' na UI do Job"
    },
    "correctAnswer": "D",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2144,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas documentação",
      "B": "Apenas segurança",
      "C": "Versionamento, reprodutibilidade e automação de deployment",
      "D": "Apenas performance"
    },
    "correctAnswer": "C",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2145,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Não é possível",
      "B": "Usando 'depends_on' na definição de cada tarefa",
      "C": "Manualmente na UI",
      "D": "Através de variáveis de ambiente"
    },
    "correctAnswer": "B",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2146,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "Deployment tradicional é mais rápido",
      "B": "Não há diferença",
      "C": "DAB é apenas para notebooks",
      "D": "DAB permite Infrastructure as Code; deployment tradicional é manual"
    },
    "correctAnswer": "D",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2147,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas permissões",
      "B": "Apenas clusters",
      "C": "Apenas notebooks",
      "D": "Arquivo databricks.yml com definições de recursos"
    },
    "correctAnswer": "D",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2148,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Reiniciando o cluster",
      "B": "Reexecutando manualmente o notebook",
      "C": "Deletando e recriando o Job",
      "D": "Usando o botão 'Repair and rerun' na UI do Job"
    },
    "correctAnswer": "D",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2149,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas segurança",
      "B": "Versionamento, reprodutibilidade e automação de deployment",
      "C": "Apenas documentação",
      "D": "Apenas performance"
    },
    "correctAnswer": "B",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2150,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Não é possível",
      "B": "Através de variáveis de ambiente",
      "C": "Usando 'depends_on' na definição de cada tarefa",
      "D": "Manualmente na UI"
    },
    "correctAnswer": "C",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2151,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "Deployment tradicional é mais rápido",
      "B": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "C": "DAB é apenas para notebooks",
      "D": "Não há diferença"
    },
    "correctAnswer": "B",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2152,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Arquivo databricks.yml com definições de recursos",
      "B": "Apenas clusters",
      "C": "Apenas permissões",
      "D": "Apenas notebooks"
    },
    "correctAnswer": "A",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2153,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Deletando e recriando o Job",
      "B": "Reexecutando manualmente o notebook",
      "C": "Reiniciando o cluster",
      "D": "Usando o botão 'Repair and rerun' na UI do Job"
    },
    "correctAnswer": "D",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2154,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas performance",
      "B": "Apenas segurança",
      "C": "Apenas documentação",
      "D": "Versionamento, reprodutibilidade e automação de deployment"
    },
    "correctAnswer": "D",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2155,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Usando 'depends_on' na definição de cada tarefa",
      "B": "Manualmente na UI",
      "C": "Não é possível",
      "D": "Através de variáveis de ambiente"
    },
    "correctAnswer": "A",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2156,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "B": "Deployment tradicional é mais rápido",
      "C": "Não há diferença",
      "D": "DAB é apenas para notebooks"
    },
    "correctAnswer": "A",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2157,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas notebooks",
      "B": "Arquivo databricks.yml com definições de recursos",
      "C": "Apenas permissões",
      "D": "Apenas clusters"
    },
    "correctAnswer": "B",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2158,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Reiniciando o cluster",
      "B": "Usando o botão 'Repair and rerun' na UI do Job",
      "C": "Reexecutando manualmente o notebook",
      "D": "Deletando e recriando o Job"
    },
    "correctAnswer": "B",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2159,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas performance",
      "B": "Apenas segurança",
      "C": "Versionamento, reprodutibilidade e automação de deployment",
      "D": "Apenas documentação"
    },
    "correctAnswer": "C",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2160,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Usando 'depends_on' na definição de cada tarefa",
      "B": "Não é possível",
      "C": "Através de variáveis de ambiente",
      "D": "Manualmente na UI"
    },
    "correctAnswer": "A",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2161,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "DAB é apenas para notebooks",
      "B": "Não há diferença",
      "C": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "D": "Deployment tradicional é mais rápido"
    },
    "correctAnswer": "C",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2162,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas clusters",
      "B": "Apenas notebooks",
      "C": "Arquivo databricks.yml com definições de recursos",
      "D": "Apenas permissões"
    },
    "correctAnswer": "C",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2163,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Deletando e recriando o Job",
      "B": "Usando o botão 'Repair and rerun' na UI do Job",
      "C": "Reexecutando manualmente o notebook",
      "D": "Reiniciando o cluster"
    },
    "correctAnswer": "B",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2164,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas performance",
      "B": "Versionamento, reprodutibilidade e automação de deployment",
      "C": "Apenas documentação",
      "D": "Apenas segurança"
    },
    "correctAnswer": "B",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2165,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Através de variáveis de ambiente",
      "B": "Não é possível",
      "C": "Usando 'depends_on' na definição de cada tarefa",
      "D": "Manualmente na UI"
    },
    "correctAnswer": "C",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2166,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "Não há diferença",
      "B": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "C": "DAB é apenas para notebooks",
      "D": "Deployment tradicional é mais rápido"
    },
    "correctAnswer": "B",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2167,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Arquivo databricks.yml com definições de recursos",
      "B": "Apenas clusters",
      "C": "Apenas notebooks",
      "D": "Apenas permissões"
    },
    "correctAnswer": "A",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2168,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Deletando e recriando o Job",
      "B": "Reiniciando o cluster",
      "C": "Reexecutando manualmente o notebook",
      "D": "Usando o botão 'Repair and rerun' na UI do Job"
    },
    "correctAnswer": "D",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2169,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas documentação",
      "B": "Apenas performance",
      "C": "Apenas segurança",
      "D": "Versionamento, reprodutibilidade e automação de deployment"
    },
    "correctAnswer": "D",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2170,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Não é possível",
      "B": "Usando 'depends_on' na definição de cada tarefa",
      "C": "Manualmente na UI",
      "D": "Através de variáveis de ambiente"
    },
    "correctAnswer": "B",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2171,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "Não há diferença",
      "B": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "C": "Deployment tradicional é mais rápido",
      "D": "DAB é apenas para notebooks"
    },
    "correctAnswer": "B",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2172,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Arquivo databricks.yml com definições de recursos",
      "B": "Apenas notebooks",
      "C": "Apenas clusters",
      "D": "Apenas permissões"
    },
    "correctAnswer": "A",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2173,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Reexecutando manualmente o notebook",
      "B": "Usando o botão 'Repair and rerun' na UI do Job",
      "C": "Reiniciando o cluster",
      "D": "Deletando e recriando o Job"
    },
    "correctAnswer": "B",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2174,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas documentação",
      "B": "Apenas performance",
      "C": "Apenas segurança",
      "D": "Versionamento, reprodutibilidade e automação de deployment"
    },
    "correctAnswer": "D",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2175,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Manualmente na UI",
      "B": "Não é possível",
      "C": "Usando 'depends_on' na definição de cada tarefa",
      "D": "Através de variáveis de ambiente"
    },
    "correctAnswer": "C",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2176,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "Deployment tradicional é mais rápido",
      "B": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "C": "DAB é apenas para notebooks",
      "D": "Não há diferença"
    },
    "correctAnswer": "B",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2177,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas permissões",
      "B": "Apenas clusters",
      "C": "Arquivo databricks.yml com definições de recursos",
      "D": "Apenas notebooks"
    },
    "correctAnswer": "C",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2178,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Reexecutando manualmente o notebook",
      "B": "Deletando e recriando o Job",
      "C": "Reiniciando o cluster",
      "D": "Usando o botão 'Repair and rerun' na UI do Job"
    },
    "correctAnswer": "D",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2179,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas segurança",
      "B": "Versionamento, reprodutibilidade e automação de deployment",
      "C": "Apenas documentação",
      "D": "Apenas performance"
    },
    "correctAnswer": "B",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2180,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Manualmente na UI",
      "B": "Usando 'depends_on' na definição de cada tarefa",
      "C": "Não é possível",
      "D": "Através de variáveis de ambiente"
    },
    "correctAnswer": "B",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2181,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "Não há diferença",
      "B": "DAB é apenas para notebooks",
      "C": "Deployment tradicional é mais rápido",
      "D": "DAB permite Infrastructure as Code; deployment tradicional é manual"
    },
    "correctAnswer": "D",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2182,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas notebooks",
      "B": "Apenas clusters",
      "C": "Arquivo databricks.yml com definições de recursos",
      "D": "Apenas permissões"
    },
    "correctAnswer": "C",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2183,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Reexecutando manualmente o notebook",
      "B": "Reiniciando o cluster",
      "C": "Usando o botão 'Repair and rerun' na UI do Job",
      "D": "Deletando e recriando o Job"
    },
    "correctAnswer": "C",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2184,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas documentação",
      "B": "Apenas segurança",
      "C": "Apenas performance",
      "D": "Versionamento, reprodutibilidade e automação de deployment"
    },
    "correctAnswer": "D",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2185,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Não é possível",
      "B": "Usando 'depends_on' na definição de cada tarefa",
      "C": "Através de variáveis de ambiente",
      "D": "Manualmente na UI"
    },
    "correctAnswer": "B",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2186,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "Não há diferença",
      "B": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "C": "Deployment tradicional é mais rápido",
      "D": "DAB é apenas para notebooks"
    },
    "correctAnswer": "B",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2187,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas permissões",
      "B": "Apenas notebooks",
      "C": "Arquivo databricks.yml com definições de recursos",
      "D": "Apenas clusters"
    },
    "correctAnswer": "C",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2188,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Deletando e recriando o Job",
      "B": "Reiniciando o cluster",
      "C": "Usando o botão 'Repair and rerun' na UI do Job",
      "D": "Reexecutando manualmente o notebook"
    },
    "correctAnswer": "C",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2189,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas performance",
      "B": "Apenas documentação",
      "C": "Versionamento, reprodutibilidade e automação de deployment",
      "D": "Apenas segurança"
    },
    "correctAnswer": "C",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2190,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Usando 'depends_on' na definição de cada tarefa",
      "B": "Através de variáveis de ambiente",
      "C": "Não é possível",
      "D": "Manualmente na UI"
    },
    "correctAnswer": "A",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2191,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "B": "DAB é apenas para notebooks",
      "C": "Não há diferença",
      "D": "Deployment tradicional é mais rápido"
    },
    "correctAnswer": "A",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2192,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas notebooks",
      "B": "Apenas permissões",
      "C": "Arquivo databricks.yml com definições de recursos",
      "D": "Apenas clusters"
    },
    "correctAnswer": "C",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2193,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Reiniciando o cluster",
      "B": "Reexecutando manualmente o notebook",
      "C": "Deletando e recriando o Job",
      "D": "Usando o botão 'Repair and rerun' na UI do Job"
    },
    "correctAnswer": "D",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2194,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Versionamento, reprodutibilidade e automação de deployment",
      "B": "Apenas segurança",
      "C": "Apenas documentação",
      "D": "Apenas performance"
    },
    "correctAnswer": "A",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2195,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Usando 'depends_on' na definição de cada tarefa",
      "B": "Através de variáveis de ambiente",
      "C": "Não é possível",
      "D": "Manualmente na UI"
    },
    "correctAnswer": "A",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2196,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "Deployment tradicional é mais rápido",
      "B": "DAB é apenas para notebooks",
      "C": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "D": "Não há diferença"
    },
    "correctAnswer": "C",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2197,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas permissões",
      "B": "Apenas clusters",
      "C": "Arquivo databricks.yml com definições de recursos",
      "D": "Apenas notebooks"
    },
    "correctAnswer": "C",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2198,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Deletando e recriando o Job",
      "B": "Usando o botão 'Repair and rerun' na UI do Job",
      "C": "Reexecutando manualmente o notebook",
      "D": "Reiniciando o cluster"
    },
    "correctAnswer": "B",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2199,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Versionamento, reprodutibilidade e automação de deployment",
      "B": "Apenas performance",
      "C": "Apenas segurança",
      "D": "Apenas documentação"
    },
    "correctAnswer": "A",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2200,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Não é possível",
      "B": "Usando 'depends_on' na definição de cada tarefa",
      "C": "Manualmente na UI",
      "D": "Através de variáveis de ambiente"
    },
    "correctAnswer": "B",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2201,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "Não há diferença",
      "B": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "C": "DAB é apenas para notebooks",
      "D": "Deployment tradicional é mais rápido"
    },
    "correctAnswer": "B",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2202,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Arquivo databricks.yml com definições de recursos",
      "B": "Apenas clusters",
      "C": "Apenas permissões",
      "D": "Apenas notebooks"
    },
    "correctAnswer": "A",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2203,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Usando o botão 'Repair and rerun' na UI do Job",
      "B": "Reexecutando manualmente o notebook",
      "C": "Deletando e recriando o Job",
      "D": "Reiniciando o cluster"
    },
    "correctAnswer": "A",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2204,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas segurança",
      "B": "Versionamento, reprodutibilidade e automação de deployment",
      "C": "Apenas documentação",
      "D": "Apenas performance"
    },
    "correctAnswer": "B",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2205,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Manualmente na UI",
      "B": "Usando 'depends_on' na definição de cada tarefa",
      "C": "Através de variáveis de ambiente",
      "D": "Não é possível"
    },
    "correctAnswer": "B",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2206,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "B": "Não há diferença",
      "C": "DAB é apenas para notebooks",
      "D": "Deployment tradicional é mais rápido"
    },
    "correctAnswer": "A",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2207,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas clusters",
      "B": "Apenas notebooks",
      "C": "Arquivo databricks.yml com definições de recursos",
      "D": "Apenas permissões"
    },
    "correctAnswer": "C",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2208,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Usando o botão 'Repair and rerun' na UI do Job",
      "B": "Reiniciando o cluster",
      "C": "Reexecutando manualmente o notebook",
      "D": "Deletando e recriando o Job"
    },
    "correctAnswer": "A",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2209,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas documentação",
      "B": "Apenas segurança",
      "C": "Versionamento, reprodutibilidade e automação de deployment",
      "D": "Apenas performance"
    },
    "correctAnswer": "C",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2210,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Usando 'depends_on' na definição de cada tarefa",
      "B": "Através de variáveis de ambiente",
      "C": "Manualmente na UI",
      "D": "Não é possível"
    },
    "correctAnswer": "A",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2211,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "Não há diferença",
      "B": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "C": "Deployment tradicional é mais rápido",
      "D": "DAB é apenas para notebooks"
    },
    "correctAnswer": "B",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2212,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas notebooks",
      "B": "Apenas permissões",
      "C": "Apenas clusters",
      "D": "Arquivo databricks.yml com definições de recursos"
    },
    "correctAnswer": "D",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2213,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Reexecutando manualmente o notebook",
      "B": "Deletando e recriando o Job",
      "C": "Reiniciando o cluster",
      "D": "Usando o botão 'Repair and rerun' na UI do Job"
    },
    "correctAnswer": "D",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2214,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas documentação",
      "B": "Versionamento, reprodutibilidade e automação de deployment",
      "C": "Apenas segurança",
      "D": "Apenas performance"
    },
    "correctAnswer": "B",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2215,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Usando 'depends_on' na definição de cada tarefa",
      "B": "Através de variáveis de ambiente",
      "C": "Não é possível",
      "D": "Manualmente na UI"
    },
    "correctAnswer": "A",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2216,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "Não há diferença",
      "B": "DAB é apenas para notebooks",
      "C": "Deployment tradicional é mais rápido",
      "D": "DAB permite Infrastructure as Code; deployment tradicional é manual"
    },
    "correctAnswer": "D",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2217,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas clusters",
      "B": "Apenas notebooks",
      "C": "Arquivo databricks.yml com definições de recursos",
      "D": "Apenas permissões"
    },
    "correctAnswer": "C",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2218,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Deletando e recriando o Job",
      "B": "Reiniciando o cluster",
      "C": "Reexecutando manualmente o notebook",
      "D": "Usando o botão 'Repair and rerun' na UI do Job"
    },
    "correctAnswer": "D",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2219,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas segurança",
      "B": "Apenas performance",
      "C": "Versionamento, reprodutibilidade e automação de deployment",
      "D": "Apenas documentação"
    },
    "correctAnswer": "C",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2220,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Não é possível",
      "B": "Manualmente na UI",
      "C": "Através de variáveis de ambiente",
      "D": "Usando 'depends_on' na definição de cada tarefa"
    },
    "correctAnswer": "D",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2221,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "Deployment tradicional é mais rápido",
      "B": "Não há diferença",
      "C": "DAB é apenas para notebooks",
      "D": "DAB permite Infrastructure as Code; deployment tradicional é manual"
    },
    "correctAnswer": "D",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2222,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas permissões",
      "B": "Apenas notebooks",
      "C": "Arquivo databricks.yml com definições de recursos",
      "D": "Apenas clusters"
    },
    "correctAnswer": "C",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2223,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Reiniciando o cluster",
      "B": "Reexecutando manualmente o notebook",
      "C": "Usando o botão 'Repair and rerun' na UI do Job",
      "D": "Deletando e recriando o Job"
    },
    "correctAnswer": "C",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2224,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas documentação",
      "B": "Apenas segurança",
      "C": "Apenas performance",
      "D": "Versionamento, reprodutibilidade e automação de deployment"
    },
    "correctAnswer": "D",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2225,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Não é possível",
      "B": "Através de variáveis de ambiente",
      "C": "Usando 'depends_on' na definição de cada tarefa",
      "D": "Manualmente na UI"
    },
    "correctAnswer": "C",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2226,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "DAB é apenas para notebooks",
      "B": "Deployment tradicional é mais rápido",
      "C": "Não há diferença",
      "D": "DAB permite Infrastructure as Code; deployment tradicional é manual"
    },
    "correctAnswer": "D",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2227,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas permissões",
      "B": "Arquivo databricks.yml com definições de recursos",
      "C": "Apenas notebooks",
      "D": "Apenas clusters"
    },
    "correctAnswer": "B",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2228,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Deletando e recriando o Job",
      "B": "Reiniciando o cluster",
      "C": "Reexecutando manualmente o notebook",
      "D": "Usando o botão 'Repair and rerun' na UI do Job"
    },
    "correctAnswer": "D",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2229,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas segurança",
      "B": "Apenas performance",
      "C": "Versionamento, reprodutibilidade e automação de deployment",
      "D": "Apenas documentação"
    },
    "correctAnswer": "C",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2230,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Manualmente na UI",
      "B": "Através de variáveis de ambiente",
      "C": "Não é possível",
      "D": "Usando 'depends_on' na definição de cada tarefa"
    },
    "correctAnswer": "D",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2231,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "DAB é apenas para notebooks",
      "B": "Não há diferença",
      "C": "Deployment tradicional é mais rápido",
      "D": "DAB permite Infrastructure as Code; deployment tradicional é manual"
    },
    "correctAnswer": "D",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2232,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas permissões",
      "B": "Arquivo databricks.yml com definições de recursos",
      "C": "Apenas notebooks",
      "D": "Apenas clusters"
    },
    "correctAnswer": "B",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2233,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Deletando e recriando o Job",
      "B": "Reexecutando manualmente o notebook",
      "C": "Reiniciando o cluster",
      "D": "Usando o botão 'Repair and rerun' na UI do Job"
    },
    "correctAnswer": "D",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2234,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas documentação",
      "B": "Versionamento, reprodutibilidade e automação de deployment",
      "C": "Apenas segurança",
      "D": "Apenas performance"
    },
    "correctAnswer": "B",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2235,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Usando 'depends_on' na definição de cada tarefa",
      "B": "Não é possível",
      "C": "Através de variáveis de ambiente",
      "D": "Manualmente na UI"
    },
    "correctAnswer": "A",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2236,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "Deployment tradicional é mais rápido",
      "B": "DAB é apenas para notebooks",
      "C": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "D": "Não há diferença"
    },
    "correctAnswer": "C",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2237,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas permissões",
      "B": "Apenas clusters",
      "C": "Arquivo databricks.yml com definições de recursos",
      "D": "Apenas notebooks"
    },
    "correctAnswer": "C",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2238,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Deletando e recriando o Job",
      "B": "Usando o botão 'Repair and rerun' na UI do Job",
      "C": "Reexecutando manualmente o notebook",
      "D": "Reiniciando o cluster"
    },
    "correctAnswer": "B",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2239,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas segurança",
      "B": "Apenas performance",
      "C": "Apenas documentação",
      "D": "Versionamento, reprodutibilidade e automação de deployment"
    },
    "correctAnswer": "D",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2240,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Usando 'depends_on' na definição de cada tarefa",
      "B": "Através de variáveis de ambiente",
      "C": "Não é possível",
      "D": "Manualmente na UI"
    },
    "correctAnswer": "A",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2241,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "Deployment tradicional é mais rápido",
      "B": "Não há diferença",
      "C": "DAB é apenas para notebooks",
      "D": "DAB permite Infrastructure as Code; deployment tradicional é manual"
    },
    "correctAnswer": "D",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2242,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Arquivo databricks.yml com definições de recursos",
      "B": "Apenas permissões",
      "C": "Apenas clusters",
      "D": "Apenas notebooks"
    },
    "correctAnswer": "A",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2243,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Reiniciando o cluster",
      "B": "Reexecutando manualmente o notebook",
      "C": "Deletando e recriando o Job",
      "D": "Usando o botão 'Repair and rerun' na UI do Job"
    },
    "correctAnswer": "D",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2244,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas segurança",
      "B": "Apenas documentação",
      "C": "Versionamento, reprodutibilidade e automação de deployment",
      "D": "Apenas performance"
    },
    "correctAnswer": "C",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2245,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Não é possível",
      "B": "Manualmente na UI",
      "C": "Através de variáveis de ambiente",
      "D": "Usando 'depends_on' na definição de cada tarefa"
    },
    "correctAnswer": "D",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2246,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "DAB é apenas para notebooks",
      "B": "Não há diferença",
      "C": "Deployment tradicional é mais rápido",
      "D": "DAB permite Infrastructure as Code; deployment tradicional é manual"
    },
    "correctAnswer": "D",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2247,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas notebooks",
      "B": "Arquivo databricks.yml com definições de recursos",
      "C": "Apenas clusters",
      "D": "Apenas permissões"
    },
    "correctAnswer": "B",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2248,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Deletando e recriando o Job",
      "B": "Reexecutando manualmente o notebook",
      "C": "Usando o botão 'Repair and rerun' na UI do Job",
      "D": "Reiniciando o cluster"
    },
    "correctAnswer": "C",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2249,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas documentação",
      "B": "Apenas segurança",
      "C": "Versionamento, reprodutibilidade e automação de deployment",
      "D": "Apenas performance"
    },
    "correctAnswer": "C",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2250,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Usando 'depends_on' na definição de cada tarefa",
      "B": "Através de variáveis de ambiente",
      "C": "Não é possível",
      "D": "Manualmente na UI"
    },
    "correctAnswer": "A",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2251,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "Não há diferença",
      "B": "DAB é apenas para notebooks",
      "C": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "D": "Deployment tradicional é mais rápido"
    },
    "correctAnswer": "C",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2252,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas clusters",
      "B": "Arquivo databricks.yml com definições de recursos",
      "C": "Apenas permissões",
      "D": "Apenas notebooks"
    },
    "correctAnswer": "B",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2253,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Reiniciando o cluster",
      "B": "Reexecutando manualmente o notebook",
      "C": "Usando o botão 'Repair and rerun' na UI do Job",
      "D": "Deletando e recriando o Job"
    },
    "correctAnswer": "C",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2254,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas performance",
      "B": "Apenas segurança",
      "C": "Apenas documentação",
      "D": "Versionamento, reprodutibilidade e automação de deployment"
    },
    "correctAnswer": "D",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2255,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Não é possível",
      "B": "Manualmente na UI",
      "C": "Usando 'depends_on' na definição de cada tarefa",
      "D": "Através de variáveis de ambiente"
    },
    "correctAnswer": "C",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2256,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "B": "DAB é apenas para notebooks",
      "C": "Deployment tradicional é mais rápido",
      "D": "Não há diferença"
    },
    "correctAnswer": "A",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2257,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Arquivo databricks.yml com definições de recursos",
      "B": "Apenas clusters",
      "C": "Apenas permissões",
      "D": "Apenas notebooks"
    },
    "correctAnswer": "A",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2258,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Reiniciando o cluster",
      "B": "Deletando e recriando o Job",
      "C": "Reexecutando manualmente o notebook",
      "D": "Usando o botão 'Repair and rerun' na UI do Job"
    },
    "correctAnswer": "D",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2259,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas performance",
      "B": "Apenas documentação",
      "C": "Versionamento, reprodutibilidade e automação de deployment",
      "D": "Apenas segurança"
    },
    "correctAnswer": "C",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2260,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Através de variáveis de ambiente",
      "B": "Não é possível",
      "C": "Manualmente na UI",
      "D": "Usando 'depends_on' na definição de cada tarefa"
    },
    "correctAnswer": "D",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2261,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "DAB é apenas para notebooks",
      "B": "Deployment tradicional é mais rápido",
      "C": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "D": "Não há diferença"
    },
    "correctAnswer": "C",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2262,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas notebooks",
      "B": "Apenas permissões",
      "C": "Arquivo databricks.yml com definições de recursos",
      "D": "Apenas clusters"
    },
    "correctAnswer": "C",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2263,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Reexecutando manualmente o notebook",
      "B": "Reiniciando o cluster",
      "C": "Deletando e recriando o Job",
      "D": "Usando o botão 'Repair and rerun' na UI do Job"
    },
    "correctAnswer": "D",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2264,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas performance",
      "B": "Apenas documentação",
      "C": "Versionamento, reprodutibilidade e automação de deployment",
      "D": "Apenas segurança"
    },
    "correctAnswer": "C",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2265,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Manualmente na UI",
      "B": "Usando 'depends_on' na definição de cada tarefa",
      "C": "Não é possível",
      "D": "Através de variáveis de ambiente"
    },
    "correctAnswer": "B",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2266,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "Deployment tradicional é mais rápido",
      "B": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "C": "DAB é apenas para notebooks",
      "D": "Não há diferença"
    },
    "correctAnswer": "B",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2267,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas permissões",
      "B": "Arquivo databricks.yml com definições de recursos",
      "C": "Apenas notebooks",
      "D": "Apenas clusters"
    },
    "correctAnswer": "B",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2268,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Reexecutando manualmente o notebook",
      "B": "Reiniciando o cluster",
      "C": "Deletando e recriando o Job",
      "D": "Usando o botão 'Repair and rerun' na UI do Job"
    },
    "correctAnswer": "D",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2269,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Versionamento, reprodutibilidade e automação de deployment",
      "B": "Apenas segurança",
      "C": "Apenas performance",
      "D": "Apenas documentação"
    },
    "correctAnswer": "A",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2270,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Não é possível",
      "B": "Usando 'depends_on' na definição de cada tarefa",
      "C": "Manualmente na UI",
      "D": "Através de variáveis de ambiente"
    },
    "correctAnswer": "B",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2271,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "Deployment tradicional é mais rápido",
      "B": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "C": "DAB é apenas para notebooks",
      "D": "Não há diferença"
    },
    "correctAnswer": "B",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2272,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas notebooks",
      "B": "Apenas permissões",
      "C": "Apenas clusters",
      "D": "Arquivo databricks.yml com definições de recursos"
    },
    "correctAnswer": "D",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2273,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Reiniciando o cluster",
      "B": "Reexecutando manualmente o notebook",
      "C": "Usando o botão 'Repair and rerun' na UI do Job",
      "D": "Deletando e recriando o Job"
    },
    "correctAnswer": "C",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2274,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas documentação",
      "B": "Apenas performance",
      "C": "Versionamento, reprodutibilidade e automação de deployment",
      "D": "Apenas segurança"
    },
    "correctAnswer": "C",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2275,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Não é possível",
      "B": "Manualmente na UI",
      "C": "Usando 'depends_on' na definição de cada tarefa",
      "D": "Através de variáveis de ambiente"
    },
    "correctAnswer": "C",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2276,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "DAB é apenas para notebooks",
      "B": "Não há diferença",
      "C": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "D": "Deployment tradicional é mais rápido"
    },
    "correctAnswer": "C",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2277,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas notebooks",
      "B": "Apenas permissões",
      "C": "Apenas clusters",
      "D": "Arquivo databricks.yml com definições de recursos"
    },
    "correctAnswer": "D",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2278,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Usando o botão 'Repair and rerun' na UI do Job",
      "B": "Reexecutando manualmente o notebook",
      "C": "Deletando e recriando o Job",
      "D": "Reiniciando o cluster"
    },
    "correctAnswer": "A",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2279,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas documentação",
      "B": "Versionamento, reprodutibilidade e automação de deployment",
      "C": "Apenas performance",
      "D": "Apenas segurança"
    },
    "correctAnswer": "B",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2280,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Usando 'depends_on' na definição de cada tarefa",
      "B": "Não é possível",
      "C": "Através de variáveis de ambiente",
      "D": "Manualmente na UI"
    },
    "correctAnswer": "A",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2281,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "Deployment tradicional é mais rápido",
      "B": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "C": "Não há diferença",
      "D": "DAB é apenas para notebooks"
    },
    "correctAnswer": "B",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2282,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas clusters",
      "B": "Apenas permissões",
      "C": "Apenas notebooks",
      "D": "Arquivo databricks.yml com definições de recursos"
    },
    "correctAnswer": "D",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2283,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Usando o botão 'Repair and rerun' na UI do Job",
      "B": "Reiniciando o cluster",
      "C": "Reexecutando manualmente o notebook",
      "D": "Deletando e recriando o Job"
    },
    "correctAnswer": "A",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2284,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas documentação",
      "B": "Apenas segurança",
      "C": "Apenas performance",
      "D": "Versionamento, reprodutibilidade e automação de deployment"
    },
    "correctAnswer": "D",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2285,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Não é possível",
      "B": "Através de variáveis de ambiente",
      "C": "Manualmente na UI",
      "D": "Usando 'depends_on' na definição de cada tarefa"
    },
    "correctAnswer": "D",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2286,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "Deployment tradicional é mais rápido",
      "B": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "C": "DAB é apenas para notebooks",
      "D": "Não há diferença"
    },
    "correctAnswer": "B",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2287,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas permissões",
      "B": "Arquivo databricks.yml com definições de recursos",
      "C": "Apenas clusters",
      "D": "Apenas notebooks"
    },
    "correctAnswer": "B",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2288,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Reiniciando o cluster",
      "B": "Reexecutando manualmente o notebook",
      "C": "Usando o botão 'Repair and rerun' na UI do Job",
      "D": "Deletando e recriando o Job"
    },
    "correctAnswer": "C",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2289,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas segurança",
      "B": "Apenas documentação",
      "C": "Apenas performance",
      "D": "Versionamento, reprodutibilidade e automação de deployment"
    },
    "correctAnswer": "D",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2290,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Usando 'depends_on' na definição de cada tarefa",
      "B": "Através de variáveis de ambiente",
      "C": "Não é possível",
      "D": "Manualmente na UI"
    },
    "correctAnswer": "A",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2291,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "Não há diferença",
      "B": "Deployment tradicional é mais rápido",
      "C": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "D": "DAB é apenas para notebooks"
    },
    "correctAnswer": "C",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2292,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Arquivo databricks.yml com definições de recursos",
      "B": "Apenas clusters",
      "C": "Apenas notebooks",
      "D": "Apenas permissões"
    },
    "correctAnswer": "A",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2293,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Deletando e recriando o Job",
      "B": "Reexecutando manualmente o notebook",
      "C": "Reiniciando o cluster",
      "D": "Usando o botão 'Repair and rerun' na UI do Job"
    },
    "correctAnswer": "D",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2294,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas performance",
      "B": "Apenas documentação",
      "C": "Apenas segurança",
      "D": "Versionamento, reprodutibilidade e automação de deployment"
    },
    "correctAnswer": "D",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2295,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Usando 'depends_on' na definição de cada tarefa",
      "B": "Manualmente na UI",
      "C": "Através de variáveis de ambiente",
      "D": "Não é possível"
    },
    "correctAnswer": "A",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2296,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "DAB é apenas para notebooks",
      "B": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "C": "Deployment tradicional é mais rápido",
      "D": "Não há diferença"
    },
    "correctAnswer": "B",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2297,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas clusters",
      "B": "Apenas permissões",
      "C": "Arquivo databricks.yml com definições de recursos",
      "D": "Apenas notebooks"
    },
    "correctAnswer": "C",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2298,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Usando o botão 'Repair and rerun' na UI do Job",
      "B": "Deletando e recriando o Job",
      "C": "Reexecutando manualmente o notebook",
      "D": "Reiniciando o cluster"
    },
    "correctAnswer": "A",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2299,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Versionamento, reprodutibilidade e automação de deployment",
      "B": "Apenas segurança",
      "C": "Apenas documentação",
      "D": "Apenas performance"
    },
    "correctAnswer": "A",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2300,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Usando 'depends_on' na definição de cada tarefa",
      "B": "Através de variáveis de ambiente",
      "C": "Manualmente na UI",
      "D": "Não é possível"
    },
    "correctAnswer": "A",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2301,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "DAB é apenas para notebooks",
      "B": "Deployment tradicional é mais rápido",
      "C": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "D": "Não há diferença"
    },
    "correctAnswer": "C",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2302,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Arquivo databricks.yml com definições de recursos",
      "B": "Apenas permissões",
      "C": "Apenas notebooks",
      "D": "Apenas clusters"
    },
    "correctAnswer": "A",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2303,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Reexecutando manualmente o notebook",
      "B": "Usando o botão 'Repair and rerun' na UI do Job",
      "C": "Deletando e recriando o Job",
      "D": "Reiniciando o cluster"
    },
    "correctAnswer": "B",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2304,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas performance",
      "B": "Apenas segurança",
      "C": "Apenas documentação",
      "D": "Versionamento, reprodutibilidade e automação de deployment"
    },
    "correctAnswer": "D",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2305,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Manualmente na UI",
      "B": "Através de variáveis de ambiente",
      "C": "Não é possível",
      "D": "Usando 'depends_on' na definição de cada tarefa"
    },
    "correctAnswer": "D",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2306,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "Não há diferença",
      "B": "Deployment tradicional é mais rápido",
      "C": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "D": "DAB é apenas para notebooks"
    },
    "correctAnswer": "C",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2307,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas permissões",
      "B": "Arquivo databricks.yml com definições de recursos",
      "C": "Apenas clusters",
      "D": "Apenas notebooks"
    },
    "correctAnswer": "B",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2308,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Deletando e recriando o Job",
      "B": "Usando o botão 'Repair and rerun' na UI do Job",
      "C": "Reiniciando o cluster",
      "D": "Reexecutando manualmente o notebook"
    },
    "correctAnswer": "B",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2309,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Versionamento, reprodutibilidade e automação de deployment",
      "B": "Apenas segurança",
      "C": "Apenas documentação",
      "D": "Apenas performance"
    },
    "correctAnswer": "A",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2310,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Manualmente na UI",
      "B": "Através de variáveis de ambiente",
      "C": "Usando 'depends_on' na definição de cada tarefa",
      "D": "Não é possível"
    },
    "correctAnswer": "C",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2311,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "Deployment tradicional é mais rápido",
      "B": "DAB é apenas para notebooks",
      "C": "Não há diferença",
      "D": "DAB permite Infrastructure as Code; deployment tradicional é manual"
    },
    "correctAnswer": "D",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2312,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas permissões",
      "B": "Apenas notebooks",
      "C": "Arquivo databricks.yml com definições de recursos",
      "D": "Apenas clusters"
    },
    "correctAnswer": "C",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2313,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Reiniciando o cluster",
      "B": "Usando o botão 'Repair and rerun' na UI do Job",
      "C": "Reexecutando manualmente o notebook",
      "D": "Deletando e recriando o Job"
    },
    "correctAnswer": "B",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2314,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas segurança",
      "B": "Versionamento, reprodutibilidade e automação de deployment",
      "C": "Apenas performance",
      "D": "Apenas documentação"
    },
    "correctAnswer": "B",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2315,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Manualmente na UI",
      "B": "Através de variáveis de ambiente",
      "C": "Usando 'depends_on' na definição de cada tarefa",
      "D": "Não é possível"
    },
    "correctAnswer": "C",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2316,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "Não há diferença",
      "B": "DAB é apenas para notebooks",
      "C": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "D": "Deployment tradicional é mais rápido"
    },
    "correctAnswer": "C",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2317,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas notebooks",
      "B": "Arquivo databricks.yml com definições de recursos",
      "C": "Apenas clusters",
      "D": "Apenas permissões"
    },
    "correctAnswer": "B",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2318,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Reiniciando o cluster",
      "B": "Reexecutando manualmente o notebook",
      "C": "Deletando e recriando o Job",
      "D": "Usando o botão 'Repair and rerun' na UI do Job"
    },
    "correctAnswer": "D",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2319,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas segurança",
      "B": "Apenas documentação",
      "C": "Versionamento, reprodutibilidade e automação de deployment",
      "D": "Apenas performance"
    },
    "correctAnswer": "C",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2320,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Através de variáveis de ambiente",
      "B": "Manualmente na UI",
      "C": "Não é possível",
      "D": "Usando 'depends_on' na definição de cada tarefa"
    },
    "correctAnswer": "D",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2321,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "Não há diferença",
      "B": "Deployment tradicional é mais rápido",
      "C": "DAB é apenas para notebooks",
      "D": "DAB permite Infrastructure as Code; deployment tradicional é manual"
    },
    "correctAnswer": "D",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2322,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Arquivo databricks.yml com definições de recursos",
      "B": "Apenas permissões",
      "C": "Apenas clusters",
      "D": "Apenas notebooks"
    },
    "correctAnswer": "A",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2323,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Deletando e recriando o Job",
      "B": "Reexecutando manualmente o notebook",
      "C": "Reiniciando o cluster",
      "D": "Usando o botão 'Repair and rerun' na UI do Job"
    },
    "correctAnswer": "D",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2324,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Versionamento, reprodutibilidade e automação de deployment",
      "B": "Apenas segurança",
      "C": "Apenas performance",
      "D": "Apenas documentação"
    },
    "correctAnswer": "A",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2325,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Não é possível",
      "B": "Através de variáveis de ambiente",
      "C": "Manualmente na UI",
      "D": "Usando 'depends_on' na definição de cada tarefa"
    },
    "correctAnswer": "D",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2326,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "Não há diferença",
      "B": "Deployment tradicional é mais rápido",
      "C": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "D": "DAB é apenas para notebooks"
    },
    "correctAnswer": "C",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2327,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas notebooks",
      "B": "Apenas clusters",
      "C": "Arquivo databricks.yml com definições de recursos",
      "D": "Apenas permissões"
    },
    "correctAnswer": "C",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2328,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Reiniciando o cluster",
      "B": "Reexecutando manualmente o notebook",
      "C": "Deletando e recriando o Job",
      "D": "Usando o botão 'Repair and rerun' na UI do Job"
    },
    "correctAnswer": "D",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2329,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas documentação",
      "B": "Apenas performance",
      "C": "Apenas segurança",
      "D": "Versionamento, reprodutibilidade e automação de deployment"
    },
    "correctAnswer": "D",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2330,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Através de variáveis de ambiente",
      "B": "Usando 'depends_on' na definição de cada tarefa",
      "C": "Não é possível",
      "D": "Manualmente na UI"
    },
    "correctAnswer": "B",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2331,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "Não há diferença",
      "B": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "C": "DAB é apenas para notebooks",
      "D": "Deployment tradicional é mais rápido"
    },
    "correctAnswer": "B",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2332,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas notebooks",
      "B": "Arquivo databricks.yml com definições de recursos",
      "C": "Apenas clusters",
      "D": "Apenas permissões"
    },
    "correctAnswer": "B",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2333,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Deletando e recriando o Job",
      "B": "Usando o botão 'Repair and rerun' na UI do Job",
      "C": "Reexecutando manualmente o notebook",
      "D": "Reiniciando o cluster"
    },
    "correctAnswer": "B",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2334,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Apenas documentação",
      "B": "Versionamento, reprodutibilidade e automação de deployment",
      "C": "Apenas performance",
      "D": "Apenas segurança"
    },
    "correctAnswer": "B",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2335,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Manualmente na UI",
      "B": "Através de variáveis de ambiente",
      "C": "Não é possível",
      "D": "Usando 'depends_on' na definição de cada tarefa"
    },
    "correctAnswer": "D",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2336,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a principal diferença entre DAB e deployment tradicional?",
    "options": {
      "A": "Deployment tradicional é mais rápido",
      "B": "Não há diferença",
      "C": "DAB permite Infrastructure as Code; deployment tradicional é manual",
      "D": "DAB é apenas para notebooks"
    },
    "correctAnswer": "C",
    "rationale": "DAB permite definir pipelines, jobs e recursos em YAML/JSON e deployar como código, enquanto deployment tradicional é manual.",
    "tip": "DAB = Infrastructure as Code para Databricks."
  },
  {
    "id": 2337,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a estrutura básica de um Asset Bundle?",
    "options": {
      "A": "Apenas permissões",
      "B": "Arquivo databricks.yml com definições de recursos",
      "C": "Apenas clusters",
      "D": "Apenas notebooks"
    },
    "correctAnswer": "B",
    "rationale": "Um Asset Bundle é estruturado em torno de um arquivo databricks.yml que define todos os recursos (jobs, pipelines, permissões, etc.).",
    "tip": "databricks.yml = configuração central do bundle."
  },
  {
    "id": 2338,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Como você repara e reexecuta uma tarefa que falhou em um Job?",
    "options": {
      "A": "Deletando e recriando o Job",
      "B": "Reexecutando manualmente o notebook",
      "C": "Usando o botão 'Repair and rerun' na UI do Job",
      "D": "Reiniciando o cluster"
    },
    "correctAnswer": "C",
    "rationale": "O Databricks fornece a funcionalidade 'Repair and rerun' que permite reexecutar tarefas a partir do ponto de falha.",
    "tip": "Repair and rerun = recuperação de falhas sem reexecução completa."
  },
  {
    "id": 2339,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Qual é a vantagem de usar DAB em relação a deployment manual?",
    "options": {
      "A": "Versionamento, reprodutibilidade e automação de deployment",
      "B": "Apenas performance",
      "C": "Apenas documentação",
      "D": "Apenas segurança"
    },
    "correctAnswer": "A",
    "rationale": "DAB oferece versionamento de código, reprodutibilidade de deployments e automação completa do processo.",
    "tip": "DAB = versionamento + reprodutibilidade + automação."
  },
  {
    "id": 2340,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Como você define dependências entre tarefas em um Asset Bundle?",
    "options": {
      "A": "Não é possível",
      "B": "Manualmente na UI",
      "C": "Usando 'depends_on' na definição de cada tarefa",
      "D": "Através de variáveis de ambiente"
    },
    "correctAnswer": "C",
    "rationale": "Asset Bundles permitem definir dependências entre tarefas usando a propriedade 'depends_on' no arquivo YAML.",
    "tip": "depends_on = ordem de execução de tarefas."
  },
  {
    "id": 2341,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Gerenciadas são mais rápidas",
      "B": "Não há diferença",
      "C": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "D": "Externas não suportam Delta"
    },
    "correctAnswer": "C",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2342,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "B": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "C": "GRANT READ ON TABLE schema.table TO user@company.com",
      "D": "GRANT ACCESS ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "A",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2343,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas documentação",
      "B": "Rastrear origem, transformações e destino dos dados",
      "C": "Apenas permissões",
      "D": "Apenas performance"
    },
    "correctAnswer": "B",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2344,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Permite compartilhar dados com usuários externos sem copiar",
      "B": "Não funciona com Unity Catalog",
      "C": "Apenas para dados internos",
      "D": "Apenas copia dados"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2345,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Não há diferença",
      "B": "Databricks não suporta compartilhamento",
      "C": "Externo é mais rápido",
      "D": "Databricks: entre workspaces; Externo: com sistemas não-Databricks"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2346,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Externas não suportam Delta",
      "B": "Gerenciadas são mais rápidas",
      "C": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "D": "Não há diferença"
    },
    "correctAnswer": "C",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2347,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "B": "GRANT READ ON TABLE schema.table TO user@company.com",
      "C": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "D": "GRANT SELECT ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "D",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2348,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas documentação",
      "B": "Apenas performance",
      "C": "Rastrear origem, transformações e destino dos dados",
      "D": "Apenas permissões"
    },
    "correctAnswer": "C",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2349,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Permite compartilhar dados com usuários externos sem copiar",
      "B": "Apenas copia dados",
      "C": "Apenas para dados internos",
      "D": "Não funciona com Unity Catalog"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2350,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Não há diferença",
      "B": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "C": "Externo é mais rápido",
      "D": "Databricks não suporta compartilhamento"
    },
    "correctAnswer": "B",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2351,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Externas não suportam Delta",
      "B": "Não há diferença",
      "C": "Gerenciadas são mais rápidas",
      "D": "Gerenciadas: dados no workspace; Externas: dados em local externo"
    },
    "correctAnswer": "D",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2352,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT READ ON TABLE schema.table TO user@company.com",
      "B": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "C": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "D": "GRANT USAGE ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "C",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2353,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Rastrear origem, transformações e destino dos dados",
      "B": "Apenas documentação",
      "C": "Apenas performance",
      "D": "Apenas permissões"
    },
    "correctAnswer": "A",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2354,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Apenas para dados internos",
      "B": "Apenas copia dados",
      "C": "Não funciona com Unity Catalog",
      "D": "Permite compartilhar dados com usuários externos sem copiar"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2355,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Externo é mais rápido",
      "B": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "C": "Não há diferença",
      "D": "Databricks não suporta compartilhamento"
    },
    "correctAnswer": "B",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2356,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Gerenciadas são mais rápidas",
      "B": "Externas não suportam Delta",
      "C": "Não há diferença",
      "D": "Gerenciadas: dados no workspace; Externas: dados em local externo"
    },
    "correctAnswer": "D",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2357,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT READ ON TABLE schema.table TO user@company.com",
      "B": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "C": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "D": "GRANT USAGE ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "C",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2358,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas performance",
      "B": "Apenas permissões",
      "C": "Rastrear origem, transformações e destino dos dados",
      "D": "Apenas documentação"
    },
    "correctAnswer": "C",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2359,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Apenas para dados internos",
      "B": "Não funciona com Unity Catalog",
      "C": "Permite compartilhar dados com usuários externos sem copiar",
      "D": "Apenas copia dados"
    },
    "correctAnswer": "C",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2360,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Não há diferença",
      "B": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "C": "Databricks não suporta compartilhamento",
      "D": "Externo é mais rápido"
    },
    "correctAnswer": "B",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2361,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Não há diferença",
      "B": "Externas não suportam Delta",
      "C": "Gerenciadas são mais rápidas",
      "D": "Gerenciadas: dados no workspace; Externas: dados em local externo"
    },
    "correctAnswer": "D",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2362,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "B": "GRANT READ ON TABLE schema.table TO user@company.com",
      "C": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "D": "GRANT ACCESS ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "A",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2363,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Rastrear origem, transformações e destino dos dados",
      "B": "Apenas permissões",
      "C": "Apenas documentação",
      "D": "Apenas performance"
    },
    "correctAnswer": "A",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2364,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Não funciona com Unity Catalog",
      "B": "Apenas para dados internos",
      "C": "Permite compartilhar dados com usuários externos sem copiar",
      "D": "Apenas copia dados"
    },
    "correctAnswer": "C",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2365,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Não há diferença",
      "B": "Externo é mais rápido",
      "C": "Databricks não suporta compartilhamento",
      "D": "Databricks: entre workspaces; Externo: com sistemas não-Databricks"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2366,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Externas não suportam Delta",
      "B": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "C": "Não há diferença",
      "D": "Gerenciadas são mais rápidas"
    },
    "correctAnswer": "B",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2367,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "B": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "C": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "D": "GRANT READ ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "A",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2368,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas permissões",
      "B": "Apenas documentação",
      "C": "Apenas performance",
      "D": "Rastrear origem, transformações e destino dos dados"
    },
    "correctAnswer": "D",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2369,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Não funciona com Unity Catalog",
      "B": "Apenas copia dados",
      "C": "Apenas para dados internos",
      "D": "Permite compartilhar dados com usuários externos sem copiar"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2370,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Não há diferença",
      "B": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "C": "Databricks não suporta compartilhamento",
      "D": "Externo é mais rápido"
    },
    "correctAnswer": "B",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2371,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Não há diferença",
      "B": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "C": "Gerenciadas são mais rápidas",
      "D": "Externas não suportam Delta"
    },
    "correctAnswer": "B",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2372,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "B": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "C": "GRANT READ ON TABLE schema.table TO user@company.com",
      "D": "GRANT USAGE ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "A",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2373,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Rastrear origem, transformações e destino dos dados",
      "B": "Apenas performance",
      "C": "Apenas permissões",
      "D": "Apenas documentação"
    },
    "correctAnswer": "A",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2374,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Apenas copia dados",
      "B": "Não funciona com Unity Catalog",
      "C": "Apenas para dados internos",
      "D": "Permite compartilhar dados com usuários externos sem copiar"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2375,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "B": "Externo é mais rápido",
      "C": "Databricks não suporta compartilhamento",
      "D": "Não há diferença"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2376,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Não há diferença",
      "B": "Gerenciadas são mais rápidas",
      "C": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "D": "Externas não suportam Delta"
    },
    "correctAnswer": "C",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2377,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "B": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "C": "GRANT READ ON TABLE schema.table TO user@company.com",
      "D": "GRANT ACCESS ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "A",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2378,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Rastrear origem, transformações e destino dos dados",
      "B": "Apenas performance",
      "C": "Apenas permissões",
      "D": "Apenas documentação"
    },
    "correctAnswer": "A",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2379,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Apenas copia dados",
      "B": "Permite compartilhar dados com usuários externos sem copiar",
      "C": "Apenas para dados internos",
      "D": "Não funciona com Unity Catalog"
    },
    "correctAnswer": "B",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2380,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Databricks não suporta compartilhamento",
      "B": "Não há diferença",
      "C": "Externo é mais rápido",
      "D": "Databricks: entre workspaces; Externo: com sistemas não-Databricks"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2381,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Não há diferença",
      "B": "Gerenciadas são mais rápidas",
      "C": "Externas não suportam Delta",
      "D": "Gerenciadas: dados no workspace; Externas: dados em local externo"
    },
    "correctAnswer": "D",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2382,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "B": "GRANT READ ON TABLE schema.table TO user@company.com",
      "C": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "D": "GRANT ACCESS ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "C",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2383,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas documentação",
      "B": "Apenas permissões",
      "C": "Rastrear origem, transformações e destino dos dados",
      "D": "Apenas performance"
    },
    "correctAnswer": "C",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2384,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Permite compartilhar dados com usuários externos sem copiar",
      "B": "Não funciona com Unity Catalog",
      "C": "Apenas para dados internos",
      "D": "Apenas copia dados"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2385,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Não há diferença",
      "B": "Externo é mais rápido",
      "C": "Databricks não suporta compartilhamento",
      "D": "Databricks: entre workspaces; Externo: com sistemas não-Databricks"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2386,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Não há diferença",
      "B": "Externas não suportam Delta",
      "C": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "D": "Gerenciadas são mais rápidas"
    },
    "correctAnswer": "C",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2387,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "B": "GRANT READ ON TABLE schema.table TO user@company.com",
      "C": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "D": "GRANT USAGE ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "A",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2388,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas documentação",
      "B": "Apenas performance",
      "C": "Apenas permissões",
      "D": "Rastrear origem, transformações e destino dos dados"
    },
    "correctAnswer": "D",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2389,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Não funciona com Unity Catalog",
      "B": "Apenas copia dados",
      "C": "Permite compartilhar dados com usuários externos sem copiar",
      "D": "Apenas para dados internos"
    },
    "correctAnswer": "C",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2390,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Databricks não suporta compartilhamento",
      "B": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "C": "Não há diferença",
      "D": "Externo é mais rápido"
    },
    "correctAnswer": "B",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2391,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Não há diferença",
      "B": "Externas não suportam Delta",
      "C": "Gerenciadas são mais rápidas",
      "D": "Gerenciadas: dados no workspace; Externas: dados em local externo"
    },
    "correctAnswer": "D",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2392,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "B": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "C": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "D": "GRANT READ ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "C",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2393,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas performance",
      "B": "Rastrear origem, transformações e destino dos dados",
      "C": "Apenas permissões",
      "D": "Apenas documentação"
    },
    "correctAnswer": "B",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2394,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Permite compartilhar dados com usuários externos sem copiar",
      "B": "Apenas para dados internos",
      "C": "Apenas copia dados",
      "D": "Não funciona com Unity Catalog"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2395,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "B": "Externo é mais rápido",
      "C": "Databricks não suporta compartilhamento",
      "D": "Não há diferença"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2396,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Externas não suportam Delta",
      "B": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "C": "Não há diferença",
      "D": "Gerenciadas são mais rápidas"
    },
    "correctAnswer": "B",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2397,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "B": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "C": "GRANT READ ON TABLE schema.table TO user@company.com",
      "D": "GRANT SELECT ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "D",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2398,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas performance",
      "B": "Apenas documentação",
      "C": "Apenas permissões",
      "D": "Rastrear origem, transformações e destino dos dados"
    },
    "correctAnswer": "D",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2399,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Apenas copia dados",
      "B": "Não funciona com Unity Catalog",
      "C": "Permite compartilhar dados com usuários externos sem copiar",
      "D": "Apenas para dados internos"
    },
    "correctAnswer": "C",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2400,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Externo é mais rápido",
      "B": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "C": "Databricks não suporta compartilhamento",
      "D": "Não há diferença"
    },
    "correctAnswer": "B",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2401,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Externas não suportam Delta",
      "B": "Gerenciadas são mais rápidas",
      "C": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "D": "Não há diferença"
    },
    "correctAnswer": "C",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2402,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT READ ON TABLE schema.table TO user@company.com",
      "B": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "C": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "D": "GRANT ACCESS ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "C",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2403,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas performance",
      "B": "Apenas permissões",
      "C": "Apenas documentação",
      "D": "Rastrear origem, transformações e destino dos dados"
    },
    "correctAnswer": "D",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2404,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Apenas copia dados",
      "B": "Não funciona com Unity Catalog",
      "C": "Apenas para dados internos",
      "D": "Permite compartilhar dados com usuários externos sem copiar"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2405,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Externo é mais rápido",
      "B": "Não há diferença",
      "C": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "D": "Databricks não suporta compartilhamento"
    },
    "correctAnswer": "C",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2406,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Não há diferença",
      "B": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "C": "Gerenciadas são mais rápidas",
      "D": "Externas não suportam Delta"
    },
    "correctAnswer": "B",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2407,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "B": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "C": "GRANT READ ON TABLE schema.table TO user@company.com",
      "D": "GRANT ACCESS ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "A",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2408,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Rastrear origem, transformações e destino dos dados",
      "B": "Apenas permissões",
      "C": "Apenas documentação",
      "D": "Apenas performance"
    },
    "correctAnswer": "A",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2409,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Permite compartilhar dados com usuários externos sem copiar",
      "B": "Apenas copia dados",
      "C": "Apenas para dados internos",
      "D": "Não funciona com Unity Catalog"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2410,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Não há diferença",
      "B": "Externo é mais rápido",
      "C": "Databricks não suporta compartilhamento",
      "D": "Databricks: entre workspaces; Externo: com sistemas não-Databricks"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2411,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Não há diferença",
      "B": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "C": "Externas não suportam Delta",
      "D": "Gerenciadas são mais rápidas"
    },
    "correctAnswer": "B",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2412,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "B": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "C": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "D": "GRANT READ ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "C",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2413,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas documentação",
      "B": "Apenas permissões",
      "C": "Rastrear origem, transformações e destino dos dados",
      "D": "Apenas performance"
    },
    "correctAnswer": "C",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2414,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Permite compartilhar dados com usuários externos sem copiar",
      "B": "Apenas para dados internos",
      "C": "Apenas copia dados",
      "D": "Não funciona com Unity Catalog"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2415,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Externo é mais rápido",
      "B": "Não há diferença",
      "C": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "D": "Databricks não suporta compartilhamento"
    },
    "correctAnswer": "C",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2416,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Externas não suportam Delta",
      "B": "Não há diferença",
      "C": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "D": "Gerenciadas são mais rápidas"
    },
    "correctAnswer": "C",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2417,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT READ ON TABLE schema.table TO user@company.com",
      "B": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "C": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "D": "GRANT ACCESS ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "B",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2418,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas permissões",
      "B": "Rastrear origem, transformações e destino dos dados",
      "C": "Apenas documentação",
      "D": "Apenas performance"
    },
    "correctAnswer": "B",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2419,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Permite compartilhar dados com usuários externos sem copiar",
      "B": "Não funciona com Unity Catalog",
      "C": "Apenas para dados internos",
      "D": "Apenas copia dados"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2420,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Databricks não suporta compartilhamento",
      "B": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "C": "Externo é mais rápido",
      "D": "Não há diferença"
    },
    "correctAnswer": "B",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2421,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "B": "Gerenciadas são mais rápidas",
      "C": "Externas não suportam Delta",
      "D": "Não há diferença"
    },
    "correctAnswer": "A",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2422,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "B": "GRANT READ ON TABLE schema.table TO user@company.com",
      "C": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "D": "GRANT USAGE ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "A",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2423,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas documentação",
      "B": "Apenas permissões",
      "C": "Rastrear origem, transformações e destino dos dados",
      "D": "Apenas performance"
    },
    "correctAnswer": "C",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2424,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Apenas para dados internos",
      "B": "Apenas copia dados",
      "C": "Permite compartilhar dados com usuários externos sem copiar",
      "D": "Não funciona com Unity Catalog"
    },
    "correctAnswer": "C",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2425,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Externo é mais rápido",
      "B": "Databricks não suporta compartilhamento",
      "C": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "D": "Não há diferença"
    },
    "correctAnswer": "C",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2426,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Não há diferença",
      "B": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "C": "Gerenciadas são mais rápidas",
      "D": "Externas não suportam Delta"
    },
    "correctAnswer": "B",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2427,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT READ ON TABLE schema.table TO user@company.com",
      "B": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "C": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "D": "GRANT SELECT ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "D",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2428,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Rastrear origem, transformações e destino dos dados",
      "B": "Apenas permissões",
      "C": "Apenas documentação",
      "D": "Apenas performance"
    },
    "correctAnswer": "A",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2429,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Permite compartilhar dados com usuários externos sem copiar",
      "B": "Não funciona com Unity Catalog",
      "C": "Apenas copia dados",
      "D": "Apenas para dados internos"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2430,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Externo é mais rápido",
      "B": "Não há diferença",
      "C": "Databricks não suporta compartilhamento",
      "D": "Databricks: entre workspaces; Externo: com sistemas não-Databricks"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2431,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Externas não suportam Delta",
      "B": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "C": "Não há diferença",
      "D": "Gerenciadas são mais rápidas"
    },
    "correctAnswer": "B",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2432,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT READ ON TABLE schema.table TO user@company.com",
      "B": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "C": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "D": "GRANT ACCESS ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "C",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2433,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas permissões",
      "B": "Apenas performance",
      "C": "Rastrear origem, transformações e destino dos dados",
      "D": "Apenas documentação"
    },
    "correctAnswer": "C",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2434,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Apenas copia dados",
      "B": "Não funciona com Unity Catalog",
      "C": "Apenas para dados internos",
      "D": "Permite compartilhar dados com usuários externos sem copiar"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2435,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Não há diferença",
      "B": "Externo é mais rápido",
      "C": "Databricks não suporta compartilhamento",
      "D": "Databricks: entre workspaces; Externo: com sistemas não-Databricks"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2436,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "B": "Externas não suportam Delta",
      "C": "Não há diferença",
      "D": "Gerenciadas são mais rápidas"
    },
    "correctAnswer": "A",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2437,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "B": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "C": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "D": "GRANT READ ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "A",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2438,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas performance",
      "B": "Apenas documentação",
      "C": "Apenas permissões",
      "D": "Rastrear origem, transformações e destino dos dados"
    },
    "correctAnswer": "D",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2439,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Apenas para dados internos",
      "B": "Não funciona com Unity Catalog",
      "C": "Permite compartilhar dados com usuários externos sem copiar",
      "D": "Apenas copia dados"
    },
    "correctAnswer": "C",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2440,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "B": "Databricks não suporta compartilhamento",
      "C": "Não há diferença",
      "D": "Externo é mais rápido"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2441,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Gerenciadas são mais rápidas",
      "B": "Não há diferença",
      "C": "Externas não suportam Delta",
      "D": "Gerenciadas: dados no workspace; Externas: dados em local externo"
    },
    "correctAnswer": "D",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2442,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "B": "GRANT READ ON TABLE schema.table TO user@company.com",
      "C": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "D": "GRANT ACCESS ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "C",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2443,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Rastrear origem, transformações e destino dos dados",
      "B": "Apenas performance",
      "C": "Apenas permissões",
      "D": "Apenas documentação"
    },
    "correctAnswer": "A",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2444,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Não funciona com Unity Catalog",
      "B": "Apenas para dados internos",
      "C": "Permite compartilhar dados com usuários externos sem copiar",
      "D": "Apenas copia dados"
    },
    "correctAnswer": "C",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2445,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Não há diferença",
      "B": "Externo é mais rápido",
      "C": "Databricks não suporta compartilhamento",
      "D": "Databricks: entre workspaces; Externo: com sistemas não-Databricks"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2446,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Não há diferença",
      "B": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "C": "Externas não suportam Delta",
      "D": "Gerenciadas são mais rápidas"
    },
    "correctAnswer": "B",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2447,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "B": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "C": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "D": "GRANT READ ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "C",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2448,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas permissões",
      "B": "Apenas performance",
      "C": "Apenas documentação",
      "D": "Rastrear origem, transformações e destino dos dados"
    },
    "correctAnswer": "D",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2449,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Apenas copia dados",
      "B": "Apenas para dados internos",
      "C": "Não funciona com Unity Catalog",
      "D": "Permite compartilhar dados com usuários externos sem copiar"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2450,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "B": "Databricks não suporta compartilhamento",
      "C": "Não há diferença",
      "D": "Externo é mais rápido"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2451,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Gerenciadas são mais rápidas",
      "B": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "C": "Não há diferença",
      "D": "Externas não suportam Delta"
    },
    "correctAnswer": "B",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2452,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "B": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "C": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "D": "GRANT READ ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "A",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2453,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas permissões",
      "B": "Apenas documentação",
      "C": "Apenas performance",
      "D": "Rastrear origem, transformações e destino dos dados"
    },
    "correctAnswer": "D",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2454,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Permite compartilhar dados com usuários externos sem copiar",
      "B": "Apenas para dados internos",
      "C": "Não funciona com Unity Catalog",
      "D": "Apenas copia dados"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2455,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Databricks não suporta compartilhamento",
      "B": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "C": "Externo é mais rápido",
      "D": "Não há diferença"
    },
    "correctAnswer": "B",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2456,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Gerenciadas são mais rápidas",
      "B": "Não há diferença",
      "C": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "D": "Externas não suportam Delta"
    },
    "correctAnswer": "C",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2457,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "B": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "C": "GRANT READ ON TABLE schema.table TO user@company.com",
      "D": "GRANT ACCESS ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "B",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2458,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Rastrear origem, transformações e destino dos dados",
      "B": "Apenas permissões",
      "C": "Apenas documentação",
      "D": "Apenas performance"
    },
    "correctAnswer": "A",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2459,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Permite compartilhar dados com usuários externos sem copiar",
      "B": "Apenas para dados internos",
      "C": "Apenas copia dados",
      "D": "Não funciona com Unity Catalog"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2460,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "B": "Não há diferença",
      "C": "Externo é mais rápido",
      "D": "Databricks não suporta compartilhamento"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2461,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Gerenciadas são mais rápidas",
      "B": "Não há diferença",
      "C": "Externas não suportam Delta",
      "D": "Gerenciadas: dados no workspace; Externas: dados em local externo"
    },
    "correctAnswer": "D",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2462,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "B": "GRANT READ ON TABLE schema.table TO user@company.com",
      "C": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "D": "GRANT USAGE ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "C",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2463,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas performance",
      "B": "Apenas documentação",
      "C": "Apenas permissões",
      "D": "Rastrear origem, transformações e destino dos dados"
    },
    "correctAnswer": "D",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2464,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Não funciona com Unity Catalog",
      "B": "Permite compartilhar dados com usuários externos sem copiar",
      "C": "Apenas copia dados",
      "D": "Apenas para dados internos"
    },
    "correctAnswer": "B",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2465,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Databricks não suporta compartilhamento",
      "B": "Externo é mais rápido",
      "C": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "D": "Não há diferença"
    },
    "correctAnswer": "C",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2466,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Externas não suportam Delta",
      "B": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "C": "Gerenciadas são mais rápidas",
      "D": "Não há diferença"
    },
    "correctAnswer": "B",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2467,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "B": "GRANT READ ON TABLE schema.table TO user@company.com",
      "C": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "D": "GRANT USAGE ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "C",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2468,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas permissões",
      "B": "Rastrear origem, transformações e destino dos dados",
      "C": "Apenas documentação",
      "D": "Apenas performance"
    },
    "correctAnswer": "B",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2469,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Apenas para dados internos",
      "B": "Não funciona com Unity Catalog",
      "C": "Permite compartilhar dados com usuários externos sem copiar",
      "D": "Apenas copia dados"
    },
    "correctAnswer": "C",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2470,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Externo é mais rápido",
      "B": "Não há diferença",
      "C": "Databricks não suporta compartilhamento",
      "D": "Databricks: entre workspaces; Externo: com sistemas não-Databricks"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2471,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Gerenciadas são mais rápidas",
      "B": "Externas não suportam Delta",
      "C": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "D": "Não há diferença"
    },
    "correctAnswer": "C",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2472,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT READ ON TABLE schema.table TO user@company.com",
      "B": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "C": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "D": "GRANT ACCESS ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "B",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2473,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas permissões",
      "B": "Apenas performance",
      "C": "Rastrear origem, transformações e destino dos dados",
      "D": "Apenas documentação"
    },
    "correctAnswer": "C",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2474,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Apenas copia dados",
      "B": "Não funciona com Unity Catalog",
      "C": "Apenas para dados internos",
      "D": "Permite compartilhar dados com usuários externos sem copiar"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2475,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Não há diferença",
      "B": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "C": "Databricks não suporta compartilhamento",
      "D": "Externo é mais rápido"
    },
    "correctAnswer": "B",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2476,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "B": "Gerenciadas são mais rápidas",
      "C": "Externas não suportam Delta",
      "D": "Não há diferença"
    },
    "correctAnswer": "A",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2477,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "B": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "C": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "D": "GRANT READ ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "C",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2478,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas permissões",
      "B": "Apenas performance",
      "C": "Rastrear origem, transformações e destino dos dados",
      "D": "Apenas documentação"
    },
    "correctAnswer": "C",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2479,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Apenas copia dados",
      "B": "Apenas para dados internos",
      "C": "Permite compartilhar dados com usuários externos sem copiar",
      "D": "Não funciona com Unity Catalog"
    },
    "correctAnswer": "C",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2480,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Não há diferença",
      "B": "Externo é mais rápido",
      "C": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "D": "Databricks não suporta compartilhamento"
    },
    "correctAnswer": "C",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2481,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "B": "Não há diferença",
      "C": "Gerenciadas são mais rápidas",
      "D": "Externas não suportam Delta"
    },
    "correctAnswer": "A",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2482,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "B": "GRANT READ ON TABLE schema.table TO user@company.com",
      "C": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "D": "GRANT SELECT ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "D",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2483,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas documentação",
      "B": "Rastrear origem, transformações e destino dos dados",
      "C": "Apenas performance",
      "D": "Apenas permissões"
    },
    "correctAnswer": "B",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2484,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Permite compartilhar dados com usuários externos sem copiar",
      "B": "Não funciona com Unity Catalog",
      "C": "Apenas para dados internos",
      "D": "Apenas copia dados"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2485,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Externo é mais rápido",
      "B": "Databricks não suporta compartilhamento",
      "C": "Não há diferença",
      "D": "Databricks: entre workspaces; Externo: com sistemas não-Databricks"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2486,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Não há diferença",
      "B": "Externas não suportam Delta",
      "C": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "D": "Gerenciadas são mais rápidas"
    },
    "correctAnswer": "C",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2487,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT READ ON TABLE schema.table TO user@company.com",
      "B": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "C": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "D": "GRANT SELECT ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "D",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2488,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas documentação",
      "B": "Apenas permissões",
      "C": "Rastrear origem, transformações e destino dos dados",
      "D": "Apenas performance"
    },
    "correctAnswer": "C",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2489,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Apenas para dados internos",
      "B": "Permite compartilhar dados com usuários externos sem copiar",
      "C": "Não funciona com Unity Catalog",
      "D": "Apenas copia dados"
    },
    "correctAnswer": "B",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2490,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "B": "Externo é mais rápido",
      "C": "Não há diferença",
      "D": "Databricks não suporta compartilhamento"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2491,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Gerenciadas são mais rápidas",
      "B": "Não há diferença",
      "C": "Externas não suportam Delta",
      "D": "Gerenciadas: dados no workspace; Externas: dados em local externo"
    },
    "correctAnswer": "D",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2492,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "B": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "C": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "D": "GRANT READ ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "A",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2493,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas documentação",
      "B": "Apenas performance",
      "C": "Rastrear origem, transformações e destino dos dados",
      "D": "Apenas permissões"
    },
    "correctAnswer": "C",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2494,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Apenas copia dados",
      "B": "Não funciona com Unity Catalog",
      "C": "Apenas para dados internos",
      "D": "Permite compartilhar dados com usuários externos sem copiar"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2495,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Externo é mais rápido",
      "B": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "C": "Não há diferença",
      "D": "Databricks não suporta compartilhamento"
    },
    "correctAnswer": "B",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2496,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Gerenciadas são mais rápidas",
      "B": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "C": "Externas não suportam Delta",
      "D": "Não há diferença"
    },
    "correctAnswer": "B",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2497,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "B": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "C": "GRANT READ ON TABLE schema.table TO user@company.com",
      "D": "GRANT SELECT ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "D",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2498,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas performance",
      "B": "Apenas permissões",
      "C": "Rastrear origem, transformações e destino dos dados",
      "D": "Apenas documentação"
    },
    "correctAnswer": "C",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2499,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Permite compartilhar dados com usuários externos sem copiar",
      "B": "Apenas para dados internos",
      "C": "Não funciona com Unity Catalog",
      "D": "Apenas copia dados"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2500,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Externo é mais rápido",
      "B": "Não há diferença",
      "C": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "D": "Databricks não suporta compartilhamento"
    },
    "correctAnswer": "C",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2501,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Não há diferença",
      "B": "Externas não suportam Delta",
      "C": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "D": "Gerenciadas são mais rápidas"
    },
    "correctAnswer": "C",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2502,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "B": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "C": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "D": "GRANT READ ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "B",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2503,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas documentação",
      "B": "Apenas performance",
      "C": "Apenas permissões",
      "D": "Rastrear origem, transformações e destino dos dados"
    },
    "correctAnswer": "D",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2504,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Permite compartilhar dados com usuários externos sem copiar",
      "B": "Apenas para dados internos",
      "C": "Apenas copia dados",
      "D": "Não funciona com Unity Catalog"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2505,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Externo é mais rápido",
      "B": "Databricks não suporta compartilhamento",
      "C": "Não há diferença",
      "D": "Databricks: entre workspaces; Externo: com sistemas não-Databricks"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2506,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Gerenciadas são mais rápidas",
      "B": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "C": "Externas não suportam Delta",
      "D": "Não há diferença"
    },
    "correctAnswer": "B",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2507,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "B": "GRANT READ ON TABLE schema.table TO user@company.com",
      "C": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "D": "GRANT USAGE ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "A",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2508,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas permissões",
      "B": "Apenas performance",
      "C": "Rastrear origem, transformações e destino dos dados",
      "D": "Apenas documentação"
    },
    "correctAnswer": "C",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2509,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Não funciona com Unity Catalog",
      "B": "Permite compartilhar dados com usuários externos sem copiar",
      "C": "Apenas copia dados",
      "D": "Apenas para dados internos"
    },
    "correctAnswer": "B",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2510,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Databricks não suporta compartilhamento",
      "B": "Externo é mais rápido",
      "C": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "D": "Não há diferença"
    },
    "correctAnswer": "C",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2511,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Gerenciadas são mais rápidas",
      "B": "Externas não suportam Delta",
      "C": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "D": "Não há diferença"
    },
    "correctAnswer": "C",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2512,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "B": "GRANT READ ON TABLE schema.table TO user@company.com",
      "C": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "D": "GRANT USAGE ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "A",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2513,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas performance",
      "B": "Apenas permissões",
      "C": "Rastrear origem, transformações e destino dos dados",
      "D": "Apenas documentação"
    },
    "correctAnswer": "C",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2514,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Permite compartilhar dados com usuários externos sem copiar",
      "B": "Apenas copia dados",
      "C": "Apenas para dados internos",
      "D": "Não funciona com Unity Catalog"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2515,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "B": "Externo é mais rápido",
      "C": "Não há diferença",
      "D": "Databricks não suporta compartilhamento"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2516,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Não há diferença",
      "B": "Gerenciadas são mais rápidas",
      "C": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "D": "Externas não suportam Delta"
    },
    "correctAnswer": "C",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2517,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "B": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "C": "GRANT READ ON TABLE schema.table TO user@company.com",
      "D": "GRANT USAGE ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "A",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2518,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas permissões",
      "B": "Apenas documentação",
      "C": "Apenas performance",
      "D": "Rastrear origem, transformações e destino dos dados"
    },
    "correctAnswer": "D",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2519,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Não funciona com Unity Catalog",
      "B": "Apenas copia dados",
      "C": "Permite compartilhar dados com usuários externos sem copiar",
      "D": "Apenas para dados internos"
    },
    "correctAnswer": "C",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2520,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Databricks não suporta compartilhamento",
      "B": "Externo é mais rápido",
      "C": "Não há diferença",
      "D": "Databricks: entre workspaces; Externo: com sistemas não-Databricks"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2521,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "B": "Não há diferença",
      "C": "Externas não suportam Delta",
      "D": "Gerenciadas são mais rápidas"
    },
    "correctAnswer": "A",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2522,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "B": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "C": "GRANT READ ON TABLE schema.table TO user@company.com",
      "D": "GRANT ACCESS ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "A",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2523,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas permissões",
      "B": "Apenas documentação",
      "C": "Rastrear origem, transformações e destino dos dados",
      "D": "Apenas performance"
    },
    "correctAnswer": "C",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2524,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Permite compartilhar dados com usuários externos sem copiar",
      "B": "Apenas para dados internos",
      "C": "Apenas copia dados",
      "D": "Não funciona com Unity Catalog"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2525,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Databricks não suporta compartilhamento",
      "B": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "C": "Externo é mais rápido",
      "D": "Não há diferença"
    },
    "correctAnswer": "B",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2526,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Não há diferença",
      "B": "Gerenciadas são mais rápidas",
      "C": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "D": "Externas não suportam Delta"
    },
    "correctAnswer": "C",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2527,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "B": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "C": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "D": "GRANT READ ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "C",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2528,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Rastrear origem, transformações e destino dos dados",
      "B": "Apenas performance",
      "C": "Apenas documentação",
      "D": "Apenas permissões"
    },
    "correctAnswer": "A",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2529,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Permite compartilhar dados com usuários externos sem copiar",
      "B": "Apenas para dados internos",
      "C": "Apenas copia dados",
      "D": "Não funciona com Unity Catalog"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2530,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Não há diferença",
      "B": "Databricks não suporta compartilhamento",
      "C": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "D": "Externo é mais rápido"
    },
    "correctAnswer": "C",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2531,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "B": "Não há diferença",
      "C": "Gerenciadas são mais rápidas",
      "D": "Externas não suportam Delta"
    },
    "correctAnswer": "A",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2532,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "B": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "C": "GRANT READ ON TABLE schema.table TO user@company.com",
      "D": "GRANT SELECT ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "D",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2533,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas permissões",
      "B": "Apenas performance",
      "C": "Apenas documentação",
      "D": "Rastrear origem, transformações e destino dos dados"
    },
    "correctAnswer": "D",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2534,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Permite compartilhar dados com usuários externos sem copiar",
      "B": "Apenas copia dados",
      "C": "Não funciona com Unity Catalog",
      "D": "Apenas para dados internos"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2535,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Externo é mais rápido",
      "B": "Não há diferença",
      "C": "Databricks não suporta compartilhamento",
      "D": "Databricks: entre workspaces; Externo: com sistemas não-Databricks"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2536,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Não há diferença",
      "B": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "C": "Externas não suportam Delta",
      "D": "Gerenciadas são mais rápidas"
    },
    "correctAnswer": "B",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2537,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "B": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "C": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "D": "GRANT READ ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "B",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2538,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas performance",
      "B": "Apenas documentação",
      "C": "Apenas permissões",
      "D": "Rastrear origem, transformações e destino dos dados"
    },
    "correctAnswer": "D",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2539,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Apenas para dados internos",
      "B": "Não funciona com Unity Catalog",
      "C": "Apenas copia dados",
      "D": "Permite compartilhar dados com usuários externos sem copiar"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2540,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Não há diferença",
      "B": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "C": "Databricks não suporta compartilhamento",
      "D": "Externo é mais rápido"
    },
    "correctAnswer": "B",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2541,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Externas não suportam Delta",
      "B": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "C": "Não há diferença",
      "D": "Gerenciadas são mais rápidas"
    },
    "correctAnswer": "B",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2542,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT READ ON TABLE schema.table TO user@company.com",
      "B": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "C": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "D": "GRANT USAGE ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "C",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2543,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas documentação",
      "B": "Apenas permissões",
      "C": "Rastrear origem, transformações e destino dos dados",
      "D": "Apenas performance"
    },
    "correctAnswer": "C",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2544,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Apenas copia dados",
      "B": "Apenas para dados internos",
      "C": "Não funciona com Unity Catalog",
      "D": "Permite compartilhar dados com usuários externos sem copiar"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2545,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Externo é mais rápido",
      "B": "Não há diferença",
      "C": "Databricks não suporta compartilhamento",
      "D": "Databricks: entre workspaces; Externo: com sistemas não-Databricks"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2546,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Externas não suportam Delta",
      "B": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "C": "Gerenciadas são mais rápidas",
      "D": "Não há diferença"
    },
    "correctAnswer": "B",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2547,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT READ ON TABLE schema.table TO user@company.com",
      "B": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "C": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "D": "GRANT USAGE ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "B",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2548,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas performance",
      "B": "Apenas permissões",
      "C": "Rastrear origem, transformações e destino dos dados",
      "D": "Apenas documentação"
    },
    "correctAnswer": "C",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2549,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Permite compartilhar dados com usuários externos sem copiar",
      "B": "Apenas para dados internos",
      "C": "Não funciona com Unity Catalog",
      "D": "Apenas copia dados"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2550,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "B": "Externo é mais rápido",
      "C": "Não há diferença",
      "D": "Databricks não suporta compartilhamento"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2551,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "B": "Não há diferença",
      "C": "Externas não suportam Delta",
      "D": "Gerenciadas são mais rápidas"
    },
    "correctAnswer": "A",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2552,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "B": "GRANT READ ON TABLE schema.table TO user@company.com",
      "C": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "D": "GRANT USAGE ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "C",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2553,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas documentação",
      "B": "Rastrear origem, transformações e destino dos dados",
      "C": "Apenas permissões",
      "D": "Apenas performance"
    },
    "correctAnswer": "B",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2554,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Apenas copia dados",
      "B": "Permite compartilhar dados com usuários externos sem copiar",
      "C": "Não funciona com Unity Catalog",
      "D": "Apenas para dados internos"
    },
    "correctAnswer": "B",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2555,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Não há diferença",
      "B": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "C": "Databricks não suporta compartilhamento",
      "D": "Externo é mais rápido"
    },
    "correctAnswer": "B",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2556,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Externas não suportam Delta",
      "B": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "C": "Não há diferença",
      "D": "Gerenciadas são mais rápidas"
    },
    "correctAnswer": "B",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2557,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "B": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "C": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "D": "GRANT READ ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "B",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2558,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas performance",
      "B": "Apenas permissões",
      "C": "Apenas documentação",
      "D": "Rastrear origem, transformações e destino dos dados"
    },
    "correctAnswer": "D",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2559,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Apenas copia dados",
      "B": "Não funciona com Unity Catalog",
      "C": "Apenas para dados internos",
      "D": "Permite compartilhar dados com usuários externos sem copiar"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2560,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "B": "Externo é mais rápido",
      "C": "Não há diferença",
      "D": "Databricks não suporta compartilhamento"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2561,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Gerenciadas são mais rápidas",
      "B": "Externas não suportam Delta",
      "C": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "D": "Não há diferença"
    },
    "correctAnswer": "C",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2562,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "B": "GRANT READ ON TABLE schema.table TO user@company.com",
      "C": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "D": "GRANT SELECT ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "D",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2563,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Rastrear origem, transformações e destino dos dados",
      "B": "Apenas documentação",
      "C": "Apenas performance",
      "D": "Apenas permissões"
    },
    "correctAnswer": "A",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2564,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Apenas para dados internos",
      "B": "Não funciona com Unity Catalog",
      "C": "Apenas copia dados",
      "D": "Permite compartilhar dados com usuários externos sem copiar"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2565,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Externo é mais rápido",
      "B": "Databricks não suporta compartilhamento",
      "C": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "D": "Não há diferença"
    },
    "correctAnswer": "C",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2566,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Gerenciadas são mais rápidas",
      "B": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "C": "Externas não suportam Delta",
      "D": "Não há diferença"
    },
    "correctAnswer": "B",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2567,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "B": "GRANT READ ON TABLE schema.table TO user@company.com",
      "C": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "D": "GRANT SELECT ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "D",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2568,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas performance",
      "B": "Apenas documentação",
      "C": "Rastrear origem, transformações e destino dos dados",
      "D": "Apenas permissões"
    },
    "correctAnswer": "C",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2569,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Não funciona com Unity Catalog",
      "B": "Permite compartilhar dados com usuários externos sem copiar",
      "C": "Apenas copia dados",
      "D": "Apenas para dados internos"
    },
    "correctAnswer": "B",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2570,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Não há diferença",
      "B": "Externo é mais rápido",
      "C": "Databricks não suporta compartilhamento",
      "D": "Databricks: entre workspaces; Externo: com sistemas não-Databricks"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2571,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Não há diferença",
      "B": "Externas não suportam Delta",
      "C": "Gerenciadas são mais rápidas",
      "D": "Gerenciadas: dados no workspace; Externas: dados em local externo"
    },
    "correctAnswer": "D",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2572,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "B": "GRANT READ ON TABLE schema.table TO user@company.com",
      "C": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "D": "GRANT USAGE ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "A",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2573,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas permissões",
      "B": "Apenas performance",
      "C": "Rastrear origem, transformações e destino dos dados",
      "D": "Apenas documentação"
    },
    "correctAnswer": "C",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2574,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Não funciona com Unity Catalog",
      "B": "Apenas para dados internos",
      "C": "Apenas copia dados",
      "D": "Permite compartilhar dados com usuários externos sem copiar"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2575,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Externo é mais rápido",
      "B": "Databricks não suporta compartilhamento",
      "C": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "D": "Não há diferença"
    },
    "correctAnswer": "C",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2576,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "B": "Externas não suportam Delta",
      "C": "Gerenciadas são mais rápidas",
      "D": "Não há diferença"
    },
    "correctAnswer": "A",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2577,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "B": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "C": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "D": "GRANT READ ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "C",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2578,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas performance",
      "B": "Apenas documentação",
      "C": "Apenas permissões",
      "D": "Rastrear origem, transformações e destino dos dados"
    },
    "correctAnswer": "D",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2579,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Apenas para dados internos",
      "B": "Não funciona com Unity Catalog",
      "C": "Permite compartilhar dados com usuários externos sem copiar",
      "D": "Apenas copia dados"
    },
    "correctAnswer": "C",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2580,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "B": "Externo é mais rápido",
      "C": "Não há diferença",
      "D": "Databricks não suporta compartilhamento"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2581,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Gerenciadas são mais rápidas",
      "B": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "C": "Não há diferença",
      "D": "Externas não suportam Delta"
    },
    "correctAnswer": "B",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2582,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "B": "GRANT READ ON TABLE schema.table TO user@company.com",
      "C": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "D": "GRANT ACCESS ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "C",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2583,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas performance",
      "B": "Rastrear origem, transformações e destino dos dados",
      "C": "Apenas permissões",
      "D": "Apenas documentação"
    },
    "correctAnswer": "B",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2584,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Apenas para dados internos",
      "B": "Não funciona com Unity Catalog",
      "C": "Permite compartilhar dados com usuários externos sem copiar",
      "D": "Apenas copia dados"
    },
    "correctAnswer": "C",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2585,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Externo é mais rápido",
      "B": "Databricks não suporta compartilhamento",
      "C": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "D": "Não há diferença"
    },
    "correctAnswer": "C",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2586,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Gerenciadas são mais rápidas",
      "B": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "C": "Externas não suportam Delta",
      "D": "Não há diferença"
    },
    "correctAnswer": "B",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2587,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "B": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "C": "GRANT READ ON TABLE schema.table TO user@company.com",
      "D": "GRANT ACCESS ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "B",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2588,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas documentação",
      "B": "Apenas permissões",
      "C": "Rastrear origem, transformações e destino dos dados",
      "D": "Apenas performance"
    },
    "correctAnswer": "C",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2589,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Não funciona com Unity Catalog",
      "B": "Permite compartilhar dados com usuários externos sem copiar",
      "C": "Apenas para dados internos",
      "D": "Apenas copia dados"
    },
    "correctAnswer": "B",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2590,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Externo é mais rápido",
      "B": "Databricks não suporta compartilhamento",
      "C": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "D": "Não há diferença"
    },
    "correctAnswer": "C",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2591,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Externas não suportam Delta",
      "B": "Não há diferença",
      "C": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "D": "Gerenciadas são mais rápidas"
    },
    "correctAnswer": "C",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2592,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "B": "GRANT READ ON TABLE schema.table TO user@company.com",
      "C": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "D": "GRANT USAGE ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "C",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2593,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas documentação",
      "B": "Apenas performance",
      "C": "Apenas permissões",
      "D": "Rastrear origem, transformações e destino dos dados"
    },
    "correctAnswer": "D",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2594,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Apenas para dados internos",
      "B": "Permite compartilhar dados com usuários externos sem copiar",
      "C": "Apenas copia dados",
      "D": "Não funciona com Unity Catalog"
    },
    "correctAnswer": "B",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2595,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Não há diferença",
      "B": "Externo é mais rápido",
      "C": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "D": "Databricks não suporta compartilhamento"
    },
    "correctAnswer": "C",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2596,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Não há diferença",
      "B": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "C": "Externas não suportam Delta",
      "D": "Gerenciadas são mais rápidas"
    },
    "correctAnswer": "B",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2597,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "B": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "C": "GRANT READ ON TABLE schema.table TO user@company.com",
      "D": "GRANT ACCESS ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "A",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2598,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas documentação",
      "B": "Rastrear origem, transformações e destino dos dados",
      "C": "Apenas performance",
      "D": "Apenas permissões"
    },
    "correctAnswer": "B",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2599,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Apenas copia dados",
      "B": "Permite compartilhar dados com usuários externos sem copiar",
      "C": "Não funciona com Unity Catalog",
      "D": "Apenas para dados internos"
    },
    "correctAnswer": "B",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2600,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Databricks não suporta compartilhamento",
      "B": "Não há diferença",
      "C": "Externo é mais rápido",
      "D": "Databricks: entre workspaces; Externo: com sistemas não-Databricks"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2601,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Não há diferença",
      "B": "Gerenciadas são mais rápidas",
      "C": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "D": "Externas não suportam Delta"
    },
    "correctAnswer": "C",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2602,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT READ ON TABLE schema.table TO user@company.com",
      "B": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "C": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "D": "GRANT USAGE ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "C",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2603,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas performance",
      "B": "Apenas permissões",
      "C": "Rastrear origem, transformações e destino dos dados",
      "D": "Apenas documentação"
    },
    "correctAnswer": "C",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2604,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Apenas para dados internos",
      "B": "Apenas copia dados",
      "C": "Não funciona com Unity Catalog",
      "D": "Permite compartilhar dados com usuários externos sem copiar"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2605,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Não há diferença",
      "B": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "C": "Databricks não suporta compartilhamento",
      "D": "Externo é mais rápido"
    },
    "correctAnswer": "B",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2606,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Não há diferença",
      "B": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "C": "Externas não suportam Delta",
      "D": "Gerenciadas são mais rápidas"
    },
    "correctAnswer": "B",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2607,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "B": "GRANT READ ON TABLE schema.table TO user@company.com",
      "C": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "D": "GRANT SELECT ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "D",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2608,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas documentação",
      "B": "Apenas permissões",
      "C": "Rastrear origem, transformações e destino dos dados",
      "D": "Apenas performance"
    },
    "correctAnswer": "C",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2609,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Apenas para dados internos",
      "B": "Não funciona com Unity Catalog",
      "C": "Apenas copia dados",
      "D": "Permite compartilhar dados com usuários externos sem copiar"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2610,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Não há diferença",
      "B": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "C": "Externo é mais rápido",
      "D": "Databricks não suporta compartilhamento"
    },
    "correctAnswer": "B",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2611,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Não há diferença",
      "B": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "C": "Gerenciadas são mais rápidas",
      "D": "Externas não suportam Delta"
    },
    "correctAnswer": "B",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2612,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "B": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "C": "GRANT READ ON TABLE schema.table TO user@company.com",
      "D": "GRANT SELECT ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "D",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2613,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Rastrear origem, transformações e destino dos dados",
      "B": "Apenas permissões",
      "C": "Apenas performance",
      "D": "Apenas documentação"
    },
    "correctAnswer": "A",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2614,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Apenas para dados internos",
      "B": "Não funciona com Unity Catalog",
      "C": "Apenas copia dados",
      "D": "Permite compartilhar dados com usuários externos sem copiar"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2615,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Não há diferença",
      "B": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "C": "Databricks não suporta compartilhamento",
      "D": "Externo é mais rápido"
    },
    "correctAnswer": "B",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2616,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "B": "Gerenciadas são mais rápidas",
      "C": "Não há diferença",
      "D": "Externas não suportam Delta"
    },
    "correctAnswer": "A",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2617,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "B": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "C": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "D": "GRANT READ ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "A",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2618,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas permissões",
      "B": "Apenas documentação",
      "C": "Rastrear origem, transformações e destino dos dados",
      "D": "Apenas performance"
    },
    "correctAnswer": "C",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2619,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Permite compartilhar dados com usuários externos sem copiar",
      "B": "Não funciona com Unity Catalog",
      "C": "Apenas copia dados",
      "D": "Apenas para dados internos"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2620,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "B": "Não há diferença",
      "C": "Databricks não suporta compartilhamento",
      "D": "Externo é mais rápido"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2621,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "B": "Não há diferença",
      "C": "Gerenciadas são mais rápidas",
      "D": "Externas não suportam Delta"
    },
    "correctAnswer": "A",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2622,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "B": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "C": "GRANT READ ON TABLE schema.table TO user@company.com",
      "D": "GRANT SELECT ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "D",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2623,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas permissões",
      "B": "Apenas documentação",
      "C": "Rastrear origem, transformações e destino dos dados",
      "D": "Apenas performance"
    },
    "correctAnswer": "C",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2624,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Apenas copia dados",
      "B": "Apenas para dados internos",
      "C": "Não funciona com Unity Catalog",
      "D": "Permite compartilhar dados com usuários externos sem copiar"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2625,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Databricks não suporta compartilhamento",
      "B": "Não há diferença",
      "C": "Externo é mais rápido",
      "D": "Databricks: entre workspaces; Externo: com sistemas não-Databricks"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2626,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Não há diferença",
      "B": "Gerenciadas são mais rápidas",
      "C": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "D": "Externas não suportam Delta"
    },
    "correctAnswer": "C",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2627,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "B": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "C": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "D": "GRANT READ ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "C",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2628,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas permissões",
      "B": "Apenas performance",
      "C": "Rastrear origem, transformações e destino dos dados",
      "D": "Apenas documentação"
    },
    "correctAnswer": "C",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2629,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Apenas para dados internos",
      "B": "Apenas copia dados",
      "C": "Permite compartilhar dados com usuários externos sem copiar",
      "D": "Não funciona com Unity Catalog"
    },
    "correctAnswer": "C",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2630,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Não há diferença",
      "B": "Externo é mais rápido",
      "C": "Databricks não suporta compartilhamento",
      "D": "Databricks: entre workspaces; Externo: com sistemas não-Databricks"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2631,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Externas não suportam Delta",
      "B": "Gerenciadas são mais rápidas",
      "C": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "D": "Não há diferença"
    },
    "correctAnswer": "C",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2632,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT READ ON TABLE schema.table TO user@company.com",
      "B": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "C": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "D": "GRANT SELECT ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "D",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2633,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas documentação",
      "B": "Rastrear origem, transformações e destino dos dados",
      "C": "Apenas permissões",
      "D": "Apenas performance"
    },
    "correctAnswer": "B",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2634,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Permite compartilhar dados com usuários externos sem copiar",
      "B": "Apenas para dados internos",
      "C": "Não funciona com Unity Catalog",
      "D": "Apenas copia dados"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2635,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "B": "Não há diferença",
      "C": "Externo é mais rápido",
      "D": "Databricks não suporta compartilhamento"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2636,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Gerenciadas são mais rápidas",
      "B": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "C": "Não há diferença",
      "D": "Externas não suportam Delta"
    },
    "correctAnswer": "B",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2637,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "B": "GRANT READ ON TABLE schema.table TO user@company.com",
      "C": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "D": "GRANT ACCESS ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "C",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2638,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Rastrear origem, transformações e destino dos dados",
      "B": "Apenas performance",
      "C": "Apenas documentação",
      "D": "Apenas permissões"
    },
    "correctAnswer": "A",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2639,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Permite compartilhar dados com usuários externos sem copiar",
      "B": "Não funciona com Unity Catalog",
      "C": "Apenas para dados internos",
      "D": "Apenas copia dados"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2640,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Não há diferença",
      "B": "Externo é mais rápido",
      "C": "Databricks não suporta compartilhamento",
      "D": "Databricks: entre workspaces; Externo: com sistemas não-Databricks"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2641,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Externas não suportam Delta",
      "B": "Não há diferença",
      "C": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "D": "Gerenciadas são mais rápidas"
    },
    "correctAnswer": "C",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2642,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT READ ON TABLE schema.table TO user@company.com",
      "B": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "C": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "D": "GRANT SELECT ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "D",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2643,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Rastrear origem, transformações e destino dos dados",
      "B": "Apenas documentação",
      "C": "Apenas performance",
      "D": "Apenas permissões"
    },
    "correctAnswer": "A",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2644,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Permite compartilhar dados com usuários externos sem copiar",
      "B": "Apenas copia dados",
      "C": "Apenas para dados internos",
      "D": "Não funciona com Unity Catalog"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2645,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Não há diferença",
      "B": "Externo é mais rápido",
      "C": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "D": "Databricks não suporta compartilhamento"
    },
    "correctAnswer": "C",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2646,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Externas não suportam Delta",
      "B": "Gerenciadas são mais rápidas",
      "C": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "D": "Não há diferença"
    },
    "correctAnswer": "C",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2647,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "B": "GRANT READ ON TABLE schema.table TO user@company.com",
      "C": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "D": "GRANT USAGE ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "A",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2648,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas permissões",
      "B": "Rastrear origem, transformações e destino dos dados",
      "C": "Apenas documentação",
      "D": "Apenas performance"
    },
    "correctAnswer": "B",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2649,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Não funciona com Unity Catalog",
      "B": "Apenas copia dados",
      "C": "Permite compartilhar dados com usuários externos sem copiar",
      "D": "Apenas para dados internos"
    },
    "correctAnswer": "C",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2650,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Externo é mais rápido",
      "B": "Não há diferença",
      "C": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "D": "Databricks não suporta compartilhamento"
    },
    "correctAnswer": "C",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2651,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "B": "Externas não suportam Delta",
      "C": "Gerenciadas são mais rápidas",
      "D": "Não há diferença"
    },
    "correctAnswer": "A",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2652,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "B": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "C": "GRANT READ ON TABLE schema.table TO user@company.com",
      "D": "GRANT SELECT ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "D",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2653,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas documentação",
      "B": "Apenas permissões",
      "C": "Apenas performance",
      "D": "Rastrear origem, transformações e destino dos dados"
    },
    "correctAnswer": "D",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2654,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Permite compartilhar dados com usuários externos sem copiar",
      "B": "Apenas copia dados",
      "C": "Não funciona com Unity Catalog",
      "D": "Apenas para dados internos"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2655,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Não há diferença",
      "B": "Databricks não suporta compartilhamento",
      "C": "Externo é mais rápido",
      "D": "Databricks: entre workspaces; Externo: com sistemas não-Databricks"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2656,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Gerenciadas são mais rápidas",
      "B": "Externas não suportam Delta",
      "C": "Não há diferença",
      "D": "Gerenciadas: dados no workspace; Externas: dados em local externo"
    },
    "correctAnswer": "D",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2657,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT READ ON TABLE schema.table TO user@company.com",
      "B": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "C": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "D": "GRANT USAGE ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "C",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2658,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Rastrear origem, transformações e destino dos dados",
      "B": "Apenas performance",
      "C": "Apenas permissões",
      "D": "Apenas documentação"
    },
    "correctAnswer": "A",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2659,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Permite compartilhar dados com usuários externos sem copiar",
      "B": "Apenas para dados internos",
      "C": "Não funciona com Unity Catalog",
      "D": "Apenas copia dados"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2660,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "B": "Databricks não suporta compartilhamento",
      "C": "Externo é mais rápido",
      "D": "Não há diferença"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2661,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "B": "Externas não suportam Delta",
      "C": "Não há diferença",
      "D": "Gerenciadas são mais rápidas"
    },
    "correctAnswer": "A",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2662,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT READ ON TABLE schema.table TO user@company.com",
      "B": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "C": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "D": "GRANT ACCESS ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "C",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2663,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Rastrear origem, transformações e destino dos dados",
      "B": "Apenas documentação",
      "C": "Apenas permissões",
      "D": "Apenas performance"
    },
    "correctAnswer": "A",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2664,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Apenas para dados internos",
      "B": "Não funciona com Unity Catalog",
      "C": "Apenas copia dados",
      "D": "Permite compartilhar dados com usuários externos sem copiar"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2665,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Externo é mais rápido",
      "B": "Databricks não suporta compartilhamento",
      "C": "Não há diferença",
      "D": "Databricks: entre workspaces; Externo: com sistemas não-Databricks"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2666,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "B": "Não há diferença",
      "C": "Gerenciadas são mais rápidas",
      "D": "Externas não suportam Delta"
    },
    "correctAnswer": "A",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2667,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "B": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "C": "GRANT READ ON TABLE schema.table TO user@company.com",
      "D": "GRANT USAGE ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "A",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2668,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Rastrear origem, transformações e destino dos dados",
      "B": "Apenas permissões",
      "C": "Apenas performance",
      "D": "Apenas documentação"
    },
    "correctAnswer": "A",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2669,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Apenas copia dados",
      "B": "Apenas para dados internos",
      "C": "Não funciona com Unity Catalog",
      "D": "Permite compartilhar dados com usuários externos sem copiar"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2670,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Externo é mais rápido",
      "B": "Databricks não suporta compartilhamento",
      "C": "Não há diferença",
      "D": "Databricks: entre workspaces; Externo: com sistemas não-Databricks"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2671,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "B": "Externas não suportam Delta",
      "C": "Gerenciadas são mais rápidas",
      "D": "Não há diferença"
    },
    "correctAnswer": "A",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2672,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT READ ON TABLE schema.table TO user@company.com",
      "B": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "C": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "D": "GRANT USAGE ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "C",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2673,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas performance",
      "B": "Apenas documentação",
      "C": "Apenas permissões",
      "D": "Rastrear origem, transformações e destino dos dados"
    },
    "correctAnswer": "D",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2674,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Apenas para dados internos",
      "B": "Apenas copia dados",
      "C": "Não funciona com Unity Catalog",
      "D": "Permite compartilhar dados com usuários externos sem copiar"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2675,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "B": "Databricks não suporta compartilhamento",
      "C": "Não há diferença",
      "D": "Externo é mais rápido"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2676,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Externas não suportam Delta",
      "B": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "C": "Gerenciadas são mais rápidas",
      "D": "Não há diferença"
    },
    "correctAnswer": "B",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2677,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "B": "GRANT READ ON TABLE schema.table TO user@company.com",
      "C": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "D": "GRANT SELECT ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "D",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2678,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas documentação",
      "B": "Rastrear origem, transformações e destino dos dados",
      "C": "Apenas performance",
      "D": "Apenas permissões"
    },
    "correctAnswer": "B",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2679,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Permite compartilhar dados com usuários externos sem copiar",
      "B": "Apenas copia dados",
      "C": "Apenas para dados internos",
      "D": "Não funciona com Unity Catalog"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2680,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Externo é mais rápido",
      "B": "Databricks não suporta compartilhamento",
      "C": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "D": "Não há diferença"
    },
    "correctAnswer": "C",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2681,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Gerenciadas são mais rápidas",
      "B": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "C": "Externas não suportam Delta",
      "D": "Não há diferença"
    },
    "correctAnswer": "B",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2682,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "B": "GRANT READ ON TABLE schema.table TO user@company.com",
      "C": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "D": "GRANT USAGE ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "C",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2683,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Rastrear origem, transformações e destino dos dados",
      "B": "Apenas performance",
      "C": "Apenas documentação",
      "D": "Apenas permissões"
    },
    "correctAnswer": "A",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2684,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Permite compartilhar dados com usuários externos sem copiar",
      "B": "Apenas para dados internos",
      "C": "Apenas copia dados",
      "D": "Não funciona com Unity Catalog"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2685,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Databricks não suporta compartilhamento",
      "B": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "C": "Não há diferença",
      "D": "Externo é mais rápido"
    },
    "correctAnswer": "B",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2686,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Não há diferença",
      "B": "Externas não suportam Delta",
      "C": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "D": "Gerenciadas são mais rápidas"
    },
    "correctAnswer": "C",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2687,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "B": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "C": "GRANT READ ON TABLE schema.table TO user@company.com",
      "D": "GRANT ACCESS ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "B",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2688,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Rastrear origem, transformações e destino dos dados",
      "B": "Apenas permissões",
      "C": "Apenas documentação",
      "D": "Apenas performance"
    },
    "correctAnswer": "A",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2689,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Não funciona com Unity Catalog",
      "B": "Apenas para dados internos",
      "C": "Permite compartilhar dados com usuários externos sem copiar",
      "D": "Apenas copia dados"
    },
    "correctAnswer": "C",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2690,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Não há diferença",
      "B": "Externo é mais rápido",
      "C": "Databricks não suporta compartilhamento",
      "D": "Databricks: entre workspaces; Externo: com sistemas não-Databricks"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2691,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Não há diferença",
      "B": "Gerenciadas são mais rápidas",
      "C": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "D": "Externas não suportam Delta"
    },
    "correctAnswer": "C",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2692,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT READ ON TABLE schema.table TO user@company.com",
      "B": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "C": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "D": "GRANT ACCESS ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "C",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2693,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas permissões",
      "B": "Apenas documentação",
      "C": "Rastrear origem, transformações e destino dos dados",
      "D": "Apenas performance"
    },
    "correctAnswer": "C",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2694,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Apenas para dados internos",
      "B": "Não funciona com Unity Catalog",
      "C": "Permite compartilhar dados com usuários externos sem copiar",
      "D": "Apenas copia dados"
    },
    "correctAnswer": "C",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2695,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Externo é mais rápido",
      "B": "Databricks não suporta compartilhamento",
      "C": "Não há diferença",
      "D": "Databricks: entre workspaces; Externo: com sistemas não-Databricks"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2696,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "B": "Não há diferença",
      "C": "Gerenciadas são mais rápidas",
      "D": "Externas não suportam Delta"
    },
    "correctAnswer": "A",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2697,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "B": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "C": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "D": "GRANT READ ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "A",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2698,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas permissões",
      "B": "Rastrear origem, transformações e destino dos dados",
      "C": "Apenas documentação",
      "D": "Apenas performance"
    },
    "correctAnswer": "B",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2699,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Apenas para dados internos",
      "B": "Permite compartilhar dados com usuários externos sem copiar",
      "C": "Não funciona com Unity Catalog",
      "D": "Apenas copia dados"
    },
    "correctAnswer": "B",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2700,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Externo é mais rápido",
      "B": "Databricks não suporta compartilhamento",
      "C": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "D": "Não há diferença"
    },
    "correctAnswer": "C",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2701,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "B": "Gerenciadas são mais rápidas",
      "C": "Não há diferença",
      "D": "Externas não suportam Delta"
    },
    "correctAnswer": "A",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2702,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT READ ON TABLE schema.table TO user@company.com",
      "B": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "C": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "D": "GRANT SELECT ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "D",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2703,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas permissões",
      "B": "Apenas documentação",
      "C": "Rastrear origem, transformações e destino dos dados",
      "D": "Apenas performance"
    },
    "correctAnswer": "C",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2704,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Permite compartilhar dados com usuários externos sem copiar",
      "B": "Não funciona com Unity Catalog",
      "C": "Apenas copia dados",
      "D": "Apenas para dados internos"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2705,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Não há diferença",
      "B": "Databricks não suporta compartilhamento",
      "C": "Externo é mais rápido",
      "D": "Databricks: entre workspaces; Externo: com sistemas não-Databricks"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2706,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Gerenciadas são mais rápidas",
      "B": "Não há diferença",
      "C": "Externas não suportam Delta",
      "D": "Gerenciadas: dados no workspace; Externas: dados em local externo"
    },
    "correctAnswer": "D",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2707,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "B": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "C": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "D": "GRANT READ ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "C",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2708,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Rastrear origem, transformações e destino dos dados",
      "B": "Apenas documentação",
      "C": "Apenas permissões",
      "D": "Apenas performance"
    },
    "correctAnswer": "A",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2709,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Não funciona com Unity Catalog",
      "B": "Apenas copia dados",
      "C": "Apenas para dados internos",
      "D": "Permite compartilhar dados com usuários externos sem copiar"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2710,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "B": "Databricks não suporta compartilhamento",
      "C": "Não há diferença",
      "D": "Externo é mais rápido"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2711,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Não há diferença",
      "B": "Externas não suportam Delta",
      "C": "Gerenciadas são mais rápidas",
      "D": "Gerenciadas: dados no workspace; Externas: dados em local externo"
    },
    "correctAnswer": "D",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2712,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "B": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "C": "GRANT READ ON TABLE schema.table TO user@company.com",
      "D": "GRANT SELECT ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "D",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2713,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas documentação",
      "B": "Rastrear origem, transformações e destino dos dados",
      "C": "Apenas performance",
      "D": "Apenas permissões"
    },
    "correctAnswer": "B",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2714,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Não funciona com Unity Catalog",
      "B": "Apenas copia dados",
      "C": "Permite compartilhar dados com usuários externos sem copiar",
      "D": "Apenas para dados internos"
    },
    "correctAnswer": "C",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2715,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Databricks não suporta compartilhamento",
      "B": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "C": "Não há diferença",
      "D": "Externo é mais rápido"
    },
    "correctAnswer": "B",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2716,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "B": "Não há diferença",
      "C": "Externas não suportam Delta",
      "D": "Gerenciadas são mais rápidas"
    },
    "correctAnswer": "A",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2717,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "B": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "C": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "D": "GRANT READ ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "C",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2718,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas permissões",
      "B": "Apenas performance",
      "C": "Apenas documentação",
      "D": "Rastrear origem, transformações e destino dos dados"
    },
    "correctAnswer": "D",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2719,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Permite compartilhar dados com usuários externos sem copiar",
      "B": "Não funciona com Unity Catalog",
      "C": "Apenas copia dados",
      "D": "Apenas para dados internos"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2720,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "B": "Externo é mais rápido",
      "C": "Databricks não suporta compartilhamento",
      "D": "Não há diferença"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2721,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Externas não suportam Delta",
      "B": "Gerenciadas são mais rápidas",
      "C": "Não há diferença",
      "D": "Gerenciadas: dados no workspace; Externas: dados em local externo"
    },
    "correctAnswer": "D",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2722,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT READ ON TABLE schema.table TO user@company.com",
      "B": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "C": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "D": "GRANT USAGE ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "B",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2723,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas performance",
      "B": "Apenas permissões",
      "C": "Rastrear origem, transformações e destino dos dados",
      "D": "Apenas documentação"
    },
    "correctAnswer": "C",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2724,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Não funciona com Unity Catalog",
      "B": "Apenas copia dados",
      "C": "Apenas para dados internos",
      "D": "Permite compartilhar dados com usuários externos sem copiar"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2725,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Não há diferença",
      "B": "Databricks não suporta compartilhamento",
      "C": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "D": "Externo é mais rápido"
    },
    "correctAnswer": "C",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2726,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Não há diferença",
      "B": "Externas não suportam Delta",
      "C": "Gerenciadas são mais rápidas",
      "D": "Gerenciadas: dados no workspace; Externas: dados em local externo"
    },
    "correctAnswer": "D",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2727,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT READ ON TABLE schema.table TO user@company.com",
      "B": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "C": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "D": "GRANT USAGE ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "C",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2728,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas permissões",
      "B": "Rastrear origem, transformações e destino dos dados",
      "C": "Apenas documentação",
      "D": "Apenas performance"
    },
    "correctAnswer": "B",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2729,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Permite compartilhar dados com usuários externos sem copiar",
      "B": "Apenas para dados internos",
      "C": "Apenas copia dados",
      "D": "Não funciona com Unity Catalog"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2730,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "B": "Externo é mais rápido",
      "C": "Não há diferença",
      "D": "Databricks não suporta compartilhamento"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2731,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Não há diferença",
      "B": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "C": "Gerenciadas são mais rápidas",
      "D": "Externas não suportam Delta"
    },
    "correctAnswer": "B",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2732,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "B": "GRANT READ ON TABLE schema.table TO user@company.com",
      "C": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "D": "GRANT SELECT ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "D",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2733,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Rastrear origem, transformações e destino dos dados",
      "B": "Apenas documentação",
      "C": "Apenas performance",
      "D": "Apenas permissões"
    },
    "correctAnswer": "A",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2734,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Permite compartilhar dados com usuários externos sem copiar",
      "B": "Apenas copia dados",
      "C": "Não funciona com Unity Catalog",
      "D": "Apenas para dados internos"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2735,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Externo é mais rápido",
      "B": "Databricks não suporta compartilhamento",
      "C": "Não há diferença",
      "D": "Databricks: entre workspaces; Externo: com sistemas não-Databricks"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2736,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Externas não suportam Delta",
      "B": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "C": "Não há diferença",
      "D": "Gerenciadas são mais rápidas"
    },
    "correctAnswer": "B",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2737,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT READ ON TABLE schema.table TO user@company.com",
      "B": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "C": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "D": "GRANT USAGE ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "B",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2738,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas documentação",
      "B": "Apenas permissões",
      "C": "Rastrear origem, transformações e destino dos dados",
      "D": "Apenas performance"
    },
    "correctAnswer": "C",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2739,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Permite compartilhar dados com usuários externos sem copiar",
      "B": "Não funciona com Unity Catalog",
      "C": "Apenas copia dados",
      "D": "Apenas para dados internos"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2740,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "B": "Databricks não suporta compartilhamento",
      "C": "Externo é mais rápido",
      "D": "Não há diferença"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2741,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Externas não suportam Delta",
      "B": "Não há diferença",
      "C": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "D": "Gerenciadas são mais rápidas"
    },
    "correctAnswer": "C",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2742,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "B": "GRANT READ ON TABLE schema.table TO user@company.com",
      "C": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "D": "GRANT USAGE ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "C",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2743,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas permissões",
      "B": "Rastrear origem, transformações e destino dos dados",
      "C": "Apenas documentação",
      "D": "Apenas performance"
    },
    "correctAnswer": "B",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2744,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Apenas para dados internos",
      "B": "Não funciona com Unity Catalog",
      "C": "Apenas copia dados",
      "D": "Permite compartilhar dados com usuários externos sem copiar"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2745,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "B": "Databricks não suporta compartilhamento",
      "C": "Não há diferença",
      "D": "Externo é mais rápido"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2746,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Externas não suportam Delta",
      "B": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "C": "Gerenciadas são mais rápidas",
      "D": "Não há diferença"
    },
    "correctAnswer": "B",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2747,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "B": "GRANT READ ON TABLE schema.table TO user@company.com",
      "C": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "D": "GRANT ACCESS ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "A",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2748,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas permissões",
      "B": "Rastrear origem, transformações e destino dos dados",
      "C": "Apenas documentação",
      "D": "Apenas performance"
    },
    "correctAnswer": "B",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2749,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Não funciona com Unity Catalog",
      "B": "Permite compartilhar dados com usuários externos sem copiar",
      "C": "Apenas para dados internos",
      "D": "Apenas copia dados"
    },
    "correctAnswer": "B",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2750,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Databricks não suporta compartilhamento",
      "B": "Externo é mais rápido",
      "C": "Não há diferença",
      "D": "Databricks: entre workspaces; Externo: com sistemas não-Databricks"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2751,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Gerenciadas são mais rápidas",
      "B": "Externas não suportam Delta",
      "C": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "D": "Não há diferença"
    },
    "correctAnswer": "C",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2752,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT READ ON TABLE schema.table TO user@company.com",
      "B": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "C": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "D": "GRANT SELECT ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "D",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2753,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas permissões",
      "B": "Apenas performance",
      "C": "Apenas documentação",
      "D": "Rastrear origem, transformações e destino dos dados"
    },
    "correctAnswer": "D",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2754,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Permite compartilhar dados com usuários externos sem copiar",
      "B": "Apenas para dados internos",
      "C": "Apenas copia dados",
      "D": "Não funciona com Unity Catalog"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2755,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Externo é mais rápido",
      "B": "Não há diferença",
      "C": "Databricks não suporta compartilhamento",
      "D": "Databricks: entre workspaces; Externo: com sistemas não-Databricks"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2756,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Gerenciadas são mais rápidas",
      "B": "Não há diferença",
      "C": "Externas não suportam Delta",
      "D": "Gerenciadas: dados no workspace; Externas: dados em local externo"
    },
    "correctAnswer": "D",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2757,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "B": "GRANT READ ON TABLE schema.table TO user@company.com",
      "C": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "D": "GRANT USAGE ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "C",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2758,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas documentação",
      "B": "Apenas permissões",
      "C": "Rastrear origem, transformações e destino dos dados",
      "D": "Apenas performance"
    },
    "correctAnswer": "C",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2759,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Não funciona com Unity Catalog",
      "B": "Apenas copia dados",
      "C": "Apenas para dados internos",
      "D": "Permite compartilhar dados com usuários externos sem copiar"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2760,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Não há diferença",
      "B": "Externo é mais rápido",
      "C": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "D": "Databricks não suporta compartilhamento"
    },
    "correctAnswer": "C",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2761,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Gerenciadas são mais rápidas",
      "B": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "C": "Externas não suportam Delta",
      "D": "Não há diferença"
    },
    "correctAnswer": "B",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2762,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "B": "GRANT READ ON TABLE schema.table TO user@company.com",
      "C": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "D": "GRANT USAGE ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "A",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2763,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Rastrear origem, transformações e destino dos dados",
      "B": "Apenas performance",
      "C": "Apenas permissões",
      "D": "Apenas documentação"
    },
    "correctAnswer": "A",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2764,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Não funciona com Unity Catalog",
      "B": "Permite compartilhar dados com usuários externos sem copiar",
      "C": "Apenas copia dados",
      "D": "Apenas para dados internos"
    },
    "correctAnswer": "B",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2765,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Não há diferença",
      "B": "Databricks não suporta compartilhamento",
      "C": "Externo é mais rápido",
      "D": "Databricks: entre workspaces; Externo: com sistemas não-Databricks"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2766,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "B": "Não há diferença",
      "C": "Externas não suportam Delta",
      "D": "Gerenciadas são mais rápidas"
    },
    "correctAnswer": "A",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2767,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "B": "GRANT READ ON TABLE schema.table TO user@company.com",
      "C": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "D": "GRANT ACCESS ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "C",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2768,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas performance",
      "B": "Apenas documentação",
      "C": "Rastrear origem, transformações e destino dos dados",
      "D": "Apenas permissões"
    },
    "correctAnswer": "C",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2769,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Apenas copia dados",
      "B": "Permite compartilhar dados com usuários externos sem copiar",
      "C": "Não funciona com Unity Catalog",
      "D": "Apenas para dados internos"
    },
    "correctAnswer": "B",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2770,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Externo é mais rápido",
      "B": "Não há diferença",
      "C": "Databricks não suporta compartilhamento",
      "D": "Databricks: entre workspaces; Externo: com sistemas não-Databricks"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2771,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Não há diferença",
      "B": "Gerenciadas são mais rápidas",
      "C": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "D": "Externas não suportam Delta"
    },
    "correctAnswer": "C",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2772,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT READ ON TABLE schema.table TO user@company.com",
      "B": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "C": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "D": "GRANT ACCESS ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "C",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2773,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas documentação",
      "B": "Apenas permissões",
      "C": "Rastrear origem, transformações e destino dos dados",
      "D": "Apenas performance"
    },
    "correctAnswer": "C",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2774,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Apenas para dados internos",
      "B": "Não funciona com Unity Catalog",
      "C": "Apenas copia dados",
      "D": "Permite compartilhar dados com usuários externos sem copiar"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2775,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Externo é mais rápido",
      "B": "Não há diferença",
      "C": "Databricks não suporta compartilhamento",
      "D": "Databricks: entre workspaces; Externo: com sistemas não-Databricks"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2776,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Não há diferença",
      "B": "Gerenciadas são mais rápidas",
      "C": "Externas não suportam Delta",
      "D": "Gerenciadas: dados no workspace; Externas: dados em local externo"
    },
    "correctAnswer": "D",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2777,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "B": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "C": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "D": "GRANT READ ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "C",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2778,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas performance",
      "B": "Apenas permissões",
      "C": "Rastrear origem, transformações e destino dos dados",
      "D": "Apenas documentação"
    },
    "correctAnswer": "C",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2779,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Permite compartilhar dados com usuários externos sem copiar",
      "B": "Apenas para dados internos",
      "C": "Não funciona com Unity Catalog",
      "D": "Apenas copia dados"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2780,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Databricks não suporta compartilhamento",
      "B": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "C": "Externo é mais rápido",
      "D": "Não há diferença"
    },
    "correctAnswer": "B",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2781,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Externas não suportam Delta",
      "B": "Não há diferença",
      "C": "Gerenciadas são mais rápidas",
      "D": "Gerenciadas: dados no workspace; Externas: dados em local externo"
    },
    "correctAnswer": "D",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2782,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "B": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "C": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "D": "GRANT READ ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "B",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2783,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas performance",
      "B": "Rastrear origem, transformações e destino dos dados",
      "C": "Apenas documentação",
      "D": "Apenas permissões"
    },
    "correctAnswer": "B",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2784,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Apenas copia dados",
      "B": "Permite compartilhar dados com usuários externos sem copiar",
      "C": "Apenas para dados internos",
      "D": "Não funciona com Unity Catalog"
    },
    "correctAnswer": "B",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2785,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Databricks não suporta compartilhamento",
      "B": "Externo é mais rápido",
      "C": "Não há diferença",
      "D": "Databricks: entre workspaces; Externo: com sistemas não-Databricks"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2786,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Não há diferença",
      "B": "Gerenciadas são mais rápidas",
      "C": "Externas não suportam Delta",
      "D": "Gerenciadas: dados no workspace; Externas: dados em local externo"
    },
    "correctAnswer": "D",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2787,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "B": "GRANT READ ON TABLE schema.table TO user@company.com",
      "C": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "D": "GRANT USAGE ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "C",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2788,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas documentação",
      "B": "Apenas permissões",
      "C": "Rastrear origem, transformações e destino dos dados",
      "D": "Apenas performance"
    },
    "correctAnswer": "C",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2789,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Não funciona com Unity Catalog",
      "B": "Apenas copia dados",
      "C": "Permite compartilhar dados com usuários externos sem copiar",
      "D": "Apenas para dados internos"
    },
    "correctAnswer": "C",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2790,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Externo é mais rápido",
      "B": "Databricks não suporta compartilhamento",
      "C": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "D": "Não há diferença"
    },
    "correctAnswer": "C",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2791,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Externas não suportam Delta",
      "B": "Gerenciadas são mais rápidas",
      "C": "Não há diferença",
      "D": "Gerenciadas: dados no workspace; Externas: dados em local externo"
    },
    "correctAnswer": "D",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2792,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "B": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "C": "GRANT READ ON TABLE schema.table TO user@company.com",
      "D": "GRANT ACCESS ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "B",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2793,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Rastrear origem, transformações e destino dos dados",
      "B": "Apenas permissões",
      "C": "Apenas performance",
      "D": "Apenas documentação"
    },
    "correctAnswer": "A",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2794,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Apenas copia dados",
      "B": "Não funciona com Unity Catalog",
      "C": "Permite compartilhar dados com usuários externos sem copiar",
      "D": "Apenas para dados internos"
    },
    "correctAnswer": "C",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2795,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Databricks não suporta compartilhamento",
      "B": "Externo é mais rápido",
      "C": "Não há diferença",
      "D": "Databricks: entre workspaces; Externo: com sistemas não-Databricks"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2796,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Não há diferença",
      "B": "Externas não suportam Delta",
      "C": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "D": "Gerenciadas são mais rápidas"
    },
    "correctAnswer": "C",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2797,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT READ ON TABLE schema.table TO user@company.com",
      "B": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "C": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "D": "GRANT USAGE ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "B",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2798,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas permissões",
      "B": "Rastrear origem, transformações e destino dos dados",
      "C": "Apenas documentação",
      "D": "Apenas performance"
    },
    "correctAnswer": "B",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2799,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Apenas copia dados",
      "B": "Permite compartilhar dados com usuários externos sem copiar",
      "C": "Não funciona com Unity Catalog",
      "D": "Apenas para dados internos"
    },
    "correctAnswer": "B",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2800,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "B": "Externo é mais rápido",
      "C": "Não há diferença",
      "D": "Databricks não suporta compartilhamento"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2801,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "B": "Gerenciadas são mais rápidas",
      "C": "Não há diferença",
      "D": "Externas não suportam Delta"
    },
    "correctAnswer": "A",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2802,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "B": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "C": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "D": "GRANT READ ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "A",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2803,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Rastrear origem, transformações e destino dos dados",
      "B": "Apenas permissões",
      "C": "Apenas performance",
      "D": "Apenas documentação"
    },
    "correctAnswer": "A",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2804,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Apenas copia dados",
      "B": "Permite compartilhar dados com usuários externos sem copiar",
      "C": "Apenas para dados internos",
      "D": "Não funciona com Unity Catalog"
    },
    "correctAnswer": "B",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2805,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Externo é mais rápido",
      "B": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "C": "Não há diferença",
      "D": "Databricks não suporta compartilhamento"
    },
    "correctAnswer": "B",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2806,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Não há diferença",
      "B": "Gerenciadas são mais rápidas",
      "C": "Externas não suportam Delta",
      "D": "Gerenciadas: dados no workspace; Externas: dados em local externo"
    },
    "correctAnswer": "D",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2807,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "B": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "C": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "D": "GRANT READ ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "C",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2808,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas permissões",
      "B": "Rastrear origem, transformações e destino dos dados",
      "C": "Apenas performance",
      "D": "Apenas documentação"
    },
    "correctAnswer": "B",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2809,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Apenas copia dados",
      "B": "Apenas para dados internos",
      "C": "Não funciona com Unity Catalog",
      "D": "Permite compartilhar dados com usuários externos sem copiar"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2810,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Databricks não suporta compartilhamento",
      "B": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "C": "Externo é mais rápido",
      "D": "Não há diferença"
    },
    "correctAnswer": "B",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2811,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Não há diferença",
      "B": "Externas não suportam Delta",
      "C": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "D": "Gerenciadas são mais rápidas"
    },
    "correctAnswer": "C",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2812,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT READ ON TABLE schema.table TO user@company.com",
      "B": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "C": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "D": "GRANT SELECT ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "D",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2813,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas performance",
      "B": "Apenas documentação",
      "C": "Apenas permissões",
      "D": "Rastrear origem, transformações e destino dos dados"
    },
    "correctAnswer": "D",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2814,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Permite compartilhar dados com usuários externos sem copiar",
      "B": "Não funciona com Unity Catalog",
      "C": "Apenas copia dados",
      "D": "Apenas para dados internos"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2815,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "B": "Databricks não suporta compartilhamento",
      "C": "Externo é mais rápido",
      "D": "Não há diferença"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2816,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Gerenciadas são mais rápidas",
      "B": "Não há diferença",
      "C": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "D": "Externas não suportam Delta"
    },
    "correctAnswer": "C",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2817,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "B": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "C": "GRANT READ ON TABLE schema.table TO user@company.com",
      "D": "GRANT ACCESS ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "A",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2818,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas performance",
      "B": "Apenas permissões",
      "C": "Rastrear origem, transformações e destino dos dados",
      "D": "Apenas documentação"
    },
    "correctAnswer": "C",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2819,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Permite compartilhar dados com usuários externos sem copiar",
      "B": "Apenas copia dados",
      "C": "Apenas para dados internos",
      "D": "Não funciona com Unity Catalog"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2820,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Não há diferença",
      "B": "Externo é mais rápido",
      "C": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "D": "Databricks não suporta compartilhamento"
    },
    "correctAnswer": "C",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2821,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Não há diferença",
      "B": "Gerenciadas são mais rápidas",
      "C": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "D": "Externas não suportam Delta"
    },
    "correctAnswer": "C",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2822,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT READ ON TABLE schema.table TO user@company.com",
      "B": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "C": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "D": "GRANT ACCESS ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "C",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2823,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas performance",
      "B": "Apenas documentação",
      "C": "Apenas permissões",
      "D": "Rastrear origem, transformações e destino dos dados"
    },
    "correctAnswer": "D",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2824,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Permite compartilhar dados com usuários externos sem copiar",
      "B": "Não funciona com Unity Catalog",
      "C": "Apenas para dados internos",
      "D": "Apenas copia dados"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2825,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Databricks não suporta compartilhamento",
      "B": "Externo é mais rápido",
      "C": "Não há diferença",
      "D": "Databricks: entre workspaces; Externo: com sistemas não-Databricks"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2826,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "B": "Gerenciadas são mais rápidas",
      "C": "Não há diferença",
      "D": "Externas não suportam Delta"
    },
    "correctAnswer": "A",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2827,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "B": "GRANT READ ON TABLE schema.table TO user@company.com",
      "C": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "D": "GRANT USAGE ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "C",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2828,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas documentação",
      "B": "Apenas performance",
      "C": "Apenas permissões",
      "D": "Rastrear origem, transformações e destino dos dados"
    },
    "correctAnswer": "D",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2829,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Apenas copia dados",
      "B": "Apenas para dados internos",
      "C": "Não funciona com Unity Catalog",
      "D": "Permite compartilhar dados com usuários externos sem copiar"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2830,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Externo é mais rápido",
      "B": "Databricks não suporta compartilhamento",
      "C": "Não há diferença",
      "D": "Databricks: entre workspaces; Externo: com sistemas não-Databricks"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2831,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Não há diferença",
      "B": "Externas não suportam Delta",
      "C": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "D": "Gerenciadas são mais rápidas"
    },
    "correctAnswer": "C",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2832,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "B": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "C": "GRANT READ ON TABLE schema.table TO user@company.com",
      "D": "GRANT ACCESS ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "B",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2833,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Rastrear origem, transformações e destino dos dados",
      "B": "Apenas performance",
      "C": "Apenas permissões",
      "D": "Apenas documentação"
    },
    "correctAnswer": "A",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2834,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Não funciona com Unity Catalog",
      "B": "Permite compartilhar dados com usuários externos sem copiar",
      "C": "Apenas copia dados",
      "D": "Apenas para dados internos"
    },
    "correctAnswer": "B",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2835,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Externo é mais rápido",
      "B": "Databricks não suporta compartilhamento",
      "C": "Não há diferença",
      "D": "Databricks: entre workspaces; Externo: com sistemas não-Databricks"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2836,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Não há diferença",
      "B": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "C": "Gerenciadas são mais rápidas",
      "D": "Externas não suportam Delta"
    },
    "correctAnswer": "B",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2837,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "B": "GRANT READ ON TABLE schema.table TO user@company.com",
      "C": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "D": "GRANT USAGE ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "A",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2838,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas permissões",
      "B": "Apenas performance",
      "C": "Rastrear origem, transformações e destino dos dados",
      "D": "Apenas documentação"
    },
    "correctAnswer": "C",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2839,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Apenas para dados internos",
      "B": "Permite compartilhar dados com usuários externos sem copiar",
      "C": "Apenas copia dados",
      "D": "Não funciona com Unity Catalog"
    },
    "correctAnswer": "B",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2840,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Databricks não suporta compartilhamento",
      "B": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "C": "Não há diferença",
      "D": "Externo é mais rápido"
    },
    "correctAnswer": "B",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2841,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Não há diferença",
      "B": "Externas não suportam Delta",
      "C": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "D": "Gerenciadas são mais rápidas"
    },
    "correctAnswer": "C",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2842,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT READ ON TABLE schema.table TO user@company.com",
      "B": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "C": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "D": "GRANT USAGE ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "C",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2843,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas permissões",
      "B": "Apenas documentação",
      "C": "Rastrear origem, transformações e destino dos dados",
      "D": "Apenas performance"
    },
    "correctAnswer": "C",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2844,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Apenas copia dados",
      "B": "Apenas para dados internos",
      "C": "Não funciona com Unity Catalog",
      "D": "Permite compartilhar dados com usuários externos sem copiar"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2845,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "B": "Databricks não suporta compartilhamento",
      "C": "Não há diferença",
      "D": "Externo é mais rápido"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2846,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Externas não suportam Delta",
      "B": "Não há diferença",
      "C": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "D": "Gerenciadas são mais rápidas"
    },
    "correctAnswer": "C",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2847,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "B": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "C": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "D": "GRANT READ ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "A",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2848,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas permissões",
      "B": "Apenas documentação",
      "C": "Rastrear origem, transformações e destino dos dados",
      "D": "Apenas performance"
    },
    "correctAnswer": "C",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2849,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Apenas copia dados",
      "B": "Apenas para dados internos",
      "C": "Não funciona com Unity Catalog",
      "D": "Permite compartilhar dados com usuários externos sem copiar"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2850,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Não há diferença",
      "B": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "C": "Databricks não suporta compartilhamento",
      "D": "Externo é mais rápido"
    },
    "correctAnswer": "B",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2851,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Não há diferença",
      "B": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "C": "Externas não suportam Delta",
      "D": "Gerenciadas são mais rápidas"
    },
    "correctAnswer": "B",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2852,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "B": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "C": "GRANT READ ON TABLE schema.table TO user@company.com",
      "D": "GRANT SELECT ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "D",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2853,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas performance",
      "B": "Rastrear origem, transformações e destino dos dados",
      "C": "Apenas permissões",
      "D": "Apenas documentação"
    },
    "correctAnswer": "B",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2854,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Permite compartilhar dados com usuários externos sem copiar",
      "B": "Não funciona com Unity Catalog",
      "C": "Apenas copia dados",
      "D": "Apenas para dados internos"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2855,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Databricks não suporta compartilhamento",
      "B": "Externo é mais rápido",
      "C": "Não há diferença",
      "D": "Databricks: entre workspaces; Externo: com sistemas não-Databricks"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2856,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "B": "Não há diferença",
      "C": "Externas não suportam Delta",
      "D": "Gerenciadas são mais rápidas"
    },
    "correctAnswer": "A",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2857,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT READ ON TABLE schema.table TO user@company.com",
      "B": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "C": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "D": "GRANT ACCESS ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "B",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2858,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas permissões",
      "B": "Rastrear origem, transformações e destino dos dados",
      "C": "Apenas documentação",
      "D": "Apenas performance"
    },
    "correctAnswer": "B",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2859,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Apenas para dados internos",
      "B": "Não funciona com Unity Catalog",
      "C": "Permite compartilhar dados com usuários externos sem copiar",
      "D": "Apenas copia dados"
    },
    "correctAnswer": "C",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2860,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "B": "Externo é mais rápido",
      "C": "Não há diferença",
      "D": "Databricks não suporta compartilhamento"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2861,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Gerenciadas são mais rápidas",
      "B": "Externas não suportam Delta",
      "C": "Não há diferença",
      "D": "Gerenciadas: dados no workspace; Externas: dados em local externo"
    },
    "correctAnswer": "D",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2862,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "B": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "C": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "D": "GRANT READ ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "B",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2863,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas permissões",
      "B": "Apenas documentação",
      "C": "Rastrear origem, transformações e destino dos dados",
      "D": "Apenas performance"
    },
    "correctAnswer": "C",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2864,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Permite compartilhar dados com usuários externos sem copiar",
      "B": "Apenas para dados internos",
      "C": "Não funciona com Unity Catalog",
      "D": "Apenas copia dados"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2865,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Não há diferença",
      "B": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "C": "Databricks não suporta compartilhamento",
      "D": "Externo é mais rápido"
    },
    "correctAnswer": "B",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2866,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Externas não suportam Delta",
      "B": "Gerenciadas são mais rápidas",
      "C": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "D": "Não há diferença"
    },
    "correctAnswer": "C",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2867,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT READ ON TABLE schema.table TO user@company.com",
      "B": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "C": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "D": "GRANT ACCESS ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "B",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2868,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas permissões",
      "B": "Rastrear origem, transformações e destino dos dados",
      "C": "Apenas performance",
      "D": "Apenas documentação"
    },
    "correctAnswer": "B",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2869,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Não funciona com Unity Catalog",
      "B": "Apenas para dados internos",
      "C": "Apenas copia dados",
      "D": "Permite compartilhar dados com usuários externos sem copiar"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2870,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Externo é mais rápido",
      "B": "Não há diferença",
      "C": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "D": "Databricks não suporta compartilhamento"
    },
    "correctAnswer": "C",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2871,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Não há diferença",
      "B": "Gerenciadas são mais rápidas",
      "C": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "D": "Externas não suportam Delta"
    },
    "correctAnswer": "C",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2872,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "B": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "C": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "D": "GRANT READ ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "B",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2873,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas performance",
      "B": "Rastrear origem, transformações e destino dos dados",
      "C": "Apenas permissões",
      "D": "Apenas documentação"
    },
    "correctAnswer": "B",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2874,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Apenas copia dados",
      "B": "Apenas para dados internos",
      "C": "Permite compartilhar dados com usuários externos sem copiar",
      "D": "Não funciona com Unity Catalog"
    },
    "correctAnswer": "C",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2875,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Databricks não suporta compartilhamento",
      "B": "Não há diferença",
      "C": "Externo é mais rápido",
      "D": "Databricks: entre workspaces; Externo: com sistemas não-Databricks"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2876,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Gerenciadas são mais rápidas",
      "B": "Externas não suportam Delta",
      "C": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "D": "Não há diferença"
    },
    "correctAnswer": "C",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2877,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "B": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "C": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "D": "GRANT READ ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "A",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2878,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Rastrear origem, transformações e destino dos dados",
      "B": "Apenas performance",
      "C": "Apenas documentação",
      "D": "Apenas permissões"
    },
    "correctAnswer": "A",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2879,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Apenas para dados internos",
      "B": "Permite compartilhar dados com usuários externos sem copiar",
      "C": "Não funciona com Unity Catalog",
      "D": "Apenas copia dados"
    },
    "correctAnswer": "B",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2880,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Não há diferença",
      "B": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "C": "Databricks não suporta compartilhamento",
      "D": "Externo é mais rápido"
    },
    "correctAnswer": "B",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2881,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Gerenciadas são mais rápidas",
      "B": "Externas não suportam Delta",
      "C": "Não há diferença",
      "D": "Gerenciadas: dados no workspace; Externas: dados em local externo"
    },
    "correctAnswer": "D",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2882,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "B": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "C": "GRANT READ ON TABLE schema.table TO user@company.com",
      "D": "GRANT ACCESS ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "B",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2883,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas documentação",
      "B": "Rastrear origem, transformações e destino dos dados",
      "C": "Apenas performance",
      "D": "Apenas permissões"
    },
    "correctAnswer": "B",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2884,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Apenas copia dados",
      "B": "Não funciona com Unity Catalog",
      "C": "Permite compartilhar dados com usuários externos sem copiar",
      "D": "Apenas para dados internos"
    },
    "correctAnswer": "C",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2885,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "B": "Databricks não suporta compartilhamento",
      "C": "Não há diferença",
      "D": "Externo é mais rápido"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2886,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "B": "Não há diferença",
      "C": "Externas não suportam Delta",
      "D": "Gerenciadas são mais rápidas"
    },
    "correctAnswer": "A",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2887,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "B": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "C": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "D": "GRANT READ ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "C",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2888,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas performance",
      "B": "Rastrear origem, transformações e destino dos dados",
      "C": "Apenas permissões",
      "D": "Apenas documentação"
    },
    "correctAnswer": "B",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2889,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Não funciona com Unity Catalog",
      "B": "Apenas copia dados",
      "C": "Permite compartilhar dados com usuários externos sem copiar",
      "D": "Apenas para dados internos"
    },
    "correctAnswer": "C",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2890,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Databricks não suporta compartilhamento",
      "B": "Não há diferença",
      "C": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "D": "Externo é mais rápido"
    },
    "correctAnswer": "C",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2891,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "B": "Gerenciadas são mais rápidas",
      "C": "Não há diferença",
      "D": "Externas não suportam Delta"
    },
    "correctAnswer": "A",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2892,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT READ ON TABLE schema.table TO user@company.com",
      "B": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "C": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "D": "GRANT ACCESS ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "C",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2893,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas documentação",
      "B": "Rastrear origem, transformações e destino dos dados",
      "C": "Apenas permissões",
      "D": "Apenas performance"
    },
    "correctAnswer": "B",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2894,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Apenas copia dados",
      "B": "Permite compartilhar dados com usuários externos sem copiar",
      "C": "Não funciona com Unity Catalog",
      "D": "Apenas para dados internos"
    },
    "correctAnswer": "B",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2895,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Databricks não suporta compartilhamento",
      "B": "Não há diferença",
      "C": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "D": "Externo é mais rápido"
    },
    "correctAnswer": "C",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2896,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Externas não suportam Delta",
      "B": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "C": "Gerenciadas são mais rápidas",
      "D": "Não há diferença"
    },
    "correctAnswer": "B",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2897,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT READ ON TABLE schema.table TO user@company.com",
      "B": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "C": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "D": "GRANT USAGE ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "B",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2898,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Rastrear origem, transformações e destino dos dados",
      "B": "Apenas performance",
      "C": "Apenas documentação",
      "D": "Apenas permissões"
    },
    "correctAnswer": "A",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2899,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Permite compartilhar dados com usuários externos sem copiar",
      "B": "Não funciona com Unity Catalog",
      "C": "Apenas para dados internos",
      "D": "Apenas copia dados"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2900,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Externo é mais rápido",
      "B": "Databricks não suporta compartilhamento",
      "C": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "D": "Não há diferença"
    },
    "correctAnswer": "C",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2901,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "B": "Gerenciadas são mais rápidas",
      "C": "Externas não suportam Delta",
      "D": "Não há diferença"
    },
    "correctAnswer": "A",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2902,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT READ ON TABLE schema.table TO user@company.com",
      "B": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "C": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "D": "GRANT ACCESS ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "B",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2903,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Rastrear origem, transformações e destino dos dados",
      "B": "Apenas performance",
      "C": "Apenas permissões",
      "D": "Apenas documentação"
    },
    "correctAnswer": "A",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2904,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Permite compartilhar dados com usuários externos sem copiar",
      "B": "Não funciona com Unity Catalog",
      "C": "Apenas copia dados",
      "D": "Apenas para dados internos"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2905,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Databricks não suporta compartilhamento",
      "B": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "C": "Externo é mais rápido",
      "D": "Não há diferença"
    },
    "correctAnswer": "B",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2906,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Não há diferença",
      "B": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "C": "Externas não suportam Delta",
      "D": "Gerenciadas são mais rápidas"
    },
    "correctAnswer": "B",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2907,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT READ ON TABLE schema.table TO user@company.com",
      "B": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "C": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "D": "GRANT USAGE ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "C",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2908,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas documentação",
      "B": "Apenas performance",
      "C": "Rastrear origem, transformações e destino dos dados",
      "D": "Apenas permissões"
    },
    "correctAnswer": "C",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2909,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Apenas copia dados",
      "B": "Permite compartilhar dados com usuários externos sem copiar",
      "C": "Apenas para dados internos",
      "D": "Não funciona com Unity Catalog"
    },
    "correctAnswer": "B",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2910,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "B": "Databricks não suporta compartilhamento",
      "C": "Não há diferença",
      "D": "Externo é mais rápido"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2911,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "B": "Gerenciadas são mais rápidas",
      "C": "Externas não suportam Delta",
      "D": "Não há diferença"
    },
    "correctAnswer": "A",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2912,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "B": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "C": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "D": "GRANT READ ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "C",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2913,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Rastrear origem, transformações e destino dos dados",
      "B": "Apenas permissões",
      "C": "Apenas performance",
      "D": "Apenas documentação"
    },
    "correctAnswer": "A",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2914,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Apenas para dados internos",
      "B": "Permite compartilhar dados com usuários externos sem copiar",
      "C": "Apenas copia dados",
      "D": "Não funciona com Unity Catalog"
    },
    "correctAnswer": "B",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2915,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Databricks não suporta compartilhamento",
      "B": "Não há diferença",
      "C": "Externo é mais rápido",
      "D": "Databricks: entre workspaces; Externo: com sistemas não-Databricks"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2916,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Externas não suportam Delta",
      "B": "Gerenciadas são mais rápidas",
      "C": "Não há diferença",
      "D": "Gerenciadas: dados no workspace; Externas: dados em local externo"
    },
    "correctAnswer": "D",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2917,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "B": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "C": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "D": "GRANT READ ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "B",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2918,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Rastrear origem, transformações e destino dos dados",
      "B": "Apenas performance",
      "C": "Apenas permissões",
      "D": "Apenas documentação"
    },
    "correctAnswer": "A",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2919,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Apenas para dados internos",
      "B": "Permite compartilhar dados com usuários externos sem copiar",
      "C": "Não funciona com Unity Catalog",
      "D": "Apenas copia dados"
    },
    "correctAnswer": "B",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2920,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Externo é mais rápido",
      "B": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "C": "Não há diferença",
      "D": "Databricks não suporta compartilhamento"
    },
    "correctAnswer": "B",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2921,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Externas não suportam Delta",
      "B": "Não há diferença",
      "C": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "D": "Gerenciadas são mais rápidas"
    },
    "correctAnswer": "C",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2922,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "B": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "C": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "D": "GRANT READ ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "A",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2923,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas performance",
      "B": "Apenas permissões",
      "C": "Apenas documentação",
      "D": "Rastrear origem, transformações e destino dos dados"
    },
    "correctAnswer": "D",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2924,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Apenas copia dados",
      "B": "Apenas para dados internos",
      "C": "Permite compartilhar dados com usuários externos sem copiar",
      "D": "Não funciona com Unity Catalog"
    },
    "correctAnswer": "C",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2925,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Databricks não suporta compartilhamento",
      "B": "Não há diferença",
      "C": "Externo é mais rápido",
      "D": "Databricks: entre workspaces; Externo: com sistemas não-Databricks"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2926,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "B": "Externas não suportam Delta",
      "C": "Gerenciadas são mais rápidas",
      "D": "Não há diferença"
    },
    "correctAnswer": "A",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2927,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT READ ON TABLE schema.table TO user@company.com",
      "B": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "C": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "D": "GRANT ACCESS ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "B",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2928,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Rastrear origem, transformações e destino dos dados",
      "B": "Apenas permissões",
      "C": "Apenas performance",
      "D": "Apenas documentação"
    },
    "correctAnswer": "A",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2929,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Permite compartilhar dados com usuários externos sem copiar",
      "B": "Apenas copia dados",
      "C": "Não funciona com Unity Catalog",
      "D": "Apenas para dados internos"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2930,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Externo é mais rápido",
      "B": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "C": "Não há diferença",
      "D": "Databricks não suporta compartilhamento"
    },
    "correctAnswer": "B",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2931,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Não há diferença",
      "B": "Gerenciadas são mais rápidas",
      "C": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "D": "Externas não suportam Delta"
    },
    "correctAnswer": "C",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2932,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT READ ON TABLE schema.table TO user@company.com",
      "B": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "C": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "D": "GRANT USAGE ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "C",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2933,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas permissões",
      "B": "Apenas performance",
      "C": "Apenas documentação",
      "D": "Rastrear origem, transformações e destino dos dados"
    },
    "correctAnswer": "D",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2934,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Apenas para dados internos",
      "B": "Permite compartilhar dados com usuários externos sem copiar",
      "C": "Apenas copia dados",
      "D": "Não funciona com Unity Catalog"
    },
    "correctAnswer": "B",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2935,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Databricks não suporta compartilhamento",
      "B": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "C": "Não há diferença",
      "D": "Externo é mais rápido"
    },
    "correctAnswer": "B",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2936,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "B": "Não há diferença",
      "C": "Externas não suportam Delta",
      "D": "Gerenciadas são mais rápidas"
    },
    "correctAnswer": "A",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2937,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "B": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "C": "GRANT READ ON TABLE schema.table TO user@company.com",
      "D": "GRANT ACCESS ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "B",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2938,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas documentação",
      "B": "Apenas performance",
      "C": "Rastrear origem, transformações e destino dos dados",
      "D": "Apenas permissões"
    },
    "correctAnswer": "C",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2939,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Apenas para dados internos",
      "B": "Permite compartilhar dados com usuários externos sem copiar",
      "C": "Apenas copia dados",
      "D": "Não funciona com Unity Catalog"
    },
    "correctAnswer": "B",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2940,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Externo é mais rápido",
      "B": "Não há diferença",
      "C": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "D": "Databricks não suporta compartilhamento"
    },
    "correctAnswer": "C",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2941,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "B": "Gerenciadas são mais rápidas",
      "C": "Não há diferença",
      "D": "Externas não suportam Delta"
    },
    "correctAnswer": "A",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2942,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "B": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "C": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "D": "GRANT READ ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "A",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2943,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas permissões",
      "B": "Apenas performance",
      "C": "Apenas documentação",
      "D": "Rastrear origem, transformações e destino dos dados"
    },
    "correctAnswer": "D",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2944,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Não funciona com Unity Catalog",
      "B": "Apenas copia dados",
      "C": "Permite compartilhar dados com usuários externos sem copiar",
      "D": "Apenas para dados internos"
    },
    "correctAnswer": "C",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2945,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Externo é mais rápido",
      "B": "Databricks não suporta compartilhamento",
      "C": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "D": "Não há diferença"
    },
    "correctAnswer": "C",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2946,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Não há diferença",
      "B": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "C": "Externas não suportam Delta",
      "D": "Gerenciadas são mais rápidas"
    },
    "correctAnswer": "B",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2947,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT READ ON TABLE schema.table TO user@company.com",
      "B": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "C": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "D": "GRANT ACCESS ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "C",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2948,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas documentação",
      "B": "Rastrear origem, transformações e destino dos dados",
      "C": "Apenas performance",
      "D": "Apenas permissões"
    },
    "correctAnswer": "B",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2949,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Apenas para dados internos",
      "B": "Permite compartilhar dados com usuários externos sem copiar",
      "C": "Apenas copia dados",
      "D": "Não funciona com Unity Catalog"
    },
    "correctAnswer": "B",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2950,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "B": "Databricks não suporta compartilhamento",
      "C": "Não há diferença",
      "D": "Externo é mais rápido"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2951,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Externas não suportam Delta",
      "B": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "C": "Gerenciadas são mais rápidas",
      "D": "Não há diferença"
    },
    "correctAnswer": "B",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2952,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "B": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "C": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "D": "GRANT READ ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "A",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2953,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas documentação",
      "B": "Apenas permissões",
      "C": "Apenas performance",
      "D": "Rastrear origem, transformações e destino dos dados"
    },
    "correctAnswer": "D",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2954,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Permite compartilhar dados com usuários externos sem copiar",
      "B": "Apenas copia dados",
      "C": "Apenas para dados internos",
      "D": "Não funciona com Unity Catalog"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2955,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "B": "Databricks não suporta compartilhamento",
      "C": "Não há diferença",
      "D": "Externo é mais rápido"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2956,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "B": "Não há diferença",
      "C": "Externas não suportam Delta",
      "D": "Gerenciadas são mais rápidas"
    },
    "correctAnswer": "A",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2957,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "B": "GRANT READ ON TABLE schema.table TO user@company.com",
      "C": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "D": "GRANT SELECT ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "D",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2958,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas performance",
      "B": "Rastrear origem, transformações e destino dos dados",
      "C": "Apenas documentação",
      "D": "Apenas permissões"
    },
    "correctAnswer": "B",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2959,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Não funciona com Unity Catalog",
      "B": "Apenas copia dados",
      "C": "Permite compartilhar dados com usuários externos sem copiar",
      "D": "Apenas para dados internos"
    },
    "correctAnswer": "C",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2960,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Databricks não suporta compartilhamento",
      "B": "Não há diferença",
      "C": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "D": "Externo é mais rápido"
    },
    "correctAnswer": "C",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2961,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Externas não suportam Delta",
      "B": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "C": "Não há diferença",
      "D": "Gerenciadas são mais rápidas"
    },
    "correctAnswer": "B",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2962,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "B": "GRANT READ ON TABLE schema.table TO user@company.com",
      "C": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "D": "GRANT SELECT ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "D",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2963,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Rastrear origem, transformações e destino dos dados",
      "B": "Apenas documentação",
      "C": "Apenas permissões",
      "D": "Apenas performance"
    },
    "correctAnswer": "A",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2964,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Apenas para dados internos",
      "B": "Apenas copia dados",
      "C": "Permite compartilhar dados com usuários externos sem copiar",
      "D": "Não funciona com Unity Catalog"
    },
    "correctAnswer": "C",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2965,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Não há diferença",
      "B": "Databricks não suporta compartilhamento",
      "C": "Externo é mais rápido",
      "D": "Databricks: entre workspaces; Externo: com sistemas não-Databricks"
    },
    "correctAnswer": "D",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2966,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Gerenciadas são mais rápidas",
      "B": "Não há diferença",
      "C": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "D": "Externas não suportam Delta"
    },
    "correctAnswer": "C",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2967,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT READ ON TABLE schema.table TO user@company.com",
      "B": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "C": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "D": "GRANT ACCESS ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "B",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2968,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas documentação",
      "B": "Apenas performance",
      "C": "Apenas permissões",
      "D": "Rastrear origem, transformações e destino dos dados"
    },
    "correctAnswer": "D",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2969,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Apenas copia dados",
      "B": "Não funciona com Unity Catalog",
      "C": "Permite compartilhar dados com usuários externos sem copiar",
      "D": "Apenas para dados internos"
    },
    "correctAnswer": "C",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2970,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "B": "Externo é mais rápido",
      "C": "Não há diferença",
      "D": "Databricks não suporta compartilhamento"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2971,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Gerenciadas são mais rápidas",
      "B": "Não há diferença",
      "C": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "D": "Externas não suportam Delta"
    },
    "correctAnswer": "C",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2972,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "B": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "C": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "D": "GRANT READ ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "C",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2973,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas performance",
      "B": "Rastrear origem, transformações e destino dos dados",
      "C": "Apenas documentação",
      "D": "Apenas permissões"
    },
    "correctAnswer": "B",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2974,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Apenas copia dados",
      "B": "Não funciona com Unity Catalog",
      "C": "Permite compartilhar dados com usuários externos sem copiar",
      "D": "Apenas para dados internos"
    },
    "correctAnswer": "C",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2975,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Não há diferença",
      "B": "Databricks não suporta compartilhamento",
      "C": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "D": "Externo é mais rápido"
    },
    "correctAnswer": "C",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2976,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Gerenciadas são mais rápidas",
      "B": "Não há diferença",
      "C": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "D": "Externas não suportam Delta"
    },
    "correctAnswer": "C",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2977,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "B": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "C": "GRANT READ ON TABLE schema.table TO user@company.com",
      "D": "GRANT ACCESS ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "A",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2978,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas documentação",
      "B": "Apenas performance",
      "C": "Rastrear origem, transformações e destino dos dados",
      "D": "Apenas permissões"
    },
    "correctAnswer": "C",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2979,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Permite compartilhar dados com usuários externos sem copiar",
      "B": "Apenas para dados internos",
      "C": "Apenas copia dados",
      "D": "Não funciona com Unity Catalog"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2980,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Databricks não suporta compartilhamento",
      "B": "Não há diferença",
      "C": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "D": "Externo é mais rápido"
    },
    "correctAnswer": "C",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2981,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Não há diferença",
      "B": "Externas não suportam Delta",
      "C": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "D": "Gerenciadas são mais rápidas"
    },
    "correctAnswer": "C",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2982,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "B": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "C": "GRANT READ ON TABLE schema.table TO user@company.com",
      "D": "GRANT USAGE ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "A",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2983,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas permissões",
      "B": "Rastrear origem, transformações e destino dos dados",
      "C": "Apenas performance",
      "D": "Apenas documentação"
    },
    "correctAnswer": "B",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2984,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Permite compartilhar dados com usuários externos sem copiar",
      "B": "Apenas para dados internos",
      "C": "Apenas copia dados",
      "D": "Não funciona com Unity Catalog"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2985,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Não há diferença",
      "B": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "C": "Databricks não suporta compartilhamento",
      "D": "Externo é mais rápido"
    },
    "correctAnswer": "B",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2986,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Não há diferença",
      "B": "Gerenciadas são mais rápidas",
      "C": "Externas não suportam Delta",
      "D": "Gerenciadas: dados no workspace; Externas: dados em local externo"
    },
    "correctAnswer": "D",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2987,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "B": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "C": "GRANT READ ON TABLE schema.table TO user@company.com",
      "D": "GRANT ACCESS ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "A",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2988,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas permissões",
      "B": "Apenas documentação",
      "C": "Rastrear origem, transformações e destino dos dados",
      "D": "Apenas performance"
    },
    "correctAnswer": "C",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2989,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Permite compartilhar dados com usuários externos sem copiar",
      "B": "Não funciona com Unity Catalog",
      "C": "Apenas copia dados",
      "D": "Apenas para dados internos"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2990,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Não há diferença",
      "B": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "C": "Databricks não suporta compartilhamento",
      "D": "Externo é mais rápido"
    },
    "correctAnswer": "B",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2991,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "B": "Não há diferença",
      "C": "Gerenciadas são mais rápidas",
      "D": "Externas não suportam Delta"
    },
    "correctAnswer": "A",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2992,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "B": "GRANT READ ON TABLE schema.table TO user@company.com",
      "C": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "D": "GRANT ACCESS ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "A",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2993,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Rastrear origem, transformações e destino dos dados",
      "B": "Apenas documentação",
      "C": "Apenas performance",
      "D": "Apenas permissões"
    },
    "correctAnswer": "A",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2994,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Apenas copia dados",
      "B": "Não funciona com Unity Catalog",
      "C": "Permite compartilhar dados com usuários externos sem copiar",
      "D": "Apenas para dados internos"
    },
    "correctAnswer": "C",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 2995,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Externo é mais rápido",
      "B": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "C": "Databricks não suporta compartilhamento",
      "D": "Não há diferença"
    },
    "correctAnswer": "B",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  },
  {
    "id": 2996,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre tabelas gerenciadas e externas no Unity Catalog?",
    "options": {
      "A": "Gerenciadas: dados no workspace; Externas: dados em local externo",
      "B": "Não há diferença",
      "C": "Externas não suportam Delta",
      "D": "Gerenciadas são mais rápidas"
    },
    "correctAnswer": "A",
    "rationale": "Tabelas gerenciadas armazenam dados no workspace do Databricks; tabelas externas referenciam dados em local externo (S3, ADLS, etc.).",
    "tip": "Gerenciadas = dados no Databricks; Externas = dados fora."
  },
  {
    "id": 2997,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como você concede permissão de leitura em uma tabela no Unity Catalog?",
    "options": {
      "A": "GRANT USAGE ON TABLE schema.table TO user@company.com",
      "B": "GRANT ACCESS ON TABLE schema.table TO user@company.com",
      "C": "GRANT SELECT ON TABLE schema.table TO user@company.com",
      "D": "GRANT READ ON TABLE schema.table TO user@company.com"
    },
    "correctAnswer": "C",
    "rationale": "O comando correto é GRANT SELECT (não READ) para conceder permissão de leitura em uma tabela.",
    "tip": "SELECT = leitura; MODIFY = escrita."
  },
  {
    "id": 2998,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Qual é o propósito da Linhagem (Lineage) no Unity Catalog?",
    "options": {
      "A": "Apenas performance",
      "B": "Apenas permissões",
      "C": "Rastrear origem, transformações e destino dos dados",
      "D": "Apenas documentação"
    },
    "correctAnswer": "C",
    "rationale": "Lineage rastreia o fluxo completo de dados: de qual tabela vieram, como foram transformados e para onde foram.",
    "tip": "Lineage = rastreamento de fluxo de dados."
  },
  {
    "id": 2999,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Como o Delta Sharing funciona no Unity Catalog?",
    "options": {
      "A": "Permite compartilhar dados com usuários externos sem copiar",
      "B": "Apenas copia dados",
      "C": "Apenas para dados internos",
      "D": "Não funciona com Unity Catalog"
    },
    "correctAnswer": "A",
    "rationale": "Delta Sharing permite compartilhar dados com usuários externos através de um protocolo seguro, sem necessidade de copiar dados.",
    "tip": "Delta Sharing = compartilhamento seguro sem cópia."
  },
  {
    "id": 3000,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Qual é a diferença entre Delta Sharing Databricks e externo?",
    "options": {
      "A": "Databricks não suporta compartilhamento",
      "B": "Databricks: entre workspaces; Externo: com sistemas não-Databricks",
      "C": "Externo é mais rápido",
      "D": "Não há diferença"
    },
    "correctAnswer": "B",
    "rationale": "Delta Sharing Databricks é entre workspaces Databricks; Delta Sharing externo permite compartilhar com sistemas não-Databricks.",
    "tip": "Databricks = entre workspaces; Externo = com terceiros."
  }
]