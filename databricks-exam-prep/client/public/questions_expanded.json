[
  {
    "id": 1,
    "category": "Databricks Intelligence Platform",
    "difficulty": "foundational",
    "question": "Uma empresa precisa construir uma plataforma de análise unificada que combine a flexibilidade de um data lake com a confiabilidade e performance de um data warehouse. O Databricks oferece uma solução para isso chamada de:",
    "options": {
      "A": "Data Mesh - uma arquitetura distribuída que fedora dados entre equipes",
      "B": "Lakehouse - uma arquitetura que unifica dados brutos, estruturados e transformados em um único repositório com governança centralizada",
      "C": "Data Vault - um modelo dimensional para armazenamento histórico de dados",
      "D": "Cloud Warehouse - um data warehouse tradicional hospedado na nuvem"
    },
    "correctAnswer": "B",
    "rationale": "Um Lakehouse é a solução oferecida pelo Databricks que combina as melhores características de data lakes (custo-benefício, flexibilidade de dados não estruturados) com as de data warehouses (ACID transactions, performance, governança). Delta Lake é a tecnologia que permite isso através de open-source storage format. Data Mesh é uma abordagem arquitetural, Data Vault é um padrão de modelagem, e Cloud Warehouse é apenas um data warehouse hospedado.",
    "tip": "Databricks se posiciona especificamente com a solução Lakehouse. Delta Lake é a tecnologia de armazenamento que viabiliza isso.",
    "officialReference": {
      "title": "What is a Lakehouse?",
      "url": "https://docs.databricks.com/en/lakehouse/index.html"
    },
    "contextScenario": "Uma fintech está consolidando dados de múltiplas fontes (APIs, bancos de dados transacionais, logs de aplicação). Precisa fazer análises em tempo real, auditar mudanças nos dados e manter compliance com regulações."
  },
  {
    "id": 2,
    "category": "Databricks Intelligence Platform",
    "difficulty": "foundational",
    "question": "Qual é o principal benefício do Delta Lake em comparação com formatos como Parquet ou CSV quando armazenados em um data lake tradicional?",
    "options": {
      "A": "Delta Lake ocupa menos espaço em disco e oferece compressão automática superior",
      "B": "Delta Lake permite transações ACID, versionamento de dados, schema enforcement e time travel queries",
      "C": "Delta Lake é mais compatível com ferramentas de BI como Tableau e Power BI",
      "D": "Delta Lake oferece replicação geográfica automática para disaster recovery"
    },
    "correctAnswer": "B",
    "rationale": "Delta Lake é um open-source storage format que fornece transações ACID sobre formatos de armazenamento em nuvem como S3/ADLS. Seus principais benefícios incluem: (1) ACID compliance garantindo integridade de dados, (2) Versionamento e time travel para recuperação de dados, (3) Schema enforcement e evolução de schema, (4) Unified Batch + Streaming. Parquet é apenas formato de armazenamento, e CSV não tem garantias de integridade. Compressão e compatibilidade com BI não são diferenciais primários.",
    "tip": "Delta Lake é sobre garantias transacionais e governança de dados, não sobre compressão ou compatibilidade de ferramentas.",
    "officialReference": {
      "title": "What is Delta Lake?",
      "url": "https://docs.databricks.com/en/delta/index.html"
    },
    "contextScenario": "Um banco de dados semanal está sendo processado via Spark jobs paralelos. Dois jobs escrevem na mesma tabela simultaneamente. Com Parquet, há risco de corrupção. Com Delta Lake, há garantias de consistency."
  },
  {
    "id": 3,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "Em um cenário de produção, uma empresa utiliza Delta Lake para armazenar dados históricos de vendas. Um data scientist solicita acesso ao estado da tabela como estava há 7 dias atrás para investigar um padrão anômalo. Qual feature do Delta Lake permite isso?",
    "options": {
      "A": "Delta Snapshot - cria backups automáticos a cada 24 horas",
      "B": "Delta Time Travel - permite consultas de versões anteriores da tabela usando VERSION AS OF ou TIMESTAMP AS OF",
      "C": "Delta Clone - permite clonagem de tabelas em snapshots históricos",
      "D": "Delta Restore - restore automático de dados corrompidos usando blockchain"
    },
    "correctAnswer": "B",
    "rationale": "Time Travel é uma feature nativa do Delta Lake que permite acessar versões anteriores da tabela através de VERSION AS OF (especificando o número da versão) ou TIMESTAMP AS OF (especificando data/hora). Exemplo: SELECT * FROM sales VERSION AS OF 100 retorna a tabela como estava na versão 100. Snapshot é conceitual, Clone é outra feature mas não de recuperação temporal, e Restore com blockchain não existe.",
    "tip": "Time Travel é acessado via cláusulas AS OF em SQL. Útil para auditing e recovery de dados acidentalmente deletados ou corrompidos.",
    "officialReference": {
      "title": "Delta Lake Time Travel",
      "url": "https://docs.databricks.com/en/delta/query-old-versions.html"
    },
    "contextScenario": "Segunda-feira: descobrir que dados foram deletados acidentalmente na sexta-feira. Com Time Travel, você consegue queryar exatamente o estado de sexta e restaurar manualmente apenas os registros necessários."
  },
  {
    "id": 4,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Uma aplicação executa múltiplas leituras concorrentes em uma tabela Delta enquanto um job de ETL executa atualização de massa com MERGE. Qual mecanismo do Delta Lake garante que os leitores nunca veem dados parcialmente commitados (dirty reads)?",
    "options": {
      "A": "Isolation Levels - usando SERIALIZABLE em todas as operações",
      "B": "Read-Write Locks - bloqueio pessimista de resources",
      "C": "Optimistic Concurrency Control + Transaction Log - Delta Log track versões, leitores usam snapshots imutáveis, escritores detectam conflitos",
      "D": "Snapshot Isolation com Versioning - cada leitor obtém uma cópia da tabela antes da escrita"
    },
    "correctAnswer": "C",
    "rationale": "Delta Lake usa Optimistic Concurrency Control (OCC) com um Transaction Log centralizado. Cada operação (read/write) corresponde a um commit no Delta Log. Leitores sempre veem uma snapshot imutável da tabela (específica a uma versão do Delta Log), e escritores usam OCC para detectar conflitos durante commits. Isso evita dirty reads sem bloquear leitores. SERIALIZABLE é nível de isolation do SQL, locks pessimistas causam contenção, e copiar toda tabela seria ineficiente.",
    "tip": "Delta usa OCC + Transaction Log. Snapshots garantem isolamento sem locks. Essa é arquitetura fundamental do Delta.",
    "officialReference": {
      "title": "Concurrency Control in Delta Lake",
      "url": "https://docs.databricks.com/en/delta/concurrency.html"
    },
    "contextScenario": "Dashboard de tempo real consultando tabela de métricas enquanto 10 workers rodam atualizações de merge concorrentemente. Delta garante que dashboard sempre vê dados consistentes."
  },
  {
    "id": 5,
    "category": "Databricks Intelligence Platform",
    "difficulty": "intermediate",
    "question": "A empresa possui um metastore no Databricks SQL e deseja implementar governança multi-tenant para 3 departamentos diferentes. Qual é a hierarquia correta de objetos no Unity Catalog para organizar isso?",
    "options": {
      "A": "Workspace > Catalog > Schema > Table",
      "B": "Metastore > Catalog > Schema > Table",
      "C": "Workspace > Database > Schema > Object",
      "D": "Cluster > Workspace > Database > Table"
    },
    "correctAnswer": "B",
    "rationale": "A hierarquia correta do Unity Catalog é: Metastore (conta/organização) > Catalog (departamento/projeto) > Schema (subdivisão lógica) > Table (dados). Cada um nesse nível representa um espaço lógico separado onde permissões podem ser aplicadas. Para 3 departamentos, você criaria 3 Catalogs dentro do Metastore. Database/Schema são termos legados do Hive metastore (pré-UC).",
    "tip": "Lembre-se: Metastore é o nível superior no Unity Catalog. Workspaces são separados do Unity Catalog hierarchy.",
    "officialReference": {
      "title": "Unity Catalog Structure",
      "url": "https://docs.databricks.com/en/data-governance/unity-catalog/index.html"
    },
    "contextScenario": "Banco com 3 departamentos (Finance, Marketing, Operations). Finance precisa ver dados apenas do Finance Catalog, Marketing do Marketing Catalog, etc. Usar ACLs no nível de Catalog resolve isso."
  },
  {
    "id": 6,
    "category": "Databricks Intelligence Platform",
    "difficulty": "advanced",
    "question": "Você tem uma tabela de clientes financeiros em UC com dados sensíveis (SSN, salário). Você precisa permitir que o departamento de Analytics veja a tabela, mas sem ver as colunas sensíveis, enquanto Finance vê tudo. Qual é a melhor abordagem usando UC?",
    "options": {
      "A": "Criar duas tabelas: uma com dados sensíveis (Finance), outra sem sensíveis (Analytics). Usar GRANT TABLE SELECT separadamente.",
      "B": "Usar GRANT com column-level masking: GRANT MASKED_SELECT ON TABLE customers (ssn, salary) TO analytics_group",
      "C": "Usar Dynamic View Filtering com UC - criar views que filtram colunas baseado no grupo do usuário",
      "D": "Usar Column-Level Access Control no UC - aplicar GRANT com column masking e dynamic masking rules baseado em tags"
    },
    "correctAnswer": "D",
    "rationale": "Unity Catalog suporta Column-Level Access Control e Data Masking. Você pode aplicar DENY ou masking rules baseado em object tags. Exemplo: criar tag 'pii', aplicar a colunas sensíveis, então usar um masking rule que redact essas colunas para usuários sem privilégio. Opção C (Views) funcionaria mas é menos elegante. Opção A duplica dados desnecessariamente. Opção B não é sintaxe correta do UC.",
    "tip": "UC suporta Column-Level Access Control e Attribute-Based Access Control (ABAC) via tags + masking rules.",
    "officialReference": {
      "title": "Column-Level Access Control",
      "url": "https://docs.databricks.com/en/data-governance/unity-catalog/column-and-row-filters.html"
    },
    "contextScenario": "Fintech com dados de clientes. Compliance exige que engenheiros de dados não vejam SSN/salary. Finance pode ver tudo. Uma tabela, múltiplos níveis de acesso, aplicado via UC masking."
  },
  {
    "id": 7,
    "category": "Development and Ingestion",
    "difficulty": "foundational",
    "question": "Qual é o principal propósito do Auto Loader no Databricks?",
    "options": {
      "A": "Carregar dados em lote de qualquer fonte HTTP usando REST APIs",
      "B": "Detectar e carregar incremente novos arquivos de data sources (S3, ADLS) com opção de schema inference automática",
      "C": "Automaticamente paralelizar queries de leitura em múltiplos clusters",
      "D": "Sincronizar dados entre múltiplos data centers em tempo real"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader é uma feature de Databricks para incremental data loading de cloud storage. Seus benefícios: (1) Detecta novos arquivos automaticamente via directory listing ou file notification services (SQS/Event Hubs), (2) Oferece schema inference/evolution, (3) Trata dados malformados com Rescue Columns, (4) Mais eficiente que polling. Não é para REST APIs (seria Autoloader Cloud Files), não paraleliza queries, não é para replicação multi-DC.",
    "tip": "Auto Loader é para detectar incremente arquivos novos em cloud storage. Pense em 'auto-detect, auto-ingest'.",
    "officialReference": {
      "title": "Auto Loader",
      "url": "https://docs.databricks.com/en/ingestion/auto-loader/index.html"
    },
    "contextScenario": "Diariamente 1000+ arquivos CSV são depositados em um bucket S3 pela parceira. Você precisa ingerir apenas arquivos novos (não reprocessar todos). Auto Loader detecta automaticamente."
  },
  {
    "id": 8,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Você implementou Auto Loader para carregar CSVs do S3. Após 2 semanas, uma mudança no formato dos arquivos (nova coluna 'invoice_type' adicionada) causa erro. Como o Auto Loader pode ser configurado para lidar automaticamente com essa mudança?",
    "options": {
      "A": "Auto Loader detecta automaticamente e para de carregar até revisar manualmente",
      "B": "Usar Schema Evolution com 'cloudFiles.schemaEvolutionMode = \"addNewColumns\"' para aceitar novas colunas",
      "C": "Auto Loader não suporta schema evolution; você precisa usar manual schema updates",
      "D": "Usar Rescue Columns para capturar dados malformados em coluna JSON adicional, depois processar manualmente"
    },
    "correctAnswer": "B",
    "rationale": "Auto Loader suporta Schema Evolution via opção cloudFiles.schemaEvolutionMode. Modo 'addNewColumns' aceita novas colunas, 'failOnNewColumns' falha, 'none' ignora. Exemplo em Spark: spark.readStream.format('cloudFiles').option('cloudFiles.schemaEvolutionMode', 'addNewColumns'). Rescue Columns são para dados malformados, não para schema evolution. Manual updates seriam ineficientes para um cenário de ingestion automática.",
    "tip": "Schema Evolution em Auto Loader permite adaptar-se a mudanças de schema sem intervalo manual. Modo principal é 'addNewColumns'.",
    "officialReference": {
      "title": "Schema Evolution in Auto Loader",
      "url": "https://docs.databricks.com/en/ingestion/auto-loader/schema.html"
    },
    "contextScenario": "Seu parceiro adiciona coluna 'invoice_type' aos CSVs. Com Schema Evolution ativado, nova coluna aparece automaticamente. Sem ela, ingestion falha."
  },
  {
    "id": 9,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Em um pipeline com Auto Loader, você observa que 0.5% dos registros são malformados (JSON inválido, encoding incorreto). Você quer processar a maioria dos dados e investigar os malformados depois. Qual é a best practice?",
    "options": {
      "A": "Aumentar timeout do Auto Loader para dar mais tempo ao parser",
      "B": "Usar Rescue Columns - Auto Loader captura registros malformados com a linha completa como string em coluna adicional",
      "C": "Usar try-catch em PySpark para ignorar erros e continuar",
      "D": "Separar arquivos malformados manualmente e reprocessá-los com parser customizado"
    },
    "correctAnswer": "B",
    "rationale": "Rescue Columns é feature nativa do Auto Loader para lidar com dados malformados sem quebrar o pipeline. Registros que não conseguem fazer parse são colocados em colunas adicionais (por padrão '_rescued_data'). Você pode depois investigar, parsear manualmente, ou aplicar transformações condicionais. Timeout não resolveria (não é timeout problem), try-catch não é tão limpo, e separação manual não seria viável em escala. Rescue Columns é design pattern padrão de Databricks.",
    "tip": "Rescue Columns permitem 'fail-safe' data loading. Dados ruins não causam falha, são capturados para investigação posterior.",
    "officialReference": {
      "title": "Rescue Columns in Auto Loader",
      "url": "https://docs.databricks.com/en/ingestion/auto-loader/rescue-columns.html"
    },
    "contextScenario": "Auto Loader ingere dados de 50 fontes. 1 fonte ocasionalmente envia registros com encoding incorreto. Rescue Columns captura esses, pipeline continua, você investiga depois."
  },
  {
    "id": 10,
    "category": "Development and Ingestion",
    "difficulty": "intermediate",
    "question": "Delta Live Tables (DLT) oferece uma abordagem declarativa para pipelines de dados. Qual é o principal benefício dessa abordagem em relação a Spark jobs tradicionais?",
    "options": {
      "A": "DLT compila código para SQL puro, oferecendo 10x de performance",
      "B": "DLT descreve o resultado desejado (não o processo); framework otimiza execução, gerencia dependencies, e oferece data quality rules nativas",
      "C": "DLT permite paralelização automática mesmo em clusters single-node",
      "D": "DLT é mais barato em termos de licença Databricks"
    },
    "correctAnswer": "B",
    "rationale": "Delta Live Tables usa paradigma declarativo: você define transformações e DLT cuida de otimização, paralelização, schema management, e quality checks. Benefícios: (1) Menos código boilerplate, (2) Otimização automática de DAG, (3) Data Quality checks built-in (CONSTRAINT, @quality), (4) Automatic schema management, (5) Web UI mostra lineage. Performance não é 10x melhor por ser SQL, nem é sobre single-node parallelization ou custo de licença.",
    "tip": "DLT é declarativo: você diz o que quer, não como quer. Framework otimiza tudo.",
    "officialReference": {
      "title": "Delta Live Tables",
      "url": "https://docs.databricks.com/en/delta-live-tables/index.html"
    },
    "contextScenario": "Você escreve 50 linhas de código em Spark para fazer transformações em 5 stages. DLT faz o mesmo com 15 linhas, otimiza dependency order, e oferece quality checks nativos."
  },
  {
    "id": 11,
    "category": "Development and Ingestion",
    "difficulty": "advanced",
    "question": "Em um pipeline DLT, você define uma expectation: @dlt.expect('valid_amount', 'amount > 0'). Se 5% dos registros violam essa expectation, qual é o comportamento padrão?",
    "options": {
      "A": "Pipeline falha e não avança; você precisa corrigir os dados antes de rodar novamente",
      "B": "Pipeline continua, mas registros com violação são rejeitados; você pode consultar a tabela de qualidade (data quality dashboard)",
      "C": "Pipeline continua, registros violando são marcados com flag 'invalid' em coluna adicional",
      "D": "Pipeline continua, dados ruins são automaticamente enviados para quarentena em tabela separada"
    },
    "correctAnswer": "B",
    "rationale": "DLT expectations com @dlt.expect() permitem validações. Por padrão, violações não causam falha de pipeline (modo 'warn'). Os registros são incluídos, mas a métrica de violação é rastreada no Data Quality dashboard. Você pode mudar para mode 'fail' ou 'drop' se precisar. Isso é diferente de constraints que podem causar falha. Não há flag automática nem quarentena automática (B é mais próximo do comportamento padrão).",
    "tip": "DLT expectations por padrão são 'warn' - pipeline continua, você monitora via dashboard. Use 'drop' se quiser rejeitar registros.",
    "officialReference": {
      "title": "DLT Data Quality",
      "url": "https://docs.databricks.com/en/delta-live-tables/data-quality.html"
    },
    "contextScenario": "Seu pipeline DLT tem expectation para amounts >= 0. Ocasionalmente há valores -1 (erro de integração). Com modo 'warn', pipeline rooda, você vê violações no dashboard, investiga."
  },
  {
    "id": 12,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Você tem um DataFrame com colunas: order_id, customer_id, items (Array of structs com {product_id, quantity}). Precisa gerar um registro por item. Qual é a função correta?",
    "options": {
      "A": "df.select('*', explode('items')) - isso mantém todas as colunas e explode items",
      "B": "df.select('order_id', 'customer_id', explode('items').alias('item')) - isso cria uma coluna 'item' com cada struct",
      "C": "df.selectExpr('*', 'explode(items) as item') - sintaxe SQL alternativa",
      "D": "df.flatMap() - função Python que flattena estruturas aninhadas"
    },
    "correctAnswer": "B",
    "rationale": "EXPLODE transforma um array em múltiplas linhas. A sintaxe correta em PySpark é: df.select('order_id', 'customer_id', F.explode('items').alias('item')). Opção A está correta funcionalmente mas perde clareza ao usar select('*'). Opção C com selectExpr também funciona. Opção D flatMap é método Python, não é função Spark padrão para isso. B é mais explícito e best practice.",
    "tip": "EXPLODE cria uma linha por elemento de array. Sempre use .alias() para nomear a coluna explodida.",
    "officialReference": {
      "title": "Spark SQL Built-in Functions",
      "url": "https://docs.databricks.com/en/sql/language-manual/functions/explode.html"
    },
    "contextScenario": "Você tem 1M pedidos, cada um com 5 items em média. Precisa gerar relatório de 5M linhas (1 por item) para análise de vendas por produto."
  },
  {
    "id": 13,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Você precisa fazer JOIN entre tabela 'orders' (1 bilhão registros) e 'products' (10k registros). A tabela 'products' cabe em memória. Qual estratégia de join o Spark deve usar e como forçá-la?",
    "options": {
      "A": "Sort-Merge Join - mais eficiente para qualquer tamanho; não há opção de forçar",
      "B": "Broadcast Join - replica 'products' em todos os executors; forçar com hint: /*+ BROADCAST(products) */",
      "C": "Hash Join - distribui ambas as tabelas por hash key; padrão do Spark",
      "D": "Nested Loop Join - mais eficiente para tabelas pequenas; deve ser usada quando possível"
    },
    "correctAnswer": "B",
    "rationale": "Broadcast Join é estratégia ideal para JOIN entre tabela grande e pequena (que cabe em memória). Tabela pequena é enviada para todos executors, evitando shuffle custoso. Tamanho padrão é 10MB mas configurável. Syntaxe em Spark SQL: SELECT /*+ BROADCAST(products) */ * FROM orders JOIN products. Em PySpark DataFrame: df1.join(F.broadcast(df2), 'key'). Sort-Merge Join é para tabelas pré-sortidas, Hash Join causa shuffle (ineficiente aqui), Nested Loop é muito lento.",
    "tip": "Broadcast Join é a primeira otimização a tentar quando uma das tabelas cabe em memória. Sempre use para JOINs com dimensões pequenas.",
    "officialReference": {
      "title": "Spark Join Strategies",
      "url": "https://docs.databricks.com/en/sql/language-manual/hints/broadcast-join.html"
    },
    "contextScenario": "JOIN de fatos (1B registros) com dimensão produtos (10k). Sem broadcast, Spark faria shuffle de 1B registros. Com broadcast, apenas 10k replicados, query roda 10x mais rápido."
  },
  {
    "id": 14,
    "category": "Data Processing & Transformations",
    "difficulty": "advanced",
    "question": "Em um pipeline Spark, você está processando um DataFrame com milhões de registros e nota que a query é lenta. Você suspeita de skew de dados (algumas partições muito maiores que outras). Como você pode diagnositcar e resolver?",
    "options": {
      "A": "Usar EXPLAIN EXTENDED; se ver skew, repartição com salt: adicionar número aleatório à key antes de JOINs",
      "B": "Usar Spark Adaptive Query Execution (AQE) que detecta e resolve skew automaticamente",
      "C": "Monitorar via Spark UI (Tasks tab); se task duração varia muito, indica skew; usar broadcast ou custom partitioner",
      "D": "Reduzir partition count - menos partições = menos chance de skew"
    },
    "correctAnswer": "B",
    "rationale": "Adaptive Query Execution (AQE) é feature do Spark 3.0+ que detecta skew em runtime e aplica otimizações automaticamente (reparticionamento dinâmico, broadcast join size adjustment, etc.). Configurar com: spark.sql.adaptive.enabled = true (padrão em Databricks). Opção A (salting) funciona mas é manual. Opção C via Spark UI é para diagnóstico. Opção D não resolveria o problema.",
    "tip": "AQE é a forma moderna de lidar com skew. Ative com spark.sql.adaptive.enabled = true.",
    "officialReference": {
      "title": "Adaptive Query Execution",
      "url": "https://docs.databricks.com/en/sql/query-optimization/adaptive-query-execution.html"
    },
    "contextScenario": "JOIN entre orders (alguns customers têm 100M pedidos, maioria tem <1k). Sem AQE, alguns tasks processam 100M registros, outros 1k. AQE detecta, repartição dinamicamente."
  },
  {
    "id": 15,
    "category": "Data Processing & Transformations",
    "difficulty": "intermediate",
    "question": "Você tem uma tabela Delta com 500GB de dados sobre vendas. Seus queries tipicamente filtram por date_column E region. Como otimizar para essas queries?",
    "options": {
      "A": "Criar índice tradicional como em bancos SQL relacionais",
      "B": "Usar OPTIMIZE e Z-ORDER BY (date_column, region) - reorganiza dados para co-locate registros com valores similares",
      "C": "Particionar tabela por region; dentro de cada partition, fazer Z-ORDER por date_column",
      "D": "Usar VACUUM para limpar versões antigas, depois rodar ANALYZE TABLE para gerar estatísticas"
    },
    "correctAnswer": "B",
    "rationale": "Z-ORDER (baseado em space-filling curves) reorganiza dados em arquivo para co-locate registros com valores similares em múltiplas colunas. Melhora performance de queries com múltiplos filtros (date_column AND region). Comando: OPTIMIZE table_name ZORDER BY (date_column, region). Opção C também é válida (particionamento + Z-ORDER é best practice), mas B está mais direta. Índices tradicionais não existem em Delta, VACUUM/ANALYZE não otimizam para queries.",
    "tip": "Z-ORDER é para co-locate dados. Use para colunas com high cardinality que aparecem em WHERE clauses. Geralmente 1-2 colunas.",
    "officialReference": {
      "title": "Delta Lake Z-ORDER",
      "url": "https://docs.databricks.com/en/delta/data-skipping.html"
    },
    "contextScenario": "Queries: SELECT * FROM sales WHERE date > '2024-01-01' AND region = 'APAC'. Sem Z-ORDER, Spark lê 500GB. Com Z-ORDER por date+region, Spark pula 90% dos files via data skipping."
  },
  {
    "id": 16,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Você precisa aplicar diferentes niveis de acesso a uma tabela Delta de clientes baseado em região do usuário (um user em 'US' deve ver apenas dados de US). Qual é a best practice usando Unity Catalog?",
    "options": {
      "A": "Criar tabelas separadas por região: customers_us, customers_eu, etc.; aplicar GRANT separadamente",
      "B": "Usar uma tabela com Row Filters (Row-Level Security) no UC; criar função SQL que retorna região do user; aplicar GRANT com WHERE clause",
      "C": "Usar views SQL normais com WHERE clauses por região",
      "D": "Usar tags UC para marcar regiões, depois aplicar Row Filters dinamicamente baseado no user tags"
    },
    "correctAnswer": "B",
    "rationale": "Unity Catalog suporta Row-Level Security (RLS) via Row Filters. Você define uma função SQL que retorna verdadeiro/falso baseado no contexto do user. Exemplo: CREATE FUNCTION region_filter() RETURNS BOOLEAN RETURN current_user() IN (...); depois ALTER TABLE customers SET ROW FILTER region_filter() ON CONDITION region = current_user_region(). Opção A duplica dados, Opção C não oferece enforcement em nível Catalog, Opção D com tags é mais para column-level.",
    "tip": "Row Filters em UC são a forma centralizada de aplicar RLS. Funciona em query time, não requer duplicação de dados.",
    "officialReference": {
      "title": "Row Filters in UC",
      "url": "https://docs.databricks.com/en/data-governance/unity-catalog/column-and-row-filters.html"
    },
    "contextScenario": "Banco global com clientes em US/EU/APAC. Um query 'SELECT * FROM customers' deve retornar automaticamente apenas dados da região do user."
  },
  {
    "id": 17,
    "category": "Data Governance & Quality",
    "difficulty": "advanced",
    "question": "Você implementou um lakehouse em UC com múltiplas etapas (bronze, silver, gold). Você quer auditar acesso de dados - saber quem acessou qual coluna de qual tabela em qual timestamp. Como habilitar isso?",
    "options": {
      "A": "Usar Databricks Audit Logs - captura todos os eventos de acesso ao UC; disponível via Account API e pode ser streamado para Databricks SQL",
      "B": "Habilitar query logging na tabela via ALTER TABLE ... SET TBLPROPERTIES",
      "C": "Usar Delta Lake transaction log como audit trail (arquivo _delta_log/)",
      "D": "Implementar triggers customizadas em cada query"
    },
    "correctAnswer": "A",
    "rationale": "Databricks Audit Logs (Account-level API) capturam todos os eventos, incluindo Data Access Audit que mostra usuario, ação (SELECT, INSERT, etc), recurso (tabela, coluna), timestamp. Disponível em account admin settings, pode ser streamado para event hubs ou armazenado em Delta tables. Essa é forma empresarial de compliance/auditing. Delta Log é para versionamento, triggers customizadas não são práticas.",
    "tip": "Audit Logs = rastreamento de acesso. Para compliance e investigação forense de dados.",
    "officialReference": {
      "title": "Databricks Audit Logs",
      "url": "https://docs.databricks.com/en/administration/audit-logs.html"
    },
    "contextScenario": "Fintech: auditoria regulatória requer rastreamento de quem acessou dados de cliente. Audit Logs fornece trilha imutável."
  },
  {
    "id": 18,
    "category": "Data Governance & Quality",
    "difficulty": "intermediate",
    "question": "Você deseja garantir que uma coluna 'price' em uma tabela Delta nunca receba valores negativos ou nulos. Como implementar isso usando Delta Constraints?",
    "options": {
      "A": "Usar NOT NULL e CHECK constraints na criação da tabela: CREATE TABLE products (price DECIMAL NOT NULL CHECK (price > 0))",
      "B": "Usar Spark StructType com nullable=False; sistema automático rejeita nulos",
      "C": "Usar dbt tests para validação pós-load",
      "D": "Implementar validação em Python antes de escrever em Delta"
    },
    "correctAnswer": "A",
    "rationale": "Delta Lake 1.3+ suporta table constraints: NOT NULL (nulos), CHECK (condições lógicas), UNIQUE, PRIMARY KEY. Sintaxe: CREATE TABLE products (price DECIMAL NOT NULL, CHECK (price > 0)). Violações causam erro de commit. Opção B (StructType nullable) é em memória, não persiste. Opção C (dbt) é validação, não enforcement. Opção D é pré-validação, não enforcement transacional.",
    "tip": "Delta Constraints são enforcement ao nível de tabela. Garantem integridade de dados.",
    "officialReference": {
      "title": "Delta Lake Constraints",
      "url": "https://docs.databricks.com/en/delta/table-constraints.html"
    },
    "contextScenario": "E-commerce: campo price nunca deve ser negativo ou nulo. Com constraints, qualquer INSERT/UPDATE com price <= 0 falha imediatamente."
  },
  {
    "id": 19,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Você tem um pipeline ETL que consiste em 3 jobs: ingest_data, transform_data, load_warehouse. O segundo job (transform) deve rodar APENAS após o primeiro completar. Como configurar isso em Databricks Jobs?",
    "options": {
      "A": "Criar 3 jobs separados e usar cron schedule com delays: 'ingest' a cada 1h, 'transform' a cada 1h com +10min offset",
      "B": "Usar task_depends_on no Databricks Workflows: 'transform' task tem depends_on=['ingest'], 'load' tem depends_on=['transform']",
      "C": "Escrever check de status em um arquivo, verificar antes de rodar próximo job",
      "D": "Usar Apache Airflow para orquestração, integrado com Databricks via hook"
    },
    "correctAnswer": "B",
    "rationale": "Databricks Workflows (Jobs UI) permite definir tasks e dependências. Você cria um job com 3 tasks (ingest, transform, load) e especifica task_depends_on para ordenar. Exemplo em config.json: task_transform: {depends_on: [task_ingest]}. Isso garante causal ordering e retry automático. Opção A com cron delays é frágil. Opção C é manual/unreliable. Opção D com Airflow também funciona mas é complexidade adicional.",
    "tip": "task_depends_on é a forma padrão de orquestração em Databricks. Define DAG de tasks.",
    "officialReference": {
      "title": "Databricks Workflows",
      "url": "https://docs.databricks.com/en/workflows/index.html"
    },
    "contextScenario": "ETL diário: ingest de 10 fontes (1h), transform dos dados (30min), load para warehouse (20min). Com task_depends_on, pipeline inteiro roda em sequência sem intervenção manual."
  },
  {
    "id": 20,
    "category": "Productionizing Data Pipelines",
    "difficulty": "advanced",
    "question": "Um job em Databricks falha ocasionalmente (timeout de conexão com fonte). Você quer configurar retry automático: máximo 3 tentativas, esperar 10 segundos entre tentativas. Como?",
    "options": {
      "A": "Usar max_retries e retry_delay na task config: max_retries: 3, retry_delay_seconds: 10",
      "B": "Wrappear job em script Python com try-except + time.sleep()",
      "C": "Usar exponential backoff: 10s, 20s, 40s; configurar em Databricks cluster settings",
      "D": "Usar job trigger com on_failure action set to 'RETRY' e max_retries=3"
    },
    "correctAnswer": "A",
    "rationale": "Databricks Jobs UI permite configurar retry policy na task level: max_retries (default 0) e retry_delay_seconds. Exemplo em YAML: max_retries: 3, retry_delay_seconds: 10. Sistema reexecuta task automaticamente. Opção B é manual, C não é forma padrão, D não existe como trigger action.",
    "tip": "max_retries + retry_delay_seconds são configuração padrão de resilience em Databricks Jobs.",
    "officialReference": {
      "title": "Job Retry Configuration",
      "url": "https://docs.databricks.com/en/workflows/jobs/create-manage.html"
    },
    "contextScenario": "Job falha 1-2x por semana por timeouts transientes. Com retry automático, 99% das falhas se resolvem sem intervenção."
  },
  {
    "id": 21,
    "category": "Productionizing Data Pipelines",
    "difficulty": "intermediate",
    "question": "Você quer monitorar um job crítico em Databricks para garantir que roda dentro de 1 hora. Se exceder, quer alertar. Como configurar?",
    "options": {
      "A": "Usar timeout_seconds na job config para cancelar job se > 3600s; não há alerta nativo",
      "B": "Configurar Databricks Alerts: criar alert baseado em job duration; enviar notificação via Slack/email se > 1h",
      "C": "Escrever logs customizados; monitorar via dashboard Databricks SQL",
      "D": "Usar DBFS para escrever tempo de execução; polling externo verifica arquivo"
    },
    "correctAnswer": "B",
    "rationale": "Databricks oferece Alerts (integração com Dashboards + Queries). Você pode criar um alert que monitora a duração de runs de um job via query SQL no Databricks SQL, e triggar notificação via webhook para Slack/Teams. Opção A (timeout) é diferente de alerta. Opção C/D são workarounds manuais.",
    "tip": "Databricks Alerts são forma nativa de monitoramento. Integram com BI tools e webhooks.",
    "officialReference": {
      "title": "Databricks Alerts",
      "url": "https://docs.databricks.com/en/sql/user/alerts.html"
    },
    "contextScenario": "Job crítico roda normalmente em 20min. Se demora > 1h, Slack notifica ops team para investigar."
  }
]