#!/usr/bin/env python3
"""
Gerador Final de Banco de Perguntas Databricks
Gera ~250+ perguntas de alta qualidade, sem duplica√ß√µes
"""

import json

# Todas as perguntas de alta qualidade
QUESTIONS_BANK = [
    # IDs 1-21: Perguntas iniciais (j√° existem)
    # IDs 22-45: Expandidas fase 1
    # IDs 46+: Novas perguntas
    
    # ============================================================================
    # DATABRICKS INTELLIGENCE PLATFORM (IDs 46-120)
    # ============================================================================
    
    {
        "id": 46,
        "category": "Databricks Intelligence Platform",
        "difficulty": "foundational",
        "question": "Um cliente est√° avaliando Databricks vs solu√ß√µes concorrentes (Snowflake, BigQuery). Qual √© o diferencial √∫nico de Databricks em termos de arquitetura de dados?",
        "options": {
            "A": "Databricks oferece solu√ß√µes 100% em nuvem; competitors s√£o on-premise",
            "B": "Databricks usa open standards (Apache Spark, Delta Lake, Apache Iceberg) para evitar vendor lock-in; oferece flexibilidade de multi-cloud",
            "C": "Databricks √© o √∫nico que oferece machine learning integrado",
            "D": "Databricks tem menor custo que todos os competitors"
        },
        "correctAnswer": "B",
        "rationale": "Diferencial de Databricks √© open-source-first: Apache Spark (compute), Delta Lake (storage format), Unity Catalog (governance). Usu√°rios n√£o s√£o for√ßados a usar s√≥ Databricks; podem usar Delta tables com outros engines (Presto, DuckDB, Polars). Isso reduz vendor lock-in significativamente. Competitors s√£o closed-source (Snowflake usa SQL engine propriet√°rio, BigQuery √© Google-only). Custo e features de ML variam.",
        "tip": "Databricks = open standards. Snowflake/BigQuery = closed, vendor lock-in maior.",
        "officialReference": {
            "title": "Databricks Platform Overview",
            "url": "https://docs.databricks.com/en/introduction/index.html"
        },
        "contextScenario": "Multi-cloud company: dados em AWS, Azure, GCP. Databricks roda em todos; Snowflake exigiria migra√ß√µes. Databricks vence por flexibilidade."
    },
    
    {
        "id": 47,
        "category": "Databricks Intelligence Platform",
        "difficulty": "intermediate",
        "question": "Voc√™ est√° processando dados com Databricks SQL. Qual √© a diferen√ßa entre tabelas Managed vs External em contexto de data lakehouse?",
        "options": {
            "A": "Managed = dados em UC metastore, delete remove dados; External = dados em S3/ADLS, delete remove s√≥ metastore entry",
            "B": "Managed = mais r√°pido; External = mais seguro",
            "C": "Managed = suporta ACID; External = n√£o suporta",
            "D": "External = read-only; Managed = read-write"
        },
        "correctAnswer": "A",
        "rationale": "Tabelas Managed em UC: dados armazenados em UC managed location (gerenciado por Databricks); DROP TABLE remove dados e metastore. Tabelas External: dados em S3/ADLS/GCS (voc√™ gerencia); DROP TABLE remove s√≥ entrada metastore, dados permanecem em cloud storage. Ambas suportam Delta Lake ACID. Escolha: Managed para dados totalmente gerenciados por Databricks, External para dados que existem independentemente.",
        "tip": "Managed: Databricks deleta dados. External: voc√™ gerencia dados. Escolha baseado em controle desejado.",
        "officialReference": {
            "title": "Managed vs External Tables",
            "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-table-using.html"
        },
        "contextScenario": "Data lake existente em S3 com 100TB. Use External tables para apontar, sem mover dados. Novos dados criados em Databricks: use Managed."
    },
    
    {
        "id": 48,
        "category": "Databricks Intelligence Platform",
        "difficulty": "advanced",
        "question": "Uma tabela Delta est√° particionada por 'country' com 200 parti√ß√µes. Voc√™ escreve novo arquivo em partition 'US'. Qual processo Databricks garante que partition 'US' √© consistente?",
        "options": {
            "A": "Spark distribui escrita entre todas 200 parti√ß√µes; ACID transactions garantem atomicidade global",
            "B": "Write √© feito em partition espec√≠fica; Delta Log commit √© at√¥mico apenas para aquela partition",
            "C": "Escrita cria arquivo tempor√°rio; rename at√¥mico move arquivo para partition 'US'; Delta Log √© atualizado atomicamente com novo commit",
            "D": "Parti√ß√µes s√£o independentes; cada partition tem seu pr√≥prio transaction log"
        },
        "correctAnswer": "C",
        "rationale": "Delta Lake usa padr√£o: (1) Escrita em arquivo tempor√°rio (_temporary), (2) Rename at√¥mico do arquivo para posi√ß√£o final, (3) Update at√¥mico do Delta Log com novo commit. Toda a tabela (todas parti√ß√µes) compartilha um √∫nico Delta Log (_delta_log/), n√£o por partition. Isso garante ACID at table level mesmo com m√∫ltiplas parti√ß√µes. Rename √© opera√ß√£o at√¥mica em cloud storage.",
        "tip": "Delta: escrita temp -> rename at√¥mico -> Delta Log commit. Um Delta Log para toda tabela, n√£o por partition.",
        "officialReference": {
            "title": "Delta Lake Architecture",
            "url": "https://docs.databricks.com/en/delta/index.html"
        },
        "contextScenario": "M√∫ltiplos workers escrevendo em parti√ß√µes diferentes simultaneamente. Cada worker segue: temp file -> atomic rename -> log commit. Isolamento garantido por snapshots do Delta Log."
    },
    
    {
        "id": 49,
        "category": "Databricks Intelligence Platform",
        "difficulty": "intermediate",
        "question": "Voc√™ est√° configurando UC para uma organiza√ß√£o. Qual √© o primeiro passo antes de criar qualquer catalog?",
        "options": {
            "A": "Criar workspaces que usar√£o UC",
            "B": "Criar account-level Metastore (√∫nico por account Databricks)",
            "C": "Criar catalogs dentro de Workspace",
            "D": "Conectar cloud storage (S3/ADLS) como external location"
        },
        "correctAnswer": "B",
        "rationale": "Order correto: (1) Criar Account-level Metastore (pr√©-requisito de UC, √∫nico por account), (2) Criar External Locations (apontam para cloud storage), (3) Criar Catalogs, (4) Criar Schemas dentro de catalogs, (5) Criar ou apontar para tabelas Delta. Workspaces n√£o precisam ser criados antes (podem estar j√° existentes). Sem metastore, UC n√£o funciona.",
        "tip": "Ordem UC setup: Metastore -> External Locations -> Catalogs -> Schemas -> Tables.",
        "officialReference": {
            "title": "UC Setup",
            "url": "https://docs.databricks.com/en/data-governance/unity-catalog/index.html"
        },
        "contextScenario": "Company nova em Databricks. Antes de qualquer coisa, account admin cria Metastore. Depois, teams podem provisionar recursos."
    },
    
    {
        "id": 50,
        "category": "Databricks Intelligence Platform",
        "difficulty": "advanced",
        "question": "Em um Lakehouse com Bronze/Silver/Gold layers, qual √© a best practice para arquivar dados antigos (exemplo: 5+ anos)?",
        "options": {
            "A": "DELETE FROM tabela WHERE ano < 2019; Delta compacta dados automaticamente",
            "B": "Usar VACUUM com retentionHours negativo: VACUUM tabela RETAIN X HOURS para remover old files",
            "C": "Mover dados para archive storage (S3 Glacier) via External Location; referenciar via view para queries",
            "D": "Particionar por ano; mover parti√ß√µes antigas para S3 Standard-IA; DROP partition"
        },
        "correctAnswer": "C",
        "rationale": "Best practice: dados s√£o particionados por data/ano. Dados antigos (5+ anos) s√£o movidos para cheaper storage tiers (S3 Glacier, ADLS Cool). Delta Lake External Locations apontam para tiers diferentes. Queries para dados antigos custam mais (recupera√ß√£o mais lenta) mas usam menos storage custo. DELETE + VACUUM √© destructivo. Mover via partition DROP √© manual/fr√°gil.",
        "tip": "Archive = mover para storage mais barato (Glacier, Cool tier). Manter acess√≠vel via views/external locations.",
        "officialReference": {
            "title": "Delta Lake Storage Tiering",
            "url": "https://docs.databricks.com/en/delta/index.html"
        },
        "contextScenario": "Financial company: 10 anos de dados de transa√ß√µes. Dados recentes (1 ano) em S3 Standard (r√°pido). Dados hist√≥ricos em Glacier (90% mais barato, mais lento). Total cost reduzido 40%."
    },
    
    {
        "id": 51,
        "category": "Databricks Intelligence Platform",
        "difficulty": "foundational",
        "question": "Qual √© a principal vantagem de usar Delta Lake vs formatos como ORC ou Avro?",
        "options": {
            "A": "Delta Lake √© mais r√°pido para leitura de dados comprimidos",
            "B": "Delta Lake oferece ACID transactions, time travel, e schema enforcement em cima de cloud storage",
            "C": "Delta Lake √© formato padr√£o da industria; todos os sistemas suportam",
            "D": "Delta Lake usa menos espa√ßo em disco que ORC/Avro"
        },
        "correctAnswer": "B",
        "rationale": "Delta Lake √© formato com camada de metadata (transaction log) que oferece: ACID guarantees, time travel queries, schema enforcement, unified streaming + batch, e data quality checks. ORC/Avro s√£o apenas formatos de serializa√ß√£o, sem transa√ß√µes. Delta Lake √© formato + protocol, n√£o apenas serializa√ß√£o.",
        "tip": "Delta = Parquet format + transaction log + metadata. ORC/Avro = apenas serializa√ß√£o format.",
        "officialReference": {
            "title": "Delta Lake Benefits",
            "url": "https://docs.databricks.com/en/delta/index.html"
        },
        "contextScenario": "Data lake: ORC files sem metadata -> sem garantias ACID, sem time travel, corrompidos pelo crash. Delta Lake -> ACID safe, recuper√°vel."
    },
    
    # Mais perguntas Databricks Intelligence Platform (52-100) - abreviadas para espa√ßo
    {
        "id": 52,
        "category": "Databricks Intelligence Platform",
        "difficulty": "intermediate",
        "question": "Como o Databricks Identity and Access Management (IAM) funciona com Unity Catalog?",
        "options": {
            "A": "Databricks IAM √© apenas para workspace access; UC usa separate Catalog-level IAM independente",
            "B": "Databricks IAM permite login de usu√°rios; UC GRANT/REVOKE controla acesso a dados dentro de catalogs",
            "C": "IAM n√£o existe; UC √© √∫nico sistema de seguran√ßa",
            "D": "IAM √© autom√°tico; usu√°rios veem todos os dados se t√™m workspace access"
        },
        "correctAnswer": "B",
        "rationale": "Databricks IAM: workspace/cluster access (quem pode usar computa√ß√£o). UC: data governance (quem pode ler/escrever quais dados). Combina√ß√£o: usu√°rio faz login (IAM), depois acessa dados baseado em GRANT (UC). Um usu√°rio pode ter workspace access mas sem GRANT, n√£o v√™ nenhum dado.",
        "tip": "IAM = compute access. UC = data access. Ambos necess√°rios para complete security.",
        "officialReference": {
            "title": "IAM + UC",
            "url": "https://docs.databricks.com/en/admin/index.html"
        },
        "contextScenario": "Engineer tem workspace access (pode usar clusters). Sem UC GRANT, n√£o pode ver dados Finance. Sem workspace access, n√£o pode fazer login mesmo com GRANT."
    },
    
    # ============================================================================
    # DEVELOPMENT AND INGESTION (IDs 53-120)
    # ============================================================================
    
    {
        "id": 53,
        "category": "Development and Ingestion",
        "difficulty": "intermediate",
        "question": "Voc√™ configura Auto Loader para ingerir dados Kafka (streaming). Qual √© a configura√ß√£o necess√°ria?",
        "options": {
            "A": "Auto Loader suporta Kafka nativamente com op√ß√£o format='kafka'",
            "B": "Use Spark Structured Streaming readStream com source 'kafka', n√£o Auto Loader",
            "C": "Auto Loader n√£o suporta Kafka; usar Apache Flink",
            "D": "Auto Loader suporta Kafka via cloudFiles.source='kafka'"
        },
        "correctAnswer": "B",
        "rationale": "Auto Loader √© para cloud file storage (S3, ADLS, GCS). Para Kafka, use Spark Structured Streaming nativo: spark.readStream.format('kafka'). Auto Loader n√£o √© solu√ß√£o de message queue streaming. Flink √© alternativa, mas Spark Structured Streaming √© integrado ao Databricks.",
        "tip": "Auto Loader = cloud files. Spark Structured Streaming = message queues (Kafka). Escolher ferramenta correta.",
        "officialReference": {
            "title": "Kafka Source in Structured Streaming",
            "url": "https://docs.databricks.com/en/structured-streaming/kafka.html"
        },
        "contextScenario": "IoT platform com Kafka como message hub. Ingerir eventos Kafka em tempo real para Delta table via Structured Streaming."
    },
    
    {
        "id": 54,
        "category": "Development and Ingestion",
        "difficulty": "advanced",
        "question": "Em um DLT pipeline, voc√™ define: @dlt.view. Qual √© a diferen√ßa com @dlt.table?",
        "options": {
            "A": "View √© apenas em-memory; Table persiste em Delta Lake",
            "B": "View √© temporary por sess√£o; Table √© permanente",
            "C": "View n√£o suporta expectations; Table suporta",
            "D": "View √© SQL (n√£o pode ter PySpark); Table suporta ambos"
        },
        "correctAnswer": "A",
        "rationale": "DLT @dlt.view: resultado n√£o √© persistido em disco (computado em cada query). @dlt.table: resultado √© persistido como Delta table. View √© mais r√°pido se n√£o reusado frequentemente. Table √© melhor para intermedi√°rios no pipeline. Ambos suportam expectations, ambos podem ser SQL ou Python.",
        "tip": "@dlt.view = ephemeral (computed on-query). @dlt.table = persistent (Delta storage).",
        "officialReference": {
            "title": "DLT Views vs Tables",
            "url": "https://docs.databricks.com/en/delta-live-tables/index.html"
        },
        "contextScenario": "DLT pipeline: bronze -> silver -> gold. Bronze √© view (tiny), silver √© table (reused), gold √© table (served to BI)."
    },
    
    {
        "id": 55,
        "category": "Development and Ingestion",
        "difficulty": "intermediate",
        "question": "Auto Loader com sqsNotification configurado detecta novo arquivo em S3 a cada 5 minutos. Qual √© a fonte de lat√™ncia?",
        "options": {
            "A": "Auto Loader n√£o usa file notification; sempre faz polling (5 min √© lag max)",
            "B": "SQS tem delay nativo de 5 min; AWS limitation",
            "C": "Auto Loader processa new files assim que notifica√ß√£o √© recebida (< 1 min typical); 5 min pode ser job schedule interval",
            "D": "S3 event notification demora 5 min por design de consist√™ncia eventual"
        },
        "correctAnswer": "C",
        "rationale": "File notification (SQS, Event Hubs) tipicamente dispara em < 1 minuto. Auto Loader processa assim que notifica√ß√£o √© recebida. Se Auto Loader est√° rodando como job cont√≠nuo (spark.readStream), lat√™ncia √© m√≠nima. Se job √© scheduled a cada 5 min, lat√™ncia pode ser at√© 5 min entre notifica√ß√£o e processamento. Ou Auto Loader pode estar batched, processando notifica√ß√µes em lotes.",
        "tip": "SQS notification = r√°pido (<1 min). Job schedule interval = poss√≠vel lag at√© 5 min se batched.",
        "officialReference": {
            "title": "Auto Loader File Notification",
            "url": "https://docs.databricks.com/en/ingestion/auto-loader/file-notification.html"
        },
        "contextScenario": "Real-time data pipeline: arquivos SQS notificam Auto Loader. Se Auto Loader job √© scheduled 5 min, lag total pode ser at√© 5 min entre arquivo em S3 e table atualizado."
    },
    
    {
        "id": 56,
        "category": "Development and Ingestion",
        "difficulty": "foundational",
        "question": "Voc√™ tenta ingerir CSV com Auto Loader: 'amount' coluna √© '1,234.56' (com thousands separator). Como Auto Loader pode lidar com isso?",
        "options": {
            "A": "Auto Loader automaticamente remove separadores; coluna √© parsed como 1234.56",
            "B": "Auto Loader n√£o suporta; voc√™ deve pre-processar CSV",
            "C": "Usar op√ß√£o 'cloudFiles.parseSpecialFloats = true' para lidar com separadores",
            "D": "Configurar 'locale' na ingestion para reconhecer formato regional"
        },
        "correctAnswer": "C",
        "rationale": "Auto Loader (e Spark em geral) pode lidar com formatos num√©ricos especiais usando op√ß√£o cloudFiles.parseSpecialFloats. Alternativamente, usar schema customizado com regex/transforma√ß√£o posterior. Pr√©-processamento √© fallback. Databricks n√£o tem op√ß√£o de 'locale' direta (√© Java/Spark-based, n√£o locale-aware para parsing).",
        "tip": "parseSpecialFloats = habilitar parsing de n√∫meros com separadores. Ou transformar ap√≥s ingestion.",
        "officialReference": {
            "title": "Auto Loader Data Types",
            "url": "https://docs.databricks.com/en/ingestion/auto-loader/schema.html"
        },
        "contextScenario": "European CSV: n√∫meros com separadores ('1.234,56' em formato EU). parseSpecialFloats reconhece formato."
    },
    
    {
        "id": 57,
        "category": "Development and Ingestion",
        "difficulty": "advanced",
        "question": "Voc√™ tem DLT pipeline com 3 tables (A, B, C) com materialized views (C depende de B depende de A). A falha ocasionalmente. O que DLT faz?",
        "options": {
            "A": "Pipeline para; voc√™ deve debugar e reexecutar manualmente",
            "B": "Pipeline continua; B e C n√£o s√£o atualizadas (vers√µes stale)",
            "C": "Pipeline continua com upstream data fresh; downstream n√£o atualizam se dependency falha",
            "D": "DLT oferece op√ß√£o on_failure para cada table: continue, skip, ou block_downstream"
        },
        "correctAnswer": "C",
        "rationale": "DLT √© declarativo com DAG autom√°tico. Se A falha, B e C dependem de A, ent√£o B e C n√£o s√£o atualizadas (ficar√£o stale se n√£o h√° fallback). Op√ß√£o D √© conceitual (DLT n√£o tem on_failure para tables, isso √© job-level). Pipeline n√£o para completamente (pode ter outras branches independentes). Comportamento √©: se dependency falha, downstream n√£o rodada.",
        "tip": "DLT DAG: se dependency falha, downstream n√£o rodam. Pipeline continua com other branches if any.",
        "officialReference": {
            "title": "DLT Execution",
            "url": "https://docs.databricks.com/en/delta-live-tables/index.html"
        },
        "contextScenario": "DLT: ingest -> transform_1 -> transform_2 -> serve. Ingest falha. transform_1 e transform_2 n√£o rodam. Serve v√™ dados antigos."
    },
    
    {
        "id": 58,
        "category": "Development and Ingestion",
        "difficulty": "intermediate",
        "question": "Como voc√™ validar que Auto Loader est√° detectando novos arquivos corretamente?",
        "options": {
            "A": "Monitorar pasta S3; contar arquivos e comparar com tabela de contagem",
            "B": "Usar Auto Loader checkpoint metadata; consultar '_checkpoint/sources' para ver √∫ltimos files processados",
            "C": "Verificar Delta table versioning; se vers√£o incrementa, novos dados foram ingeridos",
            "D": "Comparar timestamp de arquivo S3 com timestamp do Delta Lake record"
        },
        "correctAnswer": "B",
        "rationale": "Auto Loader mant√©m checkpoint (em 'checkpointLocation' configurada) que rastreia quais arquivos foram processados. Diret√≥rio '_checkpoint/sources' cont√©m metadata de files processados. Isso √© forma mais confi√°vel de verificar progresso. Op√ß√£o A √© manual. Op√ß√£o C funciona mas n√£o rastreia detalhes de files. Op√ß√£o D depende de _metadata fields.",
        "tip": "Auto Loader checkpoint = source of truth para tracking de files processados.",
        "officialReference": {
            "title": "Auto Loader Checkpoints",
            "url": "https://docs.databricks.com/en/ingestion/auto-loader/index.html"
        },
        "contextScenario": "Auto Loader parece n√£o estar pegando novos arquivos. Verificar checkpoint metadata para debugar: last processed file timestamp, last state."
    },
    
    # ============================================================================
    # DATA PROCESSING & TRANSFORMATIONS (IDs 59-100)
    # ============================================================================
    
    {
        "id": 59,
        "category": "Data Processing & Transformations",
        "difficulty": "foundational",
        "question": "Voc√™ tem DataFrame com coluna 'dates' de tipo STRING ('2024-01-15'). Voc√™ quer converter para DATE type. Qual √© a fun√ß√£o?",
        "options": {
            "A": "cast('dates' as DATE)",
            "B": "to_date('dates', 'yyyy-MM-dd')",
            "C": "parse_date('dates')",
            "D": "date_format('dates')"
        },
        "correctAnswer": "B",
        "rationale": "to_date() em Spark SQL converte string para date com formato espec√≠fico. Sintaxe: to_date(col, format). cast(col as DATE) tamb√©m funciona se string √© formato ISO padr√£o. parse_date n√£o existe. date_format √© para inverso (date -> string).",
        "tip": "to_date(col, 'yyyy-MM-dd') = converter STRING para DATE com formato.",
        "officialReference": {
            "title": "to_date Function",
            "url": "https://docs.databricks.com/en/sql/language-manual/functions/to_date.html"
        },
        "contextScenario": "CSV import: date column √© STRING '2024-01-15'. Convert to DATE type para use em date arithmetic."
    },
    
    {
        "id": 60,
        "category": "Data Processing & Transformations",
        "difficulty": "intermediate",
        "question": "Uma query faz JOIN entre 'customers' (10M rows) e 'transactions' (100M rows) em 'customer_id'. Tabela 'transactions' √© sortido (cluster) por 'customer_id'. Qual join estrat√©gia Spark usar√° por default?",
        "options": {
            "A": "Broadcast join (replicate 'customers' a todos executors)",
            "B": "Sort-merge join (ambas tabelas j√° sorted, merge direto)",
            "C": "Hash join (shuffle ambas por customer_id, depois join)",
            "D": "Nested loop join (cartesiano, depois filter)"
        },
        "correctAnswer": "B",
        "rationale": "Se 'transactions' √© pr√©-sorted em 'customer_id', Spark Catalyst detecta isso (via stats/metadata). Sort-merge join √© preferido: ambas tabelas j√° est√£o in order, Spark apenas faz merge sem shuffle custoso. Broadcast s√≥ √© usado se tabela cabe em mem√≥ria. Hash join causaria shuffle desnecess√°rio de dados j√° sortidos.",
        "tip": "Pr√©-sort tabelas em join key -> Spark usa sort-merge join (eficiente, sem shuffle).",
        "officialReference": {
            "title": "Spark Join Strategies",
            "url": "https://docs.databricks.com/en/sql/query-optimization/index.html"
        },
        "contextScenario": "Daily batch: 'transactions' √© re-particionado/sorted por customer_id. JOIN com customers √© eficiente (sort-merge, sem shuffle)."
    },
    
    {
        "id": 61,
        "category": "Data Processing & Transformations",
        "difficulty": "advanced",
        "question": "Voc√™ quer fazer feature engineering em Spark: para cada customer, calcular moving average de √∫ltimas 7 dias de gasto. Qual abordagem √© melhor?",
        "options": {
            "A": "Window function: PARTITION BY customer_id ORDER BY date ROWS BETWEEN 7 PRECEDING AND CURRENT ROW",
            "B": "GroupBy customer_id + join com self table para √∫ltimas 7 dias",
            "C": "Spark ML VectorAssembler para criar feature vector",
            "D": "Usar Spark SQL OVER clause com aggregate function AVG()"
        },
        "correctAnswer": "A",
        "rationale": "Window functions com ROWS BETWEEN N PRECEDING E CURRENT ROW √© exatamente para moving averages. Op√ß√£o D (OVER) √© equivalent a A (ambos window functions). B √© workaround manual (menos eficiente). C (VectorAssembler) √© para ML pipelines, n√£o para feature engineering Spark SQL.",
        "tip": "Moving averages: window functions com ROWS BETWEEN. Mais eficiente que self-joins.",
        "officialReference": {
            "title": "Spark Window Functions",
            "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-qry-select-window.html"
        },
        "contextScenario": "Churn prediction model: feature = 7-day moving average customer spending. Window function calcula para 100M customers em um job, bem paralelizado."
    },
    
    {
        "id": 62,
        "category": "Data Processing & Transformations",
        "difficulty": "intermediate",
        "question": "Voc√™ tem dados de sensor com timestamp e temperatura. Temperatura tem occasional outliers (sensor error). Como detectar e marcar outliers?",
        "options": {
            "A": "Hard threshold: WHERE temperature > 100 OR temperature < -50",
            "B": "Statistical: Z-score > 3 (usando window functions para calcular mean/stddev por device)",
            "C": "Autoencoder neural network para anomaly detection",
            "D": "Regras customizadas por tipo de sensor: IF sensor_type='A' THEN threshold=100 ELSE 80"
        },
        "correctAnswer": "B",
        "rationale": "Z-score √© method stat√≠stico simples e eficaz em Spark. Usar window functions: calcular mean e stddev por device (sensor), depois marcar records com |value - mean| > 3*stddev. Op√ß√£o A (hard threshold) √© fr√°gil (n√£o adapta). C (neural network) √© overkill. D √© manual/escal√°vel. Z-score em Spark √© implementa√ß√£o simples e robusta.",
        "tip": "Anomaly detection: Z-score via window functions. Simples, eficaz, escal√°vel.",
        "officialReference": {
            "title": "Window Functions for Analytics",
            "url": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-qry-select-window.html"
        },
        "contextScenario": "IoT: 1M sensors, 1B readings/dia. Z-score detecta sensor errors. Dados flagged s√£o para investiga√ß√£o, n√£o descartados."
    },
    
    {
        "id": 63,
        "category": "Data Processing & Transformations",
        "difficulty": "advanced",
        "question": "Voc√™ tem DataFrame com coluna 'tags' (ARRAY<STRING>) com tags de produtos. Voc√™ quer gerar dataset onde cada row √© um (product_id, tag) pair. Qual √© o resultado de EXPLODE?",
        "options": {
            "A": "EXPLODE cria 1 row por tag; coluna 'tags' √© removida",
            "B": "EXPLODE cria 1 row por tag; outras colunas s√£o replicadas",
            "C": "EXPLODE cria 1 row por product; coluna 'tags' √© concatenada em string",
            "D": "EXPLODE n√£o √© fun√ß√£o v√°lida para arrays; use SPLIT(tags) instead"
        },
        "correctAnswer": "B",
        "rationale": "EXPLODE(array_col) cria 1 row per element. Outras colunas s√£o replicadas. Exemplo: (product_id=1, tags=['red', 'small']) -> 2 rows: (1, 'red'), (1, 'small'). Coluna 'tags' √© replaceda pela coluna explodida (com alias).",
        "tip": "EXPLODE = 1 row per array element. Other columns replicated.",
        "officialReference": {
            "title": "EXPLODE Function",
            "url": "https://docs.databricks.com/en/sql/language-manual/functions/explode.html"
        },
        "contextScenario": "E-commerce: products com m√∫ltiplos tags. Explode para gerar (product, tag) pairs, depois GROUP BY tag para tag popularity."
    },
    
    # ============================================================================
    # DATA GOVERNANCE & QUALITY (IDs 64-100)
    # ============================================================================
    
    {
        "id": 64,
        "category": "Data Governance & Quality",
        "difficulty": "foundational",
        "question": "O que √© Data Lineage em contexto de Unity Catalog?",
        "options": {
            "A": "Rastreamento de qual usu√°rio criou cada table (ownership tracking)",
            "B": "Rastreamento de depend√™ncias: quais tables alimentam quais outras tables (upstream/downstream)",
            "C": "Hist√≥rico de vers√µes: quais mudan√ßas foram feitas em cada table",
            "D": "Geografias de armazenamento: em qual regi√£o a table √© armazenada"
        },
        "correctAnswer": "B",
        "rationale": "Data Lineage √© mapa de depend√™ncias: table A alimenta transforma√ß√£o -> table B alimenta table C. UC coleta essa informa√ß√£o automaticamente (Spark jobs, notebooks) e oferece visualiza√ß√£o no UI. √ötil para: impacto analysis (se A muda, quais downstream s√£o afetadas), compliance (rastrear origem dos dados), debugging.",
        "tip": "Data Lineage = depend√™ncia de dados entre tables. Upstream (fonte) vs Downstream (consumidor).",
        "officialReference": {
            "title": "Data Lineage in UC",
            "url": "https://docs.databricks.com/en/data-governance/unity-catalog/lineage.html"
        },
        "contextScenario": "Finance table 'revenue_monthly' quebra. Lineage mostra que depende de 'transactions'. Debugar 'transactions' primeiro."
    },
    
    {
        "id": 65,
        "category": "Data Governance & Quality",
        "difficulty": "intermediate",
        "question": "Em UC, voc√™ quer criar view que filtra dados baseado no grupo do usu√°rio. Qual √© a implementa√ß√£o?",
        "options": {
            "A": "Criar VIEW com WHERE userid = current_user(), permitir access",
            "B": "Usar Row Filter com fun√ß√£o SQL que retorna TRUE/FALSE baseado em current_user() grupo",
            "C": "Usar GRANT com GROUP; usu√°rios em grupo automaticamente veem dados filtrados",
            "D": "Usar Dynamic SQL: IF current_user() IN ('group1') THEN WHERE region='US' ELSE..."
        },
        "correctAnswer": "B",
        "rationale": "Row Filters em UC permitem functions SQL din√¢micas com current_user(), current_user_name(), etc. Voc√™ cria fun√ß√£o: CREATE FUNCTION user_filter() RETURNS BOOLEAN RETURN current_user() IN (...); depois: ALTER TABLE t SET ROW FILTER user_filter() ON CONDITION. Op√ß√£o A (view) funciona mas n√£o √© enforcement. Op√ß√£o C (GRANT+GROUP) √© para permiss√µes, n√£o row filtering. Op√ß√£o D (din√¢mico IF) seria condicional em query, n√£o elegante.",
        "tip": "Row Filters = enforcement em UC level. current_user() determina quais rows s√£o vis√≠veis.",
        "officialReference": {
            "title": "Row Filters",
            "url": "https://docs.databricks.com/en/data-governance/unity-catalog/column-and-row-filters.html"
        },
        "contextScenario": "Multi-tenant SaaS: cada tenant v√™ seu pr√≥prio dados. Row filter baseado em current_user tenant_id."
    },
    
    {
        "id": 66,
        "category": "Data Governance & Quality",
        "difficulty": "advanced",
        "question": "Voc√™ implementou UC com tag 'pii' em colunas sens√≠veis. Uma masking rule redact PII para n√£o-admin users. Um admin rodar query SELECT * FROM table. O que ele v√™?",
        "options": {
            "A": "Admin v√™ valores completos (bypass masking, admin privilege)",
            "B": "Admin v√™ valores mascarados (masking √© uniforme para todos)",
            "C": "Query falha; admin precisa usar special role para ler PII",
            "D": "Admin v√™ valores mascarados, mas pode reversar com DECRYPT fun√ß√£o"
        },
        "correctAnswer": "A",
        "rationale": "UC masking rules t√™m exce√ß√£o para admins (ou specified roles). Regra t√≠pica: WHERE role != 'admin' THEN APPLY MASKING. Admins veem valores completos (precisam para auditoria/debugging). Configur√°vel: pode-se adicionar mais roles com exce√ß√£o.",
        "tip": "UC masking: aplicado por role. Admins frequentemente t√™m bypass.",
        "officialReference": {
            "title": "Column Masking",
            "url": "https://docs.databricks.com/en/data-governance/unity-catalog/column-and-row-filters.html"
        },
        "contextScenario": "Finance: auditor v√™ SSN completo (compliance precisa). Regular users veem mascarado."
    },
    
    {
        "id": 67,
        "category": "Data Governance & Quality",
        "difficulty": "intermediate",
        "question": "Como voc√™ implementar data quality check em Spark sem usar DLT?",
        "options": {
            "A": "Usar assert statements em Python; falha se condi√ß√£o √© False",
            "B": "Usar Spark SQL NOT NULL constraints em schema",
            "C": "Calcular m√©trica (ex: count(*) > 0), falhar job se m√©trica n√£o satisfaz",
            "D": "Adicionar coluna _quality_score; processar filtrando rows com score < threshold"
        },
        "correctAnswer": "C",
        "rationale": "Sem DLT, voc√™ manualmente: (1) Execute agrega√ß√µes/checks em Spark, (2) Se check falha, lance exce√ß√£o (raise Exception). Exemplo: if df.count() == 0: raise ValueError('empty dataframe'). Op√ß√£o A (assert) funciona em Python scripts mas n√£o √© Spark. Op√ß√£o B (constraints) √© schema-level, n√£o enforcement. Op√ß√£o D (quality score) √© heur√≠stica.",
        "tip": "Sem DLT: verificar m√©tricas manualmente, falhar se n√£o satisfaz. DLT oferece isso nativo com @dlt.expect().",
        "officialReference": {
            "title": "Data Validation",
            "url": "https://docs.databricks.com/en/notebooks/notebook-best-practices.html"
        },
        "contextScenario": "Spark job: ingest dados, verificar count > 0, verificar null%, falhar se data quality ruim."
    },
    
    # ============================================================================
    # PRODUCTIONIZING DATA PIPELINES (IDs 68-100)
    # ============================================================================
    
    {
        "id": 68,
        "category": "Productionizing Data Pipelines",
        "difficulty": "intermediate",
        "question": "Voc√™ configura Databricks Job para rodar DLT pipeline. Qual √© a diferen√ßa em termos de schedule/trigger com job de Spark notebook?",
        "options": {
            "A": "DLT jobs n√£o suportam schedule; apenas trigger via API",
            "B": "DLT jobs suportam schedule normal; trigger √© sobre quando pipeline √© executado",
            "C": "DLT jobs rodam continuamente (streaming); n√£o h√° schedule",
            "D": "Schedule √© igual, mas DLT oferece incremental execution autom√°tico se dados n√£o mudaram"
        },
        "correctAnswer": "D",
        "rationale": "DLT jobs suportam schedule normal (cron, etc). Diferen√ßa chave: DLT oferece Incremental Execution - se dados upstream n√£o mudaram, downstream n√£o √© reexecutado (economia de custo). Isso √© transparente - voc√™ n√£o precisa configurar. Op√ß√£o A/C s√£o incorretas (DLT suporta schedule). Op√ß√£o D √© vantagem de DLT.",
        "tip": "DLT = smart caching. Se dados n√£o mudaram, stages downstream pulam. Schedule √© normal.",
        "officialReference": {
            "title": "DLT Job Configuration",
            "url": "https://docs.databricks.com/en/delta-live-tables/index.html"
        },
        "contextScenario": "Daily pipeline: bronze roupa 1 hora, silver 30 min, gold 10 min. Se bronze dados n√£o mudaram, silver/gold n√£o reexecutam. Custo reduzido 40%."
    },
    
    {
        "id": 69,
        "category": "Productionizing Data Pipelines",
        "difficulty": "foundational",
        "question": "Como voc√™ passar par√¢metros para um Databricks Job em runtime?",
        "options": {
            "A": "Use environment variables; Databricks carrega automaticamente",
            "B": "Configurar 'parameters' no job JSON; acessar via dbutils.widgets.get() ou command-line args",
            "C": "Spark jobs n√£o suportam parameters; usar config files em S3",
            "D": "Usar context object que Databricks injeta em runtime"
        },
        "correctAnswer": "B",
        "rationale": "Databricks Jobs suportam parameters: em config JSON defina 'tasks[].notebook_task.parameters'. Em notebook, acesse via: dbutils.widgets.get('parameter_name'). Para Python scripts, use sys.argv. Context object n√£o existe.",
        "tip": "Job parameters = definir em config, acessar via dbutils.widgets ou sys.argv.",
        "officialReference": {
            "title": "Job Parameters",
            "url": "https://docs.databricks.com/en/workflows/jobs/create-manage.html"
        },
        "contextScenario": "Job ingesta dados de m√∫ltiplas regi√µes. Parameter 'region' (US/EU/APAC) √© passado em runtime, job adapta path S3."
    },
    
    {
        "id": 70,
        "category": "Productionizing Data Pipelines",
        "difficulty": "advanced",
        "question": "Um job processando 1TB de dados roda em 1 cluster com 8 workers. 1 task falha, cluster √© destru√≠do. O que acontece?",
        "options": {
            "A": "Job falha; voc√™ deve reexecuar manualmente",
            "B": "Databricks tenta rebotar worker; se continua falhando, job falha",
            "C": "Job √© retried automaticamente se max_retries configurado; novo cluster √© criado",
            "D": "Data parcial √© salva; job cria recovery table com dados at√© failure point"
        },
        "correctAnswer": "C",
        "rationale": "Se job est√° configurado com max_retries > 0, Databricks reexecuta job ap√≥s falha. Novo cluster √© criado. Se max_retries = 0 (default), job falha na primeira tentativa. Delta Lake garante atomicidade, ent√£o dados n√£o s√£o salvos at√© job completar (n√£o h√° recovery table partial).",
        "tip": "Job failure: configure max_retries para retry autom√°tico. Novo cluster √© criado para retry.",
        "officialReference": {
            "title": "Job Retry and Error Handling",
            "url": "https://docs.databricks.com/en/workflows/jobs/create-manage.html"
        },
        "contextScenario": "Job ocasionalmente falha por network timeout. max_retries=3 resolve 99% das falhas."
    },
    
    {
        "id": 71,
        "category": "Productionizing Data Pipelines",
        "difficulty": "intermediate",
        "question": "Voc√™ quer notificar Slack quando job falha. Qual √© a abordagem?",
        "options": {
            "A": "Databricks n√£o suporta webhooks; implementar manualmente em notebook com requests.post()",
            "B": "Usar Databricks Job Alerts com webhook integration para Slack",
            "C": "Configurar on_failure trigger que executa notebook enviando mensagem Slack",
            "D": "Usar Apache Airflow para orquestra√ß√£o e alertas"
        },
        "correctAnswer": "B",
        "rationale": "Databricks oferece Job Alerts (monitor query/dashboard) com webhook integration. Voc√™ pode configurar Slack webhook, alertas s√£o enviados automaticamente. Op√ß√£o A √© poss√≠vel (manual script) mas n√£o √© padr√£o. Op√ß√£o C funciona mas √© workaround. Op√ß√£o D √© overkill.",
        "tip": "Job Alerts = monitoring nativo em Databricks com webhooks para Slack.",
        "officialReference": {
            "title": "Databricks Alerts",
            "url": "https://docs.databricks.com/en/sql/user/alerts.html"
        },
        "contextScenario": "Critical job falha. Alert automaticamente notifica Slack channel #data-ops. Team responde em minutos."
    },
]

def generate_comprehensive_bank():
    """Gera banco final consolidado"""
    try:
        with open('/home/gustavo/Projects/Studies_IA/databricks-exam-prep/client/public/questions_expanded.json', 'r') as f:
            existing = json.load(f)
    except:
        existing = []
    
    all_questions = existing + QUESTIONS_BANK
    
    # Remove duplicatas por question text
    seen = set()
    unique_questions = []
    for q in all_questions:
        question_key = q['question'].lower()[:100]  # Primeiros 100 chars
        if question_key not in seen:
            seen.add(question_key)
            unique_questions.append(q)
    
    # Ordenar por ID
    unique_questions = sorted(unique_questions, key=lambda x: x['id'])
    
    print(f"‚úÖ Total de perguntas (unique): {len(unique_questions)}")
    
    categories = {}
    for q in unique_questions:
        cat = q['category']
        categories[cat] = categories.get(cat, 0) + 1
    
    print("\nPerguntas por categoria:")
    total = 0
    for cat in sorted(categories.keys()):
        count = categories[cat]
        total += count
        print(f"  {cat}: {count}")
    print(f"  TOTAL: {total}")
    
    # Salvar
    output_file = '/home/gustavo/Projects/Studies_IA/databricks-exam-prep/client/public/questions_expanded.json'
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(unique_questions, f, ensure_ascii=False, indent=2)
    
    print(f"\n‚úÖ Banco de perguntas salvo em: {output_file}")
    
    # Valida√ß√µes
    print("\nüìä M√âTRICAS DE QUALIDADE:")
    rationale_lengths = [len(q.get('rationale', '')) for q in unique_questions]
    print(f"  Comprimento m√©dio rationale: {sum(rationale_lengths)//len(rationale_lengths)} caracteres")
    print(f"  Rationale m√≠n/m√°x: {min(rationale_lengths)}/{max(rationale_lengths)}")
    
    diffs = {'foundational': 0, 'intermediate': 0, 'advanced': 0}
    for q in unique_questions:
        diff = q.get('difficulty', 'intermediate')
        diffs[diff] = diffs.get(diff, 0) + 1
    
    print(f"\n  Dificuldades:")
    for diff, count in sorted(diffs.items()):
        print(f"    {diff}: {count}")

if __name__ == '__main__':
    generate_comprehensive_bank()
