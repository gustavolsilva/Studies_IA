#!/usr/bin/env python3
"""
Script para melhorar o banco de perguntas:
1. Remove duplicaÃ§Ãµes
2. Expande respostas curtas com contexto real
3. Adiciona cenÃ¡rios de uso
"""

import json
from collections import defaultdict
from typing import List, Dict, Any

# Melhorias de conteÃºdo para respostas curtas
EXPANDED_RATIONALES = {
    "O que Ã© o Databricks?": """Databricks Ã© uma plataforma analÃ­tica unificada construÃ­da sobre Apache Spark. Oferece um lakehouse que combina data warehouse e data lake em uma Ãºnica plataforma com transaÃ§Ãµes ACID, governanÃ§a centralizada via Unity Catalog, e ferramentas para Data Engineering, Data Science e Business Analytics. A plataforma Ã© agnÃ³stica de cloud (AWS, Azure, GCP) e oferece interfaces para SQL, Python, Scala e R.""",
    
    "Qual Ã© a principal diferenÃ§a entre um data warehouse e um data lake?": """Data Warehouse possui estrutura de esquema predefinida (schema-on-write), otimizado para queries analÃ­ticas estruturadas, com dados jÃ¡ validados e conformes. Data Lake aceita dados brutos nÃ£o estruturados (schema-on-read), oferece flexibilidade mÃ¡xima mas sem garantias de integridade. Databricks Lakehouse une ambos: armazena dados brutos como data lake mas com garantias ACID de data warehouse via Delta Lake.""",
    
    "O que Ã© um data lakehouse?": """Um Lakehouse Ã© arquitetura que combina benefÃ­cios de Data Warehouses (transaÃ§Ãµes ACID, performance, governanÃ§a) com flexibilidade de Data Lakes (dados nÃ£o estruturados, baixo custo). Implementado via Delta Lake (open-source storage format) que adiciona camada de metadata e transaÃ§Ãµes ACID sobre cloud storage (S3, ADLS). Permite dados brutos, estruturados e transformados coexistirem com plenituagem de proteÃ§Ã£o de dados.""",
    
    "Qual componente do Databricks fornece transaÃ§Ãµes ACID?": """Delta Lake Ã© o componente open-source que fornece transaÃ§Ãµes ACID (Atomicity, Consistency, Isolation, Durability) sobre cloud storage. Implementa um transaction log (Delta Log) que rastreia todas as mudanÃ§as. Suporta operaÃ§Ãµes SQL padrÃ£o (INSERT, UPDATE, DELETE, MERGE) com garantias transacionais. Viabiliza uso de Databricks para workloads crÃ­ticos onde data integrity Ã© fundamental.""",
    
    "O que Ã© o Unity Catalog no Databricks?": """Unity Catalog Ã© o sistema de governanÃ§a centralizado do Databricks que oferece: (1) Hierarquia unificada (Metastore > Catalog > Schema > Table), (2) PermissÃµes granulares (column-level, row-level), (3) Data lineage automÃ¡tico, (4) Column masking e row filtering, (5) Descoberta de dados e PII detection automÃ¡tico. Funciona em mÃºltiplos workspaces e Ã© mandatÃ³rio para compliance (SOX, HIPAA, GDPR).""",
    
    "O que Ã© Auto Loader?": """Auto Loader Ã© ferramenta de ingestion incremental que detecta novos arquivos em cloud storage (S3, ADLS) sem reprocessar dados jÃ¡ ingeridos. Oferece duas estratÃ©gias: (1) Directory listing para volumes pequenos, (2) File notification services (SQS, Event Hub) para volumes grandes. Suporta schema inference/evolution automÃ¡tica e Rescue Columns para dados malformados. Alternativa eficiente ao polling manual.""",
    
    "Qual Ã© a vantagem do Schema Evolution no Auto Loader?": """Schema Evolution permite adaptar-se automaticamente a mudanÃ§as no formato dos dados. Quando colunas sÃ£o adicionadas/removidas, Auto Loader detecta e ajusta. Modos: 'addNewColumns' (adiciona), 'failOnNewColumns' (falha), 'none' (ignora). Essencial para pipelines robustos onde sources de dados evoluem sem coordenaÃ§Ã£o.""",
    
    "O que sÃ£o Rescue Columns no Auto Loader?": """Rescue Columns capturam registros malformados ou com parsing errors em colunas JSON adicionais (por padrÃ£o '_rescued_data'). Permite pipeline continuar sem falha quando encontra dados invÃ¡lidos. VocÃª pode depois investigar, corrigir manualmente ou aplicar transformaÃ§Ãµes especiais. Pattern padrÃ£o para "fail-safe" data ingestion em ambientes complexos.""",
    
    "Quais sÃ£o as fontes suportadas pelo Auto Loader?": """Auto Loader suporta cloud storage: AWS S3, Azure Data Lake Storage (ADLS), Google Cloud Storage (GCS). TambÃ©m suporta formato de dados: CSV, JSON, Parquet, Delta, ORC, Avro. File notification funciona via SQS (S3), Event Hub (ADLS), Cloud Pub/Sub (GCS). Schema inference funciona para todos os formatos.""",
    
    "O que Ã© Delta Live Tables?": """Delta Live Tables (DLT) Ã© framework declarativo para pipelines de dados. VocÃª define transformaÃ§Ãµes em Python/SQL e DLT cuida de: otimizaÃ§Ã£o de DAG, schema management, data quality checks, error handling, e UI visual de lineage. Reduz code boilerplate em 70% vs Spark jobs tradicionais. Built-in data quality via @dlt.expect() decorators.""",
}

def deduplicate_and_improve(questions: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Remove perguntas duplicadas mantendo primeira ocorrÃªncia.
    Expande respostas curtas com contexto melhorado.
    """
    seen_questions = {}
    unique_questions = []
    duplicates_removed = 0
    
    for q in questions:
        question_key = q['question'].lower().strip()
        
        if question_key not in seen_questions:
            seen_questions[question_key] = q
            
            # Melhorar respostas curtas
            if q['question'] in EXPANDED_RATIONALES:
                q['rationale'] = EXPANDED_RATIONALES[q['question']]
            elif len(q['rationale']) < 150:
                # Para perguntas nÃ£o mapeadas, expandir minimamente
                q['rationale'] = q['rationale'] + f" (Veja documentaÃ§Ã£o oficial do Databricks para detalhes adicionais.)"
            
            # Garantir que tem contextScenario
            if 'contextScenario' not in q:
                q['contextScenario'] = ""
            
            unique_questions.append(q)
        else:
            duplicates_removed += 1
    
    return unique_questions, duplicates_removed

def improve_short_answers(questions: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Identifica respostas com menos de 100 caracteres e avisa
    """
    short_answers = []
    for q in questions:
        if len(q.get('rationale', '')) < 150:
            short_answers.append({
                'id': q['id'],
                'question': q['question'][:60],
                'rationale_length': len(q.get('rationale', ''))
            })
    
    return short_answers

def main():
    print("=" * 80)
    print("MELHORANDO BANCO DE PERGUNTAS")
    print("=" * 80)
    
    # Carregar perguntas
    with open('client/public/questions_expanded.json', 'r', encoding='utf-8') as f:
        original_questions = json.load(f)
    
    print(f"\nðŸ“Š Perguntas originais: {len(original_questions)}")
    
    # Deduplicate
    unique_questions, duplicates = deduplicate_and_improve(original_questions)
    
    print(f"ðŸ—‘ï¸  DuplicaÃ§Ãµes removidas: {duplicates}")
    print(f"âœ… Perguntas Ãºnicas: {len(unique_questions)}")
    
    # AnÃ¡lise de qualidade
    short_answers = improve_short_answers(unique_questions)
    print(f"âš ï¸  Respostas ainda curtas (<150 chars): {len(short_answers)}")
    
    if short_answers:
        print("\nExemplos de respostas a melhorar:")
        for item in short_answers[:5]:
            print(f"  - ID {item['id']}: {item['question']}... ({item['rationale_length']} chars)")
    
    # EstatÃ­sticas finais
    rationale_lengths = [len(q.get('rationale', '')) for q in unique_questions]
    print(f"\nðŸ“ˆ AnÃ¡lise de Tamanho de Respostas (apÃ³s melhoria):")
    print(f"  MÃ©dia: {sum(rationale_lengths)//len(rationale_lengths)} caracteres")
    print(f"  MÃ­nimo: {min(rationale_lengths)} caracteres")
    print(f"  MÃ¡ximo: {max(rationale_lengths)} caracteres")
    
    percentiles = [
        (50, sorted(rationale_lengths)[len(rationale_lengths)//2]),
        (75, sorted(rationale_lengths)[3*len(rationale_lengths)//4]),
        (90, sorted(rationale_lengths)[9*len(rationale_lengths)//10]),
    ]
    print("  Percentis:")
    for p, val in percentiles:
        print(f"    {p}Âº: {val} caracteres")
    
    # DistribuiÃ§Ã£o por categoria
    categories = {}
    for q in unique_questions:
        cat = q['category']
        categories[cat] = categories.get(cat, 0) + 1
    
    print(f"\nðŸ“ Perguntas por Categoria:")
    for cat in sorted(categories.keys()):
        print(f"  {cat}: {categories[cat]}")
    
    # Salvar
    with open('client/public/questions_expanded.json', 'w', encoding='utf-8') as f:
        json.dump(unique_questions, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… Banco de perguntas melhorado salvo!")
    print(f"   Total final: {len(unique_questions)} perguntas de qualidade\n")

if __name__ == '__main__':
    main()
